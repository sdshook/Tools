#!/usr/bin/env python3
"""
FORAI LLM Integration Example
Demonstrates the new LLM-powered forensic analysis capabilities
"""

import os
import sys
from FORAI import FORAI

def main():
    """Demonstrate FORAI LLM integration capabilities"""
    
    print("=== FORAI LLM Integration Demo ===\n")
    
    # Example case setup
    case_id = "DEMO_CASE_001"
    
    # Initialize FORAI with different LLM providers
    print("1. OpenAI GPT-4 Integration:")
    try:
        forai_openai = FORAI(
            case_id=case_id,
            llm_provider="openai",
            llm_model="gpt-4"
        )
        print("   ✓ OpenAI integration initialized")
    except Exception as e:
        print(f"   ✗ OpenAI integration failed: {e}")
    
    print("\n2. Anthropic Claude Integration:")
    try:
        forai_claude = FORAI(
            case_id=case_id,
            llm_provider="anthropic",
            llm_model="claude-3-sonnet-20240229"
        )
        print("   ✓ Anthropic integration initialized")
    except Exception as e:
        print(f"   ✗ Anthropic integration failed: {e}")
    
    print("\n3. Local LLM (Ollama) Integration:")
    try:
        forai_local = FORAI(
            case_id=case_id,
            llm_provider="local",
            llm_model="llama3"
        )
        print("   ✓ Local LLM integration initialized")
    except Exception as e:
        print(f"   ✗ Local LLM integration failed: {e}")
    
    # Demonstrate ad-hoc question capabilities
    print("\n=== AD-HOC FORENSIC QUESTIONS ===")
    
    sample_questions = [
        "What evidence of data exfiltration can be found in the system?",
        "Are there any signs of lateral movement in the network logs?",
        "What malware artifacts were discovered during the analysis?",
        "Can you identify the timeline of the security incident?",
        "What user accounts show suspicious activity patterns?"
    ]
    
    # Use the first available FORAI instance
    forai = None
    for instance in [forai_openai, forai_claude, forai_local]:
        if hasattr(instance, 'llm_analyzer') and instance.llm_analyzer:
            forai = instance
            break
    
    if forai:
        print(f"Using LLM provider: {forai.llm_analyzer.provider}")
        
        for i, question in enumerate(sample_questions, 1):
            print(f"\n{i}. Question: {question}")
            try:
                result = forai.answer_adhoc_question(question)
                print(f"   Answer: {result.get('answer', 'No answer available')[:200]}...")
                print(f"   Confidence: {result.get('confidence', 0):.1%}")
            except Exception as e:
                print(f"   Error: {e}")
    else:
        print("No LLM provider available for ad-hoc questions")
    
    # Demonstrate report generation capabilities
    print("\n=== LLM-ENHANCED REPORT GENERATION ===")
    print("Features:")
    print("• Executive summaries generated by LLM")
    print("• Professional case overviews")
    print("• Key findings analysis")
    print("• Technical summaries")
    print("• Comprehensive conclusions")
    print("• Limitation assessments")
    
    # Show command line usage examples
    print("\n=== COMMAND LINE USAGE EXAMPLES ===")
    print("1. Full analysis with OpenAI GPT-4:")
    print("   python FORAI.py --case-id CASE001 --full-analysis --llm-provider openai --llm-model gpt-4")
    
    print("\n2. Ad-hoc question with Anthropic Claude:")
    print("   python FORAI.py --case-id CASE001 --adhoc-question \"What malware was found?\" --llm-provider anthropic")
    
    print("\n3. Generate professional PDF report:")
    print("   python FORAI.py --case-id CASE001 --full-analysis --report pdf --llm-provider openai")
    
    print("\n4. Use local LLM (Ollama):")
    print("   python FORAI.py --case-id CASE001 --adhoc-question \"Timeline analysis?\" --llm-provider local --llm-model llama3")
    
    print("\n5. Disable LLM features:")
    print("   python FORAI.py --case-id CASE001 --full-analysis --disable-llm")
    
    print("\n=== ENVIRONMENT SETUP ===")
    print("Required environment variables:")
    print("• OPENAI_API_KEY - for OpenAI GPT models")
    print("• ANTHROPIC_API_KEY - for Anthropic Claude models")
    print("• Local LLM: Install Ollama and run 'ollama serve'")
    
    print("\n=== INSTALLATION REQUIREMENTS ===")
    print("pip install openai anthropic requests")
    
    print("\n=== FORAI LLM INTEGRATION COMPLETE ===")

if __name__ == "__main__":
    main()