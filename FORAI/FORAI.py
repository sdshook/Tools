#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FORAI.py (c) 2025 All Rights Reserved Shane D. Shook
Forensic analysis tool utilizing KAPE and Plaso timeline analysis

WORKFLOW:
Target Drive â†’ KAPE (Artifacts) â†’ log2timeline (.plaso) â†’ psort (SQLite) â†’ BHSM Database

FEATURES:
- Native artifact collection for forensic integrity
- Two-step Plaso workflow: log2timeline â†’ psort
- Custom Plaso output module for SQLite integration
- SHA256 integrity validation
- Chain of custody logging
- Timeline analysis database schema

Automated collection and processing for forensic Q&A
Supported by TinyLLaMA 1.1b
Requirements: pip install fpdf2 llama-cpp-python psutil plaso tqdm

CLI USAGE EXAMPLES:

ðŸš€ COMPLETE END-TO-END FORENSIC ANALYSIS (ONE COMMAND DOES EVERYTHING):
    # Use YOUR 12 standard forensic questions (no --question flag)
    python FORAI.py --case-id CASE001 --full-analysis --target-drive C: --chain-of-custody --verbose
    
ðŸ¤– AUTONOMOUS ANALYSIS (ANSWER ALL 12 QUESTIONS AUTOMATICALLY):
    # Local LLM model from folder
    python FORAI.py --case-id CASE001 --autonomous-analysis --llm-folder "D:\\FORAI\\LLM" --report pdf --verbose
    
    # OpenAI API with token
    python FORAI.py --case-id CASE001 --autonomous-analysis --llm-api-provider openai --llm-api-token "sk-..." --llm-model "gpt-4" --report json
    
    # Anthropic API with token
    python FORAI.py --case-id CASE001 --autonomous-analysis --llm-api-provider anthropic --llm-api-token "sk-ant-..." --llm-model "claude-3-sonnet-20240229"
    
    # Deterministic analysis only (no LLM)
    python FORAI.py --case-id CASE001 --autonomous-analysis --report json
    
    # Use custom question (with --question flag)
    python FORAI.py --case-id CASE001 --full-analysis --target-drive C: --question "Your specific custom question here" --chain-of-custody --verbose
    
    # With custom keywords for enhanced threat detection
    python FORAI.py --case-id CASE001 --full-analysis --target-drive C: --keywords-file threat_indicators.txt --chain-of-custody --verbose
    
    # With time filtering - last 30 days only
    python FORAI.py --case-id CASE001 --full-analysis --target-drive C: --days-back 30 --chain-of-custody --verbose
    
    # With specific date range and keyword flagging (YYYYMMDD format)
    python FORAI.py --case-id CASE001 --full-analysis --target-drive C: --date-from 20241201 --date-to 20241215 --keywords-file malware_iocs.txt --chain-of-custody --verbose

ðŸ”§ INDIVIDUAL WORKFLOW COMPONENTS:
    # Collect artifacts only
    python FORAI.py --case-id CASE001 --collect-artifacts --target-drive C:
    
    # Parse artifacts only (with custom keywords for flagging)
    python FORAI.py --case-id CASE001 --parse-artifacts --keywords-file suspicious_terms.txt

ðŸ“‚ USE EXISTING KAPE COLLECTION (SKIP RE-COLLECTION):
    # Full analysis using existing artifacts directory
    python FORAI.py --case-id CASE001 --full-analysis --artifacts-dir "D:\\FORAI\\artifacts\\CASE001_artifacts" --keywords-file keywords.txt --verbose
    
    # Analysis with custom question using existing collection
    python FORAI.py --case-id CASE001 --full-analysis --artifacts-dir "C:\\YourExistingKapeOutput" --question "What USB devices were connected?" --verbose

ðŸ“„ IMPORT EXISTING PLASO FILE (SKIP KAPE + LOG2TIMELINE):
    # Import existing .plaso file and create BHSM database
    python FORAI.py --case-id CASE001 --plaso-file "C:\\Evidence\\timeline.plaso" --verbose
    
    # Import plaso file with custom keywords and generate report
    python FORAI.py --case-id CASE001 --plaso-file "D:\\Cases\\CASE001.plaso" --keywords-file malware_iocs.txt --autonomous-analysis --report pdf
    
    # Import plaso file and answer specific question
    python FORAI.py --case-id CASE001 --plaso-file "timeline.plaso" --question "What anti-forensic activity occurred?" --verbose
    
    # Initialize database for a new case
    python FORAI.py --case-id CASE001 --init-db
    
    # OPTIMIZED: Direct artifact processing only (no CSV intermediary files)
    
    # Search for evidence
    python FORAI.py --case-id CASE001 --search "usb device activity"
    
    # Search with time filtering
    python FORAI.py --case-id CASE001 --search "usb device activity" --days-back 7
    python FORAI.py --case-id CASE001 --search "malware execution" --date-from 20241201 --date-to 20241215
    
    # Search with custom keywords loaded for enhanced flagging
    python FORAI.py --case-id CASE001 --search "powershell" --keywords-file malware_indicators.txt
    
    # Ask forensic questions with enhanced TinyLLama analysis
    python FORAI.py --case-id CASE001 --question "What suspicious file transfers occurred?"
    
    # Ask questions with time filtering and keyword flagging
    python FORAI.py --case-id CASE001 --question "What USB devices were connected?" --days-back 30
    python FORAI.py --case-id CASE001 --question "What network activity occurred?" --date-from 20241201 --date-to 20241215 --keywords-file network_iocs.txt
    
    # Generate comprehensive forensic report with keyword analysis
    python FORAI.py --case-id CASE001 --report json --keywords-file investigation_terms.txt
    python FORAI.py --case-id CASE001 --report pdf --keywords-file investigation_terms.txt
    
    # Generate chain of custody documentation
    python FORAI.py --case-id CASE001 --chain-of-custody
    
    # Verbose mode for detailed logging
    python FORAI.py --case-id CASE001 --search "malware" --verbose

ðŸ“‹ KEYWORD FILE EXAMPLES:
    # Create a keywords file (one keyword per line, case-insensitive)
    echo -e "mimikatz\npowershell\nnetcat\nmalicious.com\nbackdoor\ntrojan" > suspicious_terms.txt
    
    # Use keywords with full analysis workflow
    python FORAI.py --case-id CASE001 --full-analysis --target-drive C: --keywords-file suspicious_terms.txt --verbose
    
    # Process existing evidence with new keyword list
    python FORAI.py --case-id CASE001 --keywords-file new_indicators.txt --search "mimikatz" --verbose
"""

import os
import sys
import argparse
import hashlib
import sqlite3
import json
import re
import time
import subprocess
import shutil
import logging
import threading
# Required system monitoring
import psutil
from dataclasses import dataclass, field
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional, Tuple, List, Dict, Set, Union, Any, Iterator
# ThreadPoolExecutor removed - not currently used in codebase
from collections import defaultdict, deque
from functools import lru_cache, wraps
import statistics


from tqdm import tqdm
# Required PDF functionality
from fpdf import FPDF
import numpy as np
# psutil imported above with try/except

# Optional imports for LLM functionality
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    LLAMA_CPP_AVAILABLE = False
    print("Warning: llama-cpp-python not available. Local LLM functionality will be disabled.")

# Required ML functionality
try:
    from sklearn.ensemble import IsolationForest
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import accuracy_score
    import numpy as np
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("Warning: scikit-learn not available. ML functionality will be disabled.")

# ============================================================================
# INTEGRATED SEMANTIC INDEXING COMPONENTS (BHSM Lite)
# ============================================================================
# Lightweight semantic indexing without external dependencies

def l2_norm(x: np.ndarray) -> np.ndarray:
    """L2 normalization with numerical stability."""
    norm = np.linalg.norm(x)
    return x / (norm + 1e-12)

@dataclass
class PSIDocument:
    """Document stored in PSI index"""
    doc_id: str
    text: str
    vector: np.ndarray
    tags: List[str]
    valence: float
    protected: bool
    timestamp: float

class SimEmbedder:
    """Fast deterministic semantic embedder using hash-based features."""
    
    def __init__(self, dim: int = 32):
        self.dim = dim
        self._cache = {}
        
    def embed(self, text: str) -> np.ndarray:
        """Generate deterministic embedding for text"""
        if text in self._cache:
            return self._cache[text]
            
        # Normalize text
        text = text.lower().strip()
        if not text:
            return np.zeros(self.dim)
            
        # Generate multiple hash-based features
        features = np.zeros(self.dim)
        
        # Word-level features
        words = text.split()
        for i, word in enumerate(words[:self.dim//4]):
            hash_val = int(hashlib.md5(word.encode()).hexdigest()[:8], 16)
            features[i % self.dim] += (hash_val % 1000) / 1000.0
            
        # Character n-gram features
        for n in [2, 3, 4]:
            for i in range(len(text) - n + 1):
                ngram = text[i:i+n]
                hash_val = int(hashlib.md5(ngram.encode()).hexdigest()[:8], 16)
                features[(hash_val % (self.dim//4)) + (n-2)*(self.dim//4)] += 1.0
                
        # Length and structure features
        features[-4] = len(words) / 100.0  # Word count
        features[-3] = len(text) / 1000.0  # Character count
        features[-2] = text.count('.') / 10.0  # Sentence markers
        features[-1] = sum(1 for c in text if c.isupper()) / len(text) if text else 0  # Uppercase ratio
        
        # Normalize
        embedding = l2_norm(features)
        self._cache[text] = embedding
        return embedding
        
    def similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Compute cosine similarity between vectors"""
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-12)

class PSIIndex:
    """Persistent Semantic Index for fast document retrieval."""
    
    def __init__(self, db_path: Optional[Path] = None):
        self.db_path = db_path or Path("forai_psi.db")
        self.embedder = SimEmbedder()
        self._init_database()
        
    def _init_database(self):
        """Initialize SQLite database for PSI storage"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS psi_documents (
                    doc_id TEXT PRIMARY KEY,
                    text TEXT NOT NULL,
                    vector BLOB NOT NULL,
                    tags TEXT NOT NULL,
                    valence REAL NOT NULL,
                    protected INTEGER NOT NULL,
                    timestamp REAL NOT NULL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON psi_documents(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_valence ON psi_documents(valence)")
            
    def add_doc(self, doc_id: str, text: str, vector: Optional[np.ndarray] = None, 
                tags: Optional[List[str]] = None, valence: float = 0.0, protected: bool = False):
        """Add document to PSI index"""
        if vector is None:
            vector = self.embedder.embed(text)
            
        tags = tags or []
        timestamp = time.time()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO psi_documents 
                (doc_id, text, vector, tags, valence, protected, timestamp)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                doc_id, text, vector.tobytes(), 
                json.dumps(tags), valence, int(protected), timestamp
            ))
            
    def search(self, query_vector: np.ndarray, top_k: int = 5, 
               min_similarity: float = 0.1) -> List[Tuple[float, str, np.ndarray]]:
        """Search for similar documents"""
        results = []
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT doc_id, text, vector, valence FROM psi_documents")
            
            for row in cursor:
                doc_id, text, vector_bytes, valence = row
                doc_vector = np.frombuffer(vector_bytes, dtype=np.float64)
                
                similarity = self.embedder.similarity(query_vector, doc_vector)
                if similarity >= min_similarity:
                    results.append((similarity, doc_id, doc_vector))
                    
        # Sort by similarity and return top_k
        results.sort(key=lambda x: x[0], reverse=True)
        return results[:top_k]
        
    def count(self) -> int:
        """Get total number of documents"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("SELECT COUNT(*) FROM psi_documents")
            return cursor.fetchone()[0]

class BDHMemory:
    """Bidirectional Hebbian Synaptic Memory with reward gating."""
    
    def __init__(self, store_type: str = "forai", db_path: Optional[Path] = None):
        self.store_type = store_type
        self.db_path = db_path or Path(f"forai_bdh.db")
        self.embedder = SimEmbedder()
        self._init_database()
        
    def _init_database(self):
        """Initialize BDH memory database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS bdh_traces (
                    trace_id TEXT PRIMARY KEY,
                    vector BLOB NOT NULL,
                    valence REAL NOT NULL,
                    reward_count INTEGER DEFAULT 0,
                    last_reward REAL DEFAULT 0,
                    consolidation_score REAL DEFAULT 0,
                    timestamp REAL NOT NULL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_consolidation ON bdh_traces(consolidation_score)")
            
    def add_or_update(self, trace_id: str, vector: np.ndarray, valence: float = 0.0):
        """Add or update memory trace"""
        timestamp = time.time()
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO bdh_traces 
                (trace_id, vector, valence, timestamp)
                VALUES (?, ?, ?, ?)
            """, (trace_id, vector.tobytes(), valence, timestamp))
                
    def reward_gated_update(self, trace_id: str, state_vec: np.ndarray, reward: float):
        """Apply reward-gated learning to strengthen useful patterns"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT vector, valence, reward_count, consolidation_score 
                FROM bdh_traces WHERE trace_id = ?
            """, (trace_id,))
            
            row = cursor.fetchone()
            if row:
                vector_bytes, valence, reward_count, consolidation_score = row
                
                # Update consolidation score based on reward
                new_reward_count = reward_count + 1
                new_consolidation_score = consolidation_score + reward * 0.1
                new_valence = valence + reward * 0.05
                
                conn.execute("""
                    UPDATE bdh_traces 
                    SET reward_count = ?, consolidation_score = ?, valence = ?, last_reward = ?
                    WHERE trace_id = ?
                """, (new_reward_count, new_consolidation_score, new_valence, reward, trace_id))

# Global BHSM components
_GLOBAL_EMBEDDER = None
_GLOBAL_PSI = None
_GLOBAL_BDH = None

# Global Enhanced BHSM components
_GLOBAL_RETROSPECTIVE_LEARNING = None
_GLOBAL_ADAPTIVE_THRESHOLDS = None
_GLOBAL_ENHANCED_FEATURES = None
_GLOBAL_ANOMALY_DETECTOR = None

def get_global_components() -> Tuple[SimEmbedder, PSIIndex, BDHMemory]:
    """Get or create global BHSM components for FORAI"""
    global _GLOBAL_EMBEDDER, _GLOBAL_PSI, _GLOBAL_BDH
    
    if _GLOBAL_EMBEDDER is None:
        _GLOBAL_EMBEDDER = SimEmbedder()
        
    if _GLOBAL_PSI is None:
        _GLOBAL_PSI = PSIIndex(Path("forai_psi.db"))
        
    if _GLOBAL_BDH is None:
        _GLOBAL_BDH = BDHMemory("forai", Path("forai_bdh.db"))
        
    return _GLOBAL_EMBEDDER, _GLOBAL_PSI, _GLOBAL_BDH

def get_enhanced_bhsm_components():
    """Get or create enhanced BHSM components for FORAI"""
    global _GLOBAL_RETROSPECTIVE_LEARNING, _GLOBAL_ADAPTIVE_THRESHOLDS, _GLOBAL_ENHANCED_FEATURES, _GLOBAL_ANOMALY_DETECTOR

    if _GLOBAL_RETROSPECTIVE_LEARNING is None:
        _GLOBAL_RETROSPECTIVE_LEARNING = ForensicRetrospectiveLearning()

    if _GLOBAL_ADAPTIVE_THRESHOLDS is None:
        _GLOBAL_ADAPTIVE_THRESHOLDS = ForensicAdaptiveThresholds()

    if _GLOBAL_ENHANCED_FEATURES is None:
        _GLOBAL_ENHANCED_FEATURES = ForensicEnhancedFeatureExtractor()

    if _GLOBAL_ANOMALY_DETECTOR is None:
        _GLOBAL_ANOMALY_DETECTOR = ForensicAnomalyDetector()

    return _GLOBAL_RETROSPECTIVE_LEARNING, _GLOBAL_ADAPTIVE_THRESHOLDS, _GLOBAL_ENHANCED_FEATURES, _GLOBAL_ANOMALY_DETECTOR

BHSM_AVAILABLE = True  # Always available since integrated

# ============================================================================
# ENHANCED BHSM ADAPTIVE LEARNING CAPABILITIES
# Bidirectional Hebbian Synaptic Memory (BHSM) originated adaptive learning system
# These capabilities extend the core BHSM memory and learning functions
# ============================================================================

@dataclass
class MissedEvidenceEvent:
    """Represents evidence that was initially missed but later discovered"""
    case_id: str
    original_analysis_timestamp: float
    discovery_timestamp: float
    original_confidence: float
    actual_importance: float
    evidence_type: str
    evidence_text: str
    feature_vector: np.ndarray
    discovery_method: str  # "manual_review", "expert_analysis", "court_discovery", "peer_review"
    consequence_severity: float  # 1.0-3.0 scale

class ForensicRetrospectiveLearning:
    """Retrospective learning system for FORAI - learns from missed evidence"""
    
    def __init__(self, db_path: Optional[Path] = None):
        self.db_path = db_path or Path("forai_retrospective.db")
        self.missed_evidence_history = deque(maxlen=1000)
        self.false_negative_learning_rate = 2.0  # Enhanced learning for mistakes
        self.temporal_decay_factor = 0.95
        self.similarity_threshold = 0.7
        self.learning_stats = {
            'total_missed_evidence_processed': 0,
            'patterns_reinforced': 0,
            'threshold_adjustments_made': 0,
            'avg_discovery_delay_hours': 0.0,
            'avg_consequence_severity': 0.0
        }
        self._init_database()
    
    def _init_database(self):
        """Initialize retrospective learning database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS missed_evidence (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    case_id TEXT NOT NULL,
                    original_timestamp REAL NOT NULL,
                    discovery_timestamp REAL NOT NULL,
                    original_confidence REAL NOT NULL,
                    actual_importance REAL NOT NULL,
                    evidence_type TEXT NOT NULL,
                    evidence_text TEXT NOT NULL,
                    feature_vector BLOB NOT NULL,
                    discovery_method TEXT NOT NULL,
                    consequence_severity REAL NOT NULL,
                    created_at REAL NOT NULL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_missed_evidence_case ON missed_evidence(case_id)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_missed_evidence_type ON missed_evidence(evidence_type)")
    
    def add_missed_evidence(self, missed_event: MissedEvidenceEvent):
        """Add newly discovered missed evidence for learning"""
        discovery_delay = (missed_event.discovery_timestamp - missed_event.original_analysis_timestamp) / 3600.0
        
        # Update statistics
        self.learning_stats['total_missed_evidence_processed'] += 1
        total = self.learning_stats['total_missed_evidence_processed']
        
        # Running average for discovery delay
        current_avg = self.learning_stats['avg_discovery_delay_hours']
        self.learning_stats['avg_discovery_delay_hours'] = (current_avg * (total - 1) + discovery_delay) / total
        
        # Running average for consequence severity
        current_severity = self.learning_stats['avg_consequence_severity']
        self.learning_stats['avg_consequence_severity'] = (current_severity * (total - 1) + missed_event.consequence_severity) / total
        
        # Store in memory and database
        self.missed_evidence_history.append(missed_event)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO missed_evidence 
                (case_id, original_timestamp, discovery_timestamp, original_confidence, 
                 actual_importance, evidence_type, evidence_text, feature_vector, 
                 discovery_method, consequence_severity, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                missed_event.case_id,
                missed_event.original_analysis_timestamp,
                missed_event.discovery_timestamp,
                missed_event.original_confidence,
                missed_event.actual_importance,
                missed_event.evidence_type,
                missed_event.evidence_text,
                missed_event.feature_vector.tobytes(),
                missed_event.discovery_method,
                missed_event.consequence_severity,
                time.time()
            ))
        
        print(f"ðŸ“š Retrospective Learning: Added missed {missed_event.evidence_type} evidence "
              f"(delay: {discovery_delay:.1f}h, severity: {missed_event.consequence_severity:.1f})")
    
    def find_similar_missed_evidence_patterns(self, current_features: np.ndarray, evidence_type: str = None) -> List[MissedEvidenceEvent]:
        """Find similar patterns to current evidence in missed evidence history"""
        similar_events = []
        
        for missed_event in self.missed_evidence_history:
            # Filter by evidence type if specified
            if evidence_type and missed_event.evidence_type != evidence_type:
                continue
                
            # Calculate cosine similarity
            similarity = self._calculate_cosine_similarity(current_features, missed_event.feature_vector)
            
            if similarity > self.similarity_threshold:
                similar_events.append((similarity, missed_event))
        
        # Sort by similarity (highest first)
        similar_events.sort(key=lambda x: x[0], reverse=True)
        return [event for _, event in similar_events[:5]]  # Top 5 similar events
    
    def calculate_confidence_adjustment(self, current_features: np.ndarray, base_confidence: float, evidence_type: str = None) -> float:
        """Calculate confidence adjustment based on similar missed evidence"""
        similar_events = self.find_similar_missed_evidence_patterns(current_features, evidence_type)
        
        if not similar_events:
            return base_confidence
        
        # Calculate weighted adjustment
        total_weight = 0.0
        weighted_adjustment = 0.0
        
        for missed_event in similar_events:
            similarity = self._calculate_cosine_similarity(current_features, missed_event.feature_vector)
            weight = similarity * missed_event.consequence_severity
            
            # Adjustment should increase confidence for patterns similar to missed evidence
            adjustment = (missed_event.actual_importance - missed_event.original_confidence) * weight
            
            weighted_adjustment += adjustment
            total_weight += weight
        
        if total_weight > 0.0:
            final_adjustment = weighted_adjustment / total_weight
            adjusted_confidence = max(0.0, min(1.0, base_confidence + final_adjustment * self.false_negative_learning_rate))
            
            if final_adjustment > 0.1:
                print(f"ðŸŽ¯ Retrospective Adjustment: {evidence_type} confidence {base_confidence:.3f} â†’ {adjusted_confidence:.3f} "
                      f"(+{final_adjustment:.3f} from {len(similar_events)} similar missed patterns)")
            
            return adjusted_confidence
        
        return base_confidence
    
    def _calculate_cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """Calculate cosine similarity between two vectors"""
        if vec1.shape != vec2.shape:
            min_len = min(len(vec1), len(vec2))
            vec1 = vec1[:min_len]
            vec2 = vec2[:min_len]
        
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return np.dot(vec1, vec2) / (norm1 * norm2)
    
    def get_learning_stats(self) -> Dict[str, Any]:
        """Get retrospective learning statistics"""
        return self.learning_stats.copy()
    
    def cleanup_old_events(self, retention_days: int = 365):
        """Remove old missed evidence events beyond retention period"""
        cutoff_timestamp = time.time() - (retention_days * 86400)
        
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("DELETE FROM missed_evidence WHERE created_at < ?", (cutoff_timestamp,))
            deleted_count = cursor.rowcount
            
        if deleted_count > 0:
            print(f"ðŸ§¹ Cleaned up {deleted_count} old retrospective learning events")

class ForensicAdaptiveThresholds:
    """Adaptive threshold system for forensic evidence confidence scoring"""
    
    def __init__(self, db_path: Optional[Path] = None):
        self.db_path = db_path or Path("forai_thresholds.db")
        
        # Evidence type specific thresholds
        self.evidence_thresholds = {
            'registry': 0.25,      # Lower threshold - registry evidence often critical
            'network': 0.30,       # Moderate threshold
            'usb': 0.20,          # Very low - USB evidence often crucial
            'browser': 0.40,      # Higher - browser evidence can be noisy
            'file_system': 0.30,  # Moderate threshold
            'process': 0.25,      # Lower - process evidence important
            'log': 0.35,          # Moderate - logs can be verbose
            'memory': 0.20,       # Very low - memory evidence rare but critical
            'email': 0.30,        # Moderate threshold
            'default': 0.30       # Default threshold
        }
        
        # Confidence level multipliers
        self.confidence_multipliers = {
            'very_high': 0.8,     # Lower threshold for high confidence
            'high': 0.9,          # Slightly lower
            'medium': 1.0,        # No adjustment
            'low': 1.2,           # Higher threshold for low confidence
            'very_low': 1.5       # Much higher threshold
        }
        
        # Performance tracking
        self.performance_history = defaultdict(lambda: {
            'true_positives': 0,
            'false_positives': 0,
            'true_negatives': 0,
            'false_negatives': 0,
            'recent_accuracy': 0.0,
            'recent_precision': 0.0,
            'recent_recall': 0.0
        })
        
        # Adjustment parameters
        self.learning_rate = 0.15
        self.min_threshold = 0.05
        self.max_threshold = 0.85
        
        self._init_database()
    
    def _init_database(self):
        """Initialize adaptive thresholds database"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS threshold_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    evidence_type TEXT NOT NULL,
                    threshold_value REAL NOT NULL,
                    performance_accuracy REAL NOT NULL,
                    performance_precision REAL NOT NULL,
                    performance_recall REAL NOT NULL,
                    adjustment_reason TEXT,
                    timestamp REAL NOT NULL
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_threshold_type ON threshold_history(evidence_type)")
    
    def assess_evidence_confidence(self, evidence_type: str, base_similarity: float, feature_confidence: str = 'medium') -> Dict[str, Any]:
        """Assess evidence and determine confidence with adaptive thresholds"""
        
        # Get base threshold for evidence type
        base_threshold = self.evidence_thresholds.get(evidence_type, self.evidence_thresholds['default'])
        
        # Apply confidence multiplier
        confidence_multiplier = self.confidence_multipliers.get(feature_confidence, 1.0)
        adjusted_threshold = max(self.min_threshold, min(self.max_threshold, base_threshold * confidence_multiplier))
        
        # Determine if evidence meets threshold
        evidence_detected = base_similarity > adjusted_threshold
        
        # Calculate confidence score
        if evidence_detected:
            # Evidence detected - confidence based on how far above threshold
            excess = base_similarity - adjusted_threshold
            max_excess = 1.0 - adjusted_threshold
            confidence_score = 0.5 + (excess / max_excess) * 0.5 if max_excess > 0 else 1.0
        else:
            # Evidence not detected - confidence based on how far below threshold
            deficit = adjusted_threshold - base_similarity
            confidence_score = 0.5 + (deficit / adjusted_threshold) * 0.5 if adjusted_threshold > 0 else 1.0
        
        return {
            'evidence_type': evidence_type,
            'base_similarity': base_similarity,
            'base_threshold': base_threshold,
            'adjusted_threshold': adjusted_threshold,
            'confidence_multiplier': confidence_multiplier,
            'evidence_detected': evidence_detected,
            'confidence_score': confidence_score,
            'feature_confidence': feature_confidence
        }
    
    def update_performance(self, evidence_type: str, assessment: Dict[str, Any], actual_importance: bool):
        """Update performance metrics and adjust thresholds"""
        
        # Update performance counters
        perf = self.performance_history[evidence_type]
        
        if assessment['evidence_detected'] and actual_importance:
            perf['true_positives'] += 1
        elif assessment['evidence_detected'] and not actual_importance:
            perf['false_positives'] += 1
        elif not assessment['evidence_detected'] and not actual_importance:
            perf['true_negatives'] += 1
        elif not assessment['evidence_detected'] and actual_importance:
            perf['false_negatives'] += 1
        
        # Recalculate metrics
        total = sum([perf['true_positives'], perf['false_positives'], 
                    perf['true_negatives'], perf['false_negatives']])
        
        if total > 0:
            perf['recent_accuracy'] = (perf['true_positives'] + perf['true_negatives']) / total
        
        predicted_positives = perf['true_positives'] + perf['false_positives']
        if predicted_positives > 0:
            perf['recent_precision'] = perf['true_positives'] / predicted_positives
        
        actual_positives = perf['true_positives'] + perf['false_negatives']
        if actual_positives > 0:
            perf['recent_recall'] = perf['true_positives'] / actual_positives
        
        # Adjust threshold based on performance
        self._adjust_threshold(evidence_type, assessment, actual_importance)
    
    def _adjust_threshold(self, evidence_type: str, assessment: Dict[str, Any], actual_importance: bool):
        """Adjust threshold based on performance feedback"""
        current_threshold = assessment['adjusted_threshold']
        
        if assessment['evidence_detected'] and not actual_importance:
            # False positive - increase threshold moderately
            adjustment = self.learning_rate * 0.3
            reason = "false_positive_reduction"
        elif not assessment['evidence_detected'] and actual_importance:
            # False negative - CRITICAL: decrease threshold aggressively
            confidence_gap = current_threshold - assessment['base_similarity']
            adjustment = -self.learning_rate * (1.0 + confidence_gap * 2.0)
            adjustment = max(adjustment, -0.15)  # Cap at 15% reduction
            reason = "false_negative_correction"
        else:
            # Correct prediction - small adjustment toward optimal
            if actual_importance:
                optimal_threshold = assessment['base_similarity'] * 0.9
            else:
                optimal_threshold = assessment['base_similarity'] * 1.1
            adjustment = (optimal_threshold - current_threshold) * self.learning_rate * 0.1
            reason = "optimization"
        
        # Apply adjustment
        new_threshold = max(self.min_threshold, min(self.max_threshold, 
                                           self.evidence_thresholds[evidence_type] + adjustment))
        
        if abs(adjustment) > 0.01:  # Only log significant adjustments
            print(f"ðŸŽ›ï¸  Threshold Adjustment: {evidence_type} {self.evidence_thresholds[evidence_type]:.3f} â†’ {new_threshold:.3f} "
                  f"({reason})")
            
            self.evidence_thresholds[evidence_type] = new_threshold
            
            # Log to database
            with sqlite3.connect(self.db_path) as conn:
                perf = self.performance_history[evidence_type]
                conn.execute("""
                    INSERT INTO threshold_history 
                    (evidence_type, threshold_value, performance_accuracy, performance_precision, 
                     performance_recall, adjustment_reason, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    evidence_type, new_threshold, perf['recent_accuracy'],
                    perf['recent_precision'], perf['recent_recall'], reason, time.time()
                ))
    
    def get_threshold_for_evidence_type(self, evidence_type: str) -> float:
        """Get current threshold for specific evidence type"""
        return self.evidence_thresholds.get(evidence_type, self.evidence_thresholds['default'])
    
    def get_performance_stats(self) -> Dict[str, Dict[str, Any]]:
        """Get performance statistics for all evidence types"""
        return dict(self.performance_history)

class ForensicEnhancedFeatureExtractor:
    """Enhanced feature extraction for forensic evidence analysis"""
    
    def __init__(self):
        # Forensic-specific patterns
        self.anti_forensic_patterns = [
            r'sdelete|cipher\s+/w|wipe|shred|bleachbit',
            r'timestomp|setmace|touch\s+-[amt]',
            r'log\s*clear|eventlog.*clear|wevtutil.*cl',
            r'history.*clear|bash_history.*>/dev/null',
            r'secure.*delete|permanent.*delete'
        ]
        
        self.data_exfiltration_patterns = [
            r'ftp|sftp|scp|rsync|wget|curl',
            r'compress|zip|rar|7z|tar\.gz',
            r'base64|encode|encrypt|cipher',
            r'email.*attach|smtp|sendmail',
            r'cloud.*upload|dropbox|gdrive|onedrive'
        ]
        
        self.temporal_anomaly_patterns = [
            r'batch|scheduled|cron|at\s+\d+',
            r'midnight|3am|4am|weekend',
            r'rapid.*succession|bulk.*operation',
            r'automated|script|programmatic'
        ]
        
        self.privilege_escalation_patterns = [
            r'runas|sudo|su\s+root|admin',
            r'uac.*bypass|privilege.*escalat',
            r'token.*impersonat|access.*token',
            r'system.*account|service.*account'
        ]
        
        # Compile regex patterns
        self.compiled_patterns = {
            'anti_forensic': [re.compile(p, re.IGNORECASE) for p in self.anti_forensic_patterns],
            'data_exfiltration': [re.compile(p, re.IGNORECASE) for p in self.data_exfiltration_patterns],
            'temporal_anomaly': [re.compile(p, re.IGNORECASE) for p in self.temporal_anomaly_patterns],
            'privilege_escalation': [re.compile(p, re.IGNORECASE) for p in self.privilege_escalation_patterns]
        }
    
    def extract_forensic_features(self, evidence_text: str, evidence_metadata: Dict[str, Any] = None) -> np.ndarray:
        """Extract comprehensive forensic-specific features"""
        features = np.zeros(32)
        
        if not evidence_text:
            return features
        
        text_lower = evidence_text.lower()
        metadata = evidence_metadata or {}
        
        # Features 0-3: Basic text characteristics
        features[0] = min(len(evidence_text) / 1000.0, 1.0)  # Text length (normalized)
        features[1] = len(evidence_text.split()) / 100.0     # Word count (normalized)
        features[2] = self._calculate_entropy(evidence_text)  # Text entropy
        features[3] = len(set(evidence_text.lower().split())) / max(len(evidence_text.split()), 1)  # Vocabulary diversity
        
        # Features 4-7: Anti-forensic activity detection
        features[4] = self._pattern_match_score(text_lower, self.compiled_patterns['anti_forensic'])
        features[5] = self._detect_timestamp_manipulation(evidence_text)
        features[6] = self._detect_log_clearing(evidence_text)
        features[7] = self._detect_data_destruction(evidence_text)
        
        # Features 8-11: Data exfiltration indicators
        features[8] = self._pattern_match_score(text_lower, self.compiled_patterns['data_exfiltration'])
        features[9] = self._detect_large_transfers(evidence_text, metadata)
        features[10] = self._detect_compression_activity(evidence_text)
        features[11] = self._detect_external_communication(evidence_text)
        
        # Features 12-15: Temporal anomaly detection
        features[12] = self._pattern_match_score(text_lower, self.compiled_patterns['temporal_anomaly'])
        features[13] = self._detect_off_hours_activity(evidence_text, metadata)
        features[14] = self._detect_batch_operations(evidence_text)
        features[15] = self._detect_rapid_succession(evidence_text, metadata)
        
        # Features 16-19: Privilege escalation and access
        features[16] = self._pattern_match_score(text_lower, self.compiled_patterns['privilege_escalation'])
        features[17] = self._detect_admin_access(evidence_text)
        features[18] = self._detect_service_account_usage(evidence_text)
        features[19] = self._detect_token_manipulation(evidence_text)
        
        # Features 20-23: File system activity
        features[20] = self._detect_file_creation_patterns(evidence_text)
        features[21] = self._detect_file_deletion_patterns(evidence_text)
        features[22] = self._detect_file_modification_patterns(evidence_text)
        features[23] = self._detect_hidden_file_activity(evidence_text)
        
        # Features 24-27: Network and communication
        features[24] = self._detect_network_connections(evidence_text)
        features[25] = self._detect_dns_queries(evidence_text)
        features[26] = self._detect_port_scanning(evidence_text)
        features[27] = self._detect_encrypted_communication(evidence_text)
        
        # Features 28-31: System and process activity
        features[28] = self._detect_process_injection(evidence_text)
        features[29] = self._detect_registry_manipulation(evidence_text)
        features[30] = self._detect_service_manipulation(evidence_text)
        features[31] = self._calculate_overall_suspicion_score(features[:31])
        
        return features
    
    def _pattern_match_score(self, text: str, patterns: List[re.Pattern]) -> float:
        """Calculate pattern match score for given patterns"""
        matches = 0
        total_patterns = len(patterns)
        
        for pattern in patterns:
            if pattern.search(text):
                matches += 1
        
        return matches / max(total_patterns, 1)
    
    def _calculate_entropy(self, text: str) -> float:
        """Calculate Shannon entropy of text"""
        if not text:
            return 0.0
        
        # Count character frequencies
        char_counts = defaultdict(int)
        for char in text:
            char_counts[char] += 1
        
        # Calculate entropy
        text_len = len(text)
        entropy = 0.0
        
        for count in char_counts.values():
            probability = count / text_len
            if probability > 0:
                entropy -= probability * np.log2(probability)
        
        # Normalize to 0-1 range (assuming max entropy ~8 for typical text)
        return min(entropy / 8.0, 1.0)
    
    def _detect_timestamp_manipulation(self, text: str) -> float:
        """Detect timestamp manipulation indicators"""
        indicators = [
            r'timestomp', r'setmace', r'touch\s+-[amt]', r'filetime',
            r'modify.*time', r'change.*timestamp', r'backdating'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.2
        
        return min(score, 1.0)
    
    def _detect_log_clearing(self, text: str) -> float:
        """Detect log clearing activities"""
        indicators = [
            r'wevtutil.*cl', r'eventlog.*clear', r'log.*clear',
            r'history.*clear', r'bash_history.*>', r'zsh_history.*>'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.25
        
        return min(score, 1.0)
    
    def _detect_data_destruction(self, text: str) -> float:
        """Detect data destruction activities"""
        indicators = [
            r'sdelete', r'cipher\s+/w', r'shred', r'wipe', r'bleachbit',
            r'secure.*delet', r'permanent.*delet', r'overwrite'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.2
        
        return min(score, 1.0)
    
    def _detect_large_transfers(self, text: str, metadata: Dict[str, Any]) -> float:
        """Detect large data transfer activities"""
        # Check for size indicators in text
        size_patterns = [
            r'(\d+)\s*(gb|mb|tb)', r'(\d+)\s*bytes', r'size.*(\d+)',
            r'transfer.*(\d+)', r'download.*(\d+)', r'upload.*(\d+)'
        ]
        
        score = 0.0
        for pattern in size_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                try:
                    if isinstance(match, tuple):
                        size_val = float(match[0])
                        unit = match[1].lower() if len(match) > 1 else 'bytes'
                    else:
                        size_val = float(match)
                        unit = 'bytes'
                    
                    # Convert to MB for comparison
                    if unit == 'gb':
                        size_mb = size_val * 1024
                    elif unit == 'tb':
                        size_mb = size_val * 1024 * 1024
                    elif unit == 'bytes':
                        size_mb = size_val / (1024 * 1024)
                    else:
                        size_mb = size_val
                    
                    # Score based on size (>100MB gets higher score)
                    if size_mb > 100:
                        score += 0.3
                    elif size_mb > 10:
                        score += 0.1
                        
                except (ValueError, IndexError):
                    continue
        
        return min(score, 1.0)
    
    def _detect_compression_activity(self, text: str) -> float:
        """Detect compression/archiving activity"""
        indicators = [
            r'zip', r'rar', r'7z', r'tar\.gz', r'compress',
            r'archive', r'pack', r'bundle'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _detect_external_communication(self, text: str) -> float:
        """Detect external communication indicators"""
        indicators = [
            r'ftp', r'sftp', r'ssh', r'scp', r'rsync',
            r'email', r'smtp', r'http', r'https', r'curl', r'wget'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.1
        
        return min(score, 1.0)
    
    def _detect_off_hours_activity(self, text: str, metadata: Dict[str, Any]) -> float:
        """Detect off-hours activity patterns"""
        # Check for time indicators in text
        time_patterns = [
            r'(\d{1,2}):(\d{2})', r'(\d{4})-(\d{2})-(\d{2})\s+(\d{1,2}):(\d{2})'
        ]
        
        score = 0.0
        for pattern in time_patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                try:
                    if len(match) >= 2:
                        hour = int(match[0] if len(match) == 2 else match[3])
                        # Off hours: 10PM - 6AM (22:00 - 06:00)
                        if hour >= 22 or hour <= 6:
                            score += 0.2
                except (ValueError, IndexError):
                    continue
        
        return min(score, 1.0)
    
    def _detect_batch_operations(self, text: str) -> float:
        """Detect batch operation indicators"""
        indicators = [
            r'batch', r'bulk', r'mass', r'multiple', r'automated',
            r'script', r'loop', r'for.*in', r'while.*do'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _detect_rapid_succession(self, text: str, metadata: Dict[str, Any]) -> float:
        """Detect rapid succession of events"""
        # Look for multiple timestamps close together
        timestamp_pattern = r'(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})'
        timestamps = re.findall(timestamp_pattern, text)
        
        if len(timestamps) < 2:
            return 0.0
        
        try:
            # Parse timestamps and check intervals
            parsed_times = []
            for ts in timestamps[:10]:  # Limit to first 10 timestamps
                parsed_times.append(datetime.strptime(ts, '%Y-%m-%d %H:%M:%S'))
            
            # Calculate intervals between consecutive timestamps
            rapid_count = 0
            for i in range(1, len(parsed_times)):
                interval = (parsed_times[i] - parsed_times[i-1]).total_seconds()
                if interval < 5:  # Less than 5 seconds apart
                    rapid_count += 1
            
            return min(rapid_count / max(len(parsed_times) - 1, 1), 1.0)
            
        except ValueError:
            return 0.0
    
    def _detect_admin_access(self, text: str) -> float:
        """Detect administrative access indicators"""
        indicators = [
            r'administrator', r'admin', r'root', r'system',
            r'elevated', r'privilege', r'runas', r'sudo'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _detect_service_account_usage(self, text: str) -> float:
        """Detect service account usage"""
        indicators = [
            r'service.*account', r'system.*account', r'local.*service',
            r'network.*service', r'nt\s+authority'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.2
        
        return min(score, 1.0)
    
    def _detect_token_manipulation(self, text: str) -> float:
        """Detect token manipulation activities"""
        indicators = [
            r'token', r'impersonat', r'access.*token', r'security.*token',
            r'privilege.*token'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.25
        
        return min(score, 1.0)
    
    def _detect_file_creation_patterns(self, text: str) -> float:
        """Detect file creation patterns"""
        indicators = [
            r'creat', r'new.*file', r'touch', r'mkdir', r'write'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.1
        
        return min(score, 1.0)
    
    def _detect_file_deletion_patterns(self, text: str) -> float:
        """Detect file deletion patterns"""
        indicators = [
            r'delet', r'remov', r'unlink', r'rm\s+', r'del\s+'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _detect_file_modification_patterns(self, text: str) -> float:
        """Detect file modification patterns"""
        indicators = [
            r'modif', r'chang', r'edit', r'updat', r'alter'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.1
        
        return min(score, 1.0)
    
    def _detect_hidden_file_activity(self, text: str) -> float:
        """Detect hidden file activity"""
        indicators = [
            r'hidden', r'\..*file', r'attrib.*\+h', r'invisible',
            r'concealed'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.2
        
        return min(score, 1.0)
    
    def _detect_network_connections(self, text: str) -> float:
        """Detect network connection activities"""
        indicators = [
            r'connect', r'socket', r'tcp', r'udp', r'port',
            r'network', r'ip.*address'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.1
        
        return min(score, 1.0)
    
    def _detect_dns_queries(self, text: str) -> float:
        """Detect DNS query activities"""
        indicators = [
            r'dns', r'nslookup', r'dig', r'resolve', r'query'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _detect_port_scanning(self, text: str) -> float:
        """Detect port scanning activities"""
        indicators = [
            r'port.*scan', r'nmap', r'masscan', r'portscan',
            r'probe.*port'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.3
        
        return min(score, 1.0)
    
    def _detect_encrypted_communication(self, text: str) -> float:
        """Detect encrypted communication"""
        indicators = [
            r'encrypt', r'ssl', r'tls', r'https', r'cipher',
            r'crypto', r'secure'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.1
        
        return min(score, 1.0)
    
    def _detect_process_injection(self, text: str) -> float:
        """Detect process injection techniques"""
        indicators = [
            r'inject', r'hollowing', r'process.*replac', r'dll.*inject',
            r'code.*inject'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.3
        
        return min(score, 1.0)
    
    def _detect_registry_manipulation(self, text: str) -> float:
        """Detect registry manipulation"""
        indicators = [
            r'registry', r'regedit', r'reg\s+add', r'reg\s+delete',
            r'hkey', r'hklm', r'hkcu'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _detect_service_manipulation(self, text: str) -> float:
        """Detect service manipulation"""
        indicators = [
            r'service', r'sc\s+', r'net\s+start', r'net\s+stop',
            r'systemctl', r'daemon'
        ]
        
        score = 0.0
        for indicator in indicators:
            if re.search(indicator, text, re.IGNORECASE):
                score += 0.15
        
        return min(score, 1.0)
    
    def _calculate_overall_suspicion_score(self, features: np.ndarray) -> float:
        """Calculate overall suspicion score from all features"""
        # Weight different categories
        weights = np.array([
            0.05,  # Text characteristics (low weight)
            0.05, 0.05, 0.05,
            0.15, 0.15, 0.15, 0.15,  # Anti-forensic (high weight)
            0.10, 0.10, 0.10, 0.10,  # Data exfiltration (medium weight)
            0.08, 0.08, 0.08, 0.08,  # Temporal anomalies (medium weight)
            0.12, 0.12, 0.12, 0.12,  # Privilege escalation (high weight)
            0.06, 0.06, 0.06, 0.06,  # File system (low-medium weight)
            0.08, 0.08, 0.08, 0.08,  # Network (medium weight)
            0.10, 0.10, 0.10         # System/process (medium weight)
        ])
        
        # Ensure weights match features length
        weights = weights[:len(features)]
        if len(weights) < len(features):
            # Pad with equal weights for remaining features
            remaining = len(features) - len(weights)
            avg_weight = (1.0 - weights.sum()) / remaining if remaining > 0 else 0.0
            weights = np.concatenate([weights, np.full(remaining, avg_weight)])
        
        # Normalize weights to sum to 1
        weights = weights / weights.sum()
        
        return np.dot(features, weights)

class ForensicAnomalyDetector:
    """Isolation Forest-based anomaly detection for forensic timeline analysis"""
    
    def __init__(self, contamination: float = 0.1, n_estimators: int = 100):
        self.contamination = contamination  # Expected proportion of anomalies
        self.n_estimators = n_estimators
        self.isolation_forest = None
        self.feature_extractor = ForensicEnhancedFeatureExtractor()
        self.is_trained = False
        self.anomaly_threshold = -0.1  # Threshold for anomaly detection
        
    def train_on_timeline(self, evidence_data: List[Dict[str, Any]]) -> bool:
        """Train the anomaly detector on forensic timeline data"""
        if not evidence_data:
            print("âš ï¸  No evidence data provided for anomaly detection training")
            return False
        
        try:
            # Extract features from evidence
            feature_vectors = []
            for evidence in evidence_data:
                text = evidence.get('summary', '') + ' ' + evidence.get('data_json', '')
                metadata = {
                    'timestamp': evidence.get('timestamp'),
                    'artifact': evidence.get('artifact'),
                    'case_id': evidence.get('case_id')
                }
                features = self.feature_extractor.extract_forensic_features(text, metadata)
                feature_vectors.append(features)
            
            if len(feature_vectors) < 10:
                print("âš ï¸  Insufficient data for anomaly detection training (need at least 10 samples)")
                return False
            
            # Train Isolation Forest
            feature_matrix = np.array(feature_vectors)
            self.isolation_forest = IsolationForest(
                contamination=self.contamination,
                n_estimators=self.n_estimators,
                random_state=42,
                n_jobs=-1
            )
            
            self.isolation_forest.fit(feature_matrix)
            self.is_trained = True
            
            print(f"ðŸŒ² Anomaly Detection: Trained on {len(feature_vectors)} evidence samples")
            return True
            
        except Exception as e:
            print(f"âŒ Error training anomaly detector: {e}")
            return False
    
    def detect_anomalies(self, evidence_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect anomalous patterns in forensic evidence"""
        if not self.is_trained:
            print("âš ï¸  Anomaly detector not trained. Call train_on_timeline() first.")
            return []
        
        anomalies = []
        
        try:
            for evidence in evidence_data:
                text = evidence.get('summary', '') + ' ' + evidence.get('data_json', '')
                metadata = {
                    'timestamp': evidence.get('timestamp'),
                    'artifact': evidence.get('artifact'),
                    'case_id': evidence.get('case_id')
                }
                
                # Extract features
                features = self.feature_extractor.extract_forensic_features(text, metadata)
                feature_vector = features.reshape(1, -1)
                
                # Get anomaly score
                anomaly_score = self.isolation_forest.decision_function(feature_vector)[0]
                is_anomaly = anomaly_score < self.anomaly_threshold
                
                if is_anomaly:
                    anomaly_info = {
                        'evidence_id': evidence.get('id'),
                        'case_id': evidence.get('case_id'),
                        'timestamp': evidence.get('timestamp'),
                        'artifact': evidence.get('artifact'),
                        'summary': evidence.get('summary', '')[:200],  # Truncate for display
                        'anomaly_score': float(anomaly_score),
                        'anomaly_severity': self._calculate_anomaly_severity(anomaly_score),
                        'suspicious_features': self._identify_suspicious_features(features),
                        'confidence': abs(anomaly_score) / 2.0  # Normalize confidence
                    }
                    anomalies.append(anomaly_info)
            
            if anomalies:
                # Sort by anomaly severity (most anomalous first)
                anomalies.sort(key=lambda x: x['anomaly_score'])
                print(f"ðŸš¨ Detected {len(anomalies)} anomalous evidence patterns")
            
            return anomalies
            
        except Exception as e:
            print(f"âŒ Error detecting anomalies: {e}")
            return []
    
    def _calculate_anomaly_severity(self, anomaly_score: float) -> str:
        """Calculate severity level based on anomaly score"""
        if anomaly_score < -0.3:
            return "CRITICAL"
        elif anomaly_score < -0.2:
            return "HIGH"
        elif anomaly_score < -0.1:
            return "MEDIUM"
        else:
            return "LOW"
    
    def _identify_suspicious_features(self, features: np.ndarray) -> List[str]:
        """Identify which features contribute most to anomaly detection"""
        suspicious_features = []
        
        # Define feature categories and their indices
        feature_categories = {
            'anti_forensic': [4, 5, 6, 7],
            'data_exfiltration': [8, 9, 10, 11],
            'temporal_anomaly': [12, 13, 14, 15],
            'privilege_escalation': [16, 17, 18, 19],
            'file_system': [20, 21, 22, 23],
            'network': [24, 25, 26, 27],
            'system_process': [28, 29, 30, 31]
        }
        
        # Check which categories have high feature values
        for category, indices in feature_categories.items():
            category_score = np.mean([features[i] for i in indices if i < len(features)])
            if category_score > 0.3:  # Threshold for suspicious activity
                suspicious_features.append(category.replace('_', ' ').title())
        
        return suspicious_features
    
    def analyze_timeline_anomalies(self, case_id: str) -> Dict[str, Any]:
        """Analyze timeline for anomalous patterns and provide insights"""
        if not self.is_trained:
            return {'error': 'Anomaly detector not trained'}
        
        try:
            # Get evidence from database
            with get_database_connection() as conn:
                cursor = conn.execute("""
                    SELECT * FROM evidence 
                    WHERE case_id = ? 
                    ORDER BY timestamp
                """, (case_id,))
                evidence_data = [dict(row) for row in cursor.fetchall()]
            
            if not evidence_data:
                return {'error': f'No evidence found for case {case_id}'}
            
            # Detect anomalies
            anomalies = self.detect_anomalies(evidence_data)
            
            # Analyze patterns
            analysis = {
                'case_id': case_id,
                'total_evidence': len(evidence_data),
                'anomalies_detected': len(anomalies),
                'anomaly_rate': len(anomalies) / len(evidence_data) if evidence_data else 0,
                'anomalies': anomalies,
                'timeline_insights': self._generate_timeline_insights(anomalies),
                'recommendations': self._generate_recommendations(anomalies)
            }
            
            return analysis
            
        except Exception as e:
            return {'error': f'Error analyzing timeline anomalies: {e}'}
    
    def _generate_timeline_insights(self, anomalies: List[Dict[str, Any]]) -> List[str]:
        """Generate insights about timeline anomalies"""
        insights = []
        
        if not anomalies:
            insights.append("No significant anomalies detected in timeline")
            return insights
        
        # Analyze anomaly distribution
        severity_counts = defaultdict(int)
        artifact_counts = defaultdict(int)
        feature_counts = defaultdict(int)
        
        for anomaly in anomalies:
            severity_counts[anomaly['anomaly_severity']] += 1
            artifact_counts[anomaly['artifact']] += 1
            for feature in anomaly['suspicious_features']:
                feature_counts[feature] += 1
        
        # Generate insights
        if severity_counts['CRITICAL'] > 0:
            insights.append(f"ðŸš¨ {severity_counts['CRITICAL']} CRITICAL anomalies require immediate attention")
        
        if severity_counts['HIGH'] > 0:
            insights.append(f"âš ï¸  {severity_counts['HIGH']} HIGH severity anomalies detected")
        
        # Most anomalous artifact types
        if artifact_counts:
            top_artifact = max(artifact_counts.items(), key=lambda x: x[1])
            insights.append(f"ðŸ“ Most anomalous artifact type: {top_artifact[0]} ({top_artifact[1]} anomalies)")
        
        # Most suspicious feature categories
        if feature_counts:
            top_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            feature_list = [f"{feat} ({count})" for feat, count in top_features]
            insights.append(f"ðŸ” Top suspicious activities: {', '.join(feature_list)}")
        
        return insights
    
    def _generate_recommendations(self, anomalies: List[Dict[str, Any]]) -> List[str]:
        """Generate recommendations based on detected anomalies"""
        recommendations = []
        
        if not anomalies:
            recommendations.append("Continue monitoring for unusual patterns")
            return recommendations
        
        # Analyze patterns for recommendations
        critical_count = sum(1 for a in anomalies if a['anomaly_severity'] == 'CRITICAL')
        high_count = sum(1 for a in anomalies if a['anomaly_severity'] == 'HIGH')
        
        if critical_count > 0:
            recommendations.append(f"ðŸš¨ URGENT: Investigate {critical_count} critical anomalies immediately")
            recommendations.append("ðŸ” Perform deep forensic analysis on critical anomalies")
            recommendations.append("ðŸ“‹ Document findings for incident response")
        
        if high_count > 0:
            recommendations.append(f"âš ï¸  Review {high_count} high-severity anomalies for potential threats")
        
        # Feature-specific recommendations
        feature_counts = defaultdict(int)
        for anomaly in anomalies:
            for feature in anomaly['suspicious_features']:
                feature_counts[feature] += 1
        
        if feature_counts.get('Anti Forensic', 0) > 0:
            recommendations.append("ðŸ›¡ï¸  Evidence of anti-forensic activity detected - verify data integrity")
        
        if feature_counts.get('Data Exfiltration', 0) > 0:
            recommendations.append("ðŸ“¤ Potential data exfiltration detected - check network logs and file transfers")
        
        if feature_counts.get('Privilege Escalation', 0) > 0:
            recommendations.append("ðŸ” Privilege escalation activity detected - review user access and permissions")
        
        recommendations.append("ðŸ“Š Consider expanding timeline collection for additional context")
        recommendations.append("ðŸ”„ Re-run analysis after gathering additional evidence")
        
        return recommendations

# ============================================================================
# STANDARD FORENSIC QUESTIONS
# ============================================================================

STANDARD_FORENSIC_QUESTIONS = [
    {
        "id": "Q1",
        "question": "What is the computer name?",
        "description": "Identify the system hostname/computer name",
        "extractor": "extract_computer_identity",
        "keywords": ["computername", "computer name", "hostname", "system name"]
    },
    {
        "id": "Q2", 
        "question": "What is the computer make, model, and serial number?",
        "description": "Hardware identification details",
        "extractor": "extract_computer_identity",
        "keywords": ["make", "model", "serial", "manufacturer", "hardware"]
    },
    {
        "id": "Q3",
        "question": "What internal hard drives are present?",
        "description": "Internal storage devices and specifications",
        "extractor": "extract_hard_drives", 
        "keywords": ["internal", "hard drive", "storage", "disk", "drive"]
    },
    {
        "id": "Q4",
        "question": "What user accounts exist and their activity levels?",
        "description": "User accounts, SIDs, and usage patterns",
        "extractor": "extract_user_accounts",
        "keywords": ["user", "account", "sid", "username", "profile"]
    },
    {
        "id": "Q5",
        "question": "Who is the primary user of this system?",
        "description": "Most active user based on logon frequency and file activity",
        "extractor": "extract_user_accounts",
        "keywords": ["primary user", "main user", "most active", "owner"]
    },
    {
        "id": "Q6",
        "question": "Is there evidence of anti-forensic activity?",
        "description": "Data destruction, tampering, or evidence elimination attempts",
        "extractor": "extract_anti_forensic_activity",
        "keywords": ["anti-forensic", "data destruction", "tampering", "deletion", "wiping"]
    },
    {
        "id": "Q7",
        "question": "What USB or removable storage devices were connected?",
        "description": "External storage devices and connection history",
        "extractor": "extract_usb_devices",
        "keywords": ["usb", "removable", "external", "storage", "device"]
    },
    {
        "id": "Q8",
        "question": "What files were transferred to/from removable storage?",
        "description": "File transfer activity involving external devices",
        "extractor": "extract_file_transfers",
        "keywords": ["file transfer", "copy", "move", "removable", "external"]
    },
    {
        "id": "Q9",
        "question": "Is there evidence of cloud storage usage?",
        "description": "Cloud storage applications and sync activity",
        "extractor": "extract_file_transfers",
        "keywords": ["cloud", "dropbox", "onedrive", "google drive", "sync"]
    },
    {
        "id": "Q10",
        "question": "Are there any screenshot artifacts?",
        "description": "Screenshot files and screen capture activity",
        "extractor": "extract_screenshots",
        "keywords": ["screenshot", "screen capture", "snipping", "print screen"]
    },
    {
        "id": "Q11",
        "question": "What documents were printed?",
        "description": "Print job history and document printing activity",
        "extractor": "extract_print_jobs",
        "keywords": ["print", "printer", "document", "job", "spool"]
    },
    {
        "id": "Q12",
        "question": "What software was installed or modified?",
        "description": "Software installation, updates, and modification history",
        "extractor": "extract_software_changes",
        "keywords": ["software", "install", "program", "application", "modify"]
    }
]

# ============================================================================
# MACHINE LEARNING ENHANCEMENT SYSTEM
# ============================================================================

class ForensicMLAnalyzer:
    """Machine Learning analyzer for enhanced forensic question accuracy"""
    
    def __init__(self):
        self.isolation_forests = {}  # Per-question isolation forests
        self.gradient_optimizers = {}  # Per-question gradient descent optimizers
        self.feature_scalers = {}  # Per-question feature scalers
        self.ml_available = ML_AVAILABLE
        self.training_data = {}  # Historical training data per question
        
    def extract_behavioral_features(self, evidence_items: List[Dict], question_id: str) -> np.ndarray:
        """Extract behavioral features for ML analysis based on question type"""
        if not self.ml_available:
            return np.array([])
            
        features = []
        
        if question_id in ['Q6']:  # Anti-forensic activity
            features.extend(self._extract_antiforensic_features(evidence_items))
        elif question_id in ['Q7', 'Q8']:  # USB/File transfers
            features.extend(self._extract_transfer_features(evidence_items))
        elif question_id in ['Q9']:  # Cloud storage
            features.extend(self._extract_cloud_features(evidence_items))
        elif question_id in ['Q10', 'Q11', 'Q12']:  # Screenshots, printing, software
            features.extend(self._extract_activity_features(evidence_items))
        else:
            # For ad-hoc questions, extract general behavioral features
            features.extend(self._extract_general_features(evidence_items))
            
        return np.array(features) if features else np.array([0.0])
    
    def _extract_antiforensic_features(self, evidence_items: List[Dict]) -> List[float]:
        """Extract features specific to anti-forensic activity detection"""
        features = []
        
        # Temporal gap analysis
        timestamps = [item.get('timestamp', '') for item in evidence_items if item.get('timestamp')]
        if len(timestamps) > 1:
            # Calculate gaps between events
            time_gaps = self._calculate_time_gaps(timestamps)
            features.extend([
                np.mean(time_gaps) if time_gaps else 0.0,
                np.std(time_gaps) if time_gaps else 0.0,
                max(time_gaps) if time_gaps else 0.0
            ])
        else:
            features.extend([0.0, 0.0, 0.0])
        
        # Log clearing indicators
        log_clear_count = sum(1 for item in evidence_items 
                             if any(term in str(item.get('data_json', '')).lower() 
                                   for term in ['eventlog', 'clear', 'wevtutil']))
        features.append(log_clear_count)
        
        # File deletion patterns
        deletion_count = sum(1 for item in evidence_items 
                           if any(term in str(item.get('data_json', '')).lower() 
                                 for term in ['delete', 'remove', 'sdelete']))
        features.append(deletion_count)
        
        # Timestamp modification indicators
        timestamp_mod_count = sum(1 for item in evidence_items 
                                if any(term in str(item.get('data_json', '')).lower() 
                                      for term in ['timestamp', 'filetime', 'touch']))
        features.append(timestamp_mod_count)
        
        # Registry modification patterns
        registry_mod_count = sum(1 for item in evidence_items 
                               if 'registry' in str(item.get('data_json', '')).lower())
        features.append(registry_mod_count)
        
        return features
    
    def _extract_transfer_features(self, evidence_items: List[Dict]) -> List[float]:
        """Extract features for USB/file transfer analysis"""
        features = []
        
        # Volume of data transferred
        total_size = 0
        file_count = 0
        for item in evidence_items:
            data_json = str(item.get('data_json', ''))
            # Extract file sizes if available
            size_match = re.search(r'size["\s]*[:=]["\s]*(\d+)', data_json, re.IGNORECASE)
            if size_match:
                total_size += int(size_match.group(1))
                file_count += 1
        
        features.extend([total_size, file_count])
        
        # Transfer timing patterns
        timestamps = [item.get('timestamp', '') for item in evidence_items if item.get('timestamp')]
        if len(timestamps) > 1:
            time_gaps = self._calculate_time_gaps(timestamps)
            # Rapid transfers might indicate bulk data exfiltration
            rapid_transfers = sum(1 for gap in time_gaps if gap < 60)  # Less than 1 minute apart
            features.append(rapid_transfers)
        else:
            features.append(0)
        
        # File type diversity
        file_extensions = set()
        for item in evidence_items:
            data_json = str(item.get('data_json', ''))
            ext_matches = re.findall(r'\.([a-zA-Z0-9]{2,4})', data_json)
            file_extensions.update(ext_matches)
        features.append(len(file_extensions))
        
        return features
    
    def _extract_cloud_features(self, evidence_items: List[Dict]) -> List[float]:
        """Extract features for cloud storage analysis"""
        features = []
        
        # Cloud service diversity
        cloud_services = set()
        for item in evidence_items:
            data_json = str(item.get('data_json', '')).lower()
            if 'dropbox' in data_json:
                cloud_services.add('dropbox')
            elif 'onedrive' in data_json:
                cloud_services.add('onedrive')
            elif 'google' in data_json and 'drive' in data_json:
                cloud_services.add('googledrive')
            elif 'icloud' in data_json:
                cloud_services.add('icloud')
        
        features.append(len(cloud_services))
        
        # Sync activity frequency
        sync_count = sum(1 for item in evidence_items 
                        if 'sync' in str(item.get('data_json', '')).lower())
        features.append(sync_count)
        
        # Upload vs download patterns
        upload_count = sum(1 for item in evidence_items 
                          if 'upload' in str(item.get('data_json', '')).lower())
        download_count = sum(1 for item in evidence_items 
                           if 'download' in str(item.get('data_json', '')).lower())
        features.extend([upload_count, download_count])
        
        return features
    
    def _extract_activity_features(self, evidence_items: List[Dict]) -> List[float]:
        """Extract features for general activity analysis (screenshots, printing, software)"""
        features = []
        
        # Activity frequency
        features.append(len(evidence_items))
        
        # Time distribution
        timestamps = [item.get('timestamp', '') for item in evidence_items if item.get('timestamp')]
        if timestamps:
            time_gaps = self._calculate_time_gaps(timestamps)
            features.extend([
                np.mean(time_gaps) if time_gaps else 0.0,
                np.std(time_gaps) if time_gaps else 0.0
            ])
        else:
            features.extend([0.0, 0.0])
        
        # Application diversity
        applications = set()
        for item in evidence_items:
            data_json = str(item.get('data_json', ''))
            app_match = re.search(r'application["\s]*[:=]["\s]*([^"]+)', data_json, re.IGNORECASE)
            if app_match:
                applications.add(app_match.group(1).lower())
        features.append(len(applications))
        
        return features
    
    def _extract_general_features(self, evidence_items: List[Dict]) -> List[float]:
        """Extract general behavioral features for ad-hoc questions"""
        features = []
        
        # Basic statistics
        features.append(len(evidence_items))
        
        # Temporal patterns
        timestamps = [item.get('timestamp', '') for item in evidence_items if item.get('timestamp')]
        if timestamps:
            time_gaps = self._calculate_time_gaps(timestamps)
            features.extend([
                np.mean(time_gaps) if time_gaps else 0.0,
                np.std(time_gaps) if time_gaps else 0.0,
                min(time_gaps) if time_gaps else 0.0,
                max(time_gaps) if time_gaps else 0.0
            ])
        else:
            features.extend([0.0, 0.0, 0.0, 0.0])
        
        # Content complexity
        total_content_length = sum(len(str(item.get('data_json', ''))) for item in evidence_items)
        features.append(total_content_length)
        
        # Artifact diversity
        artifacts = set(item.get('artifact', '') for item in evidence_items if item.get('artifact'))
        features.append(len(artifacts))
        
        return features
    
    def _calculate_time_gaps(self, timestamps: List[str]) -> List[float]:
        """Calculate time gaps between timestamps in seconds"""
        gaps = []
        parsed_times = []
        
        for ts in timestamps:
            try:
                # Try to parse various timestamp formats
                if 'T' in ts:
                    parsed_time = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                else:
                    parsed_time = datetime.strptime(ts, '%Y-%m-%d %H:%M:%S')
                parsed_times.append(parsed_time)
            except:
                continue
        
        if len(parsed_times) > 1:
            parsed_times.sort()
            for i in range(1, len(parsed_times)):
                gap = (parsed_times[i] - parsed_times[i-1]).total_seconds()
                gaps.append(gap)
        
        return gaps
    
    def analyze_with_isolation_forest(self, evidence_items: List[Dict], question_id: str) -> Dict[str, Any]:
        """Use Isolation Forest to identify anomalous patterns in evidence"""
        if not self.ml_available or not evidence_items:
            return {"anomaly_score": 0.0, "confidence": 0.0, "anomalous_items": []}
        
        try:
            # Extract features
            features = self.extract_behavioral_features(evidence_items, question_id)
            if len(features) == 0:
                return {"anomaly_score": 0.0, "confidence": 0.0, "anomalous_items": []}
            
            # Reshape for sklearn
            X = features.reshape(1, -1) if len(features.shape) == 1 else features
            
            # Initialize or get existing isolation forest for this question
            if question_id not in self.isolation_forests:
                self.isolation_forests[question_id] = IsolationForest(
                    contamination=0.1,  # Expect 10% anomalies
                    random_state=42,
                    n_estimators=100
                )
                
                # If we have historical data, fit the model
                if question_id in self.training_data and len(self.training_data[question_id]) > 10:
                    training_features = np.array(self.training_data[question_id])
                    self.isolation_forests[question_id].fit(training_features)
                else:
                    # Fit on current data (less reliable but functional)
                    self.isolation_forests[question_id].fit(X)
            
            # Get anomaly score
            anomaly_score = self.isolation_forests[question_id].decision_function(X)[0]
            is_anomaly = self.isolation_forests[question_id].predict(X)[0] == -1
            
            # Convert to confidence score (higher = more suspicious)
            confidence = max(0.0, min(1.0, (0.5 - anomaly_score) * 2))
            
            result = {
                "anomaly_score": float(anomaly_score),
                "confidence": float(confidence),
                "is_anomalous": bool(is_anomaly),
                "anomalous_items": evidence_items if is_anomaly else []
            }
            
            # Store features for future training
            if question_id not in self.training_data:
                self.training_data[question_id] = []
            self.training_data[question_id].append(features.tolist())
            
            return result
            
        except Exception as e:
            LOGGER.warning(f"Isolation Forest analysis failed for {question_id}: {e}")
            return {"anomaly_score": 0.0, "confidence": 0.0, "anomalous_items": []}
    
    def optimize_with_gradient_descent(self, question_id: str, evidence_items: List[Dict], 
                                     ground_truth_score: float = None) -> Dict[str, Any]:
        """Use gradient descent to optimize evidence scoring for forensic accuracy"""
        if not self.ml_available:
            return {"optimized_score": 0.5, "confidence": 0.5}
        
        try:
            # Extract features
            features = self.extract_behavioral_features(evidence_items, question_id)
            if len(features) == 0:
                return {"optimized_score": 0.5, "confidence": 0.5}
            
            # Initialize gradient optimizer for this question if needed
            if question_id not in self.gradient_optimizers:
                self.gradient_optimizers[question_id] = {
                    'weights': np.random.normal(0, 0.1, len(features)),
                    'bias': 0.0,
                    'learning_rate': 0.01,
                    'momentum': np.zeros(len(features)),
                    'momentum_decay': 0.9
                }
            
            optimizer = self.gradient_optimizers[question_id]
            
            # Calculate current score
            score = self._sigmoid(np.dot(features, optimizer['weights']) + optimizer['bias'])
            
            # If we have ground truth, update weights
            if ground_truth_score is not None:
                # Calculate gradient
                error = score - ground_truth_score
                gradient_w = error * features * score * (1 - score)
                gradient_b = error * score * (1 - score)
                
                # Update with momentum
                optimizer['momentum'] = (optimizer['momentum_decay'] * optimizer['momentum'] + 
                                       optimizer['learning_rate'] * gradient_w)
                optimizer['weights'] -= optimizer['momentum']
                optimizer['bias'] -= optimizer['learning_rate'] * gradient_b
                
                # Recalculate score with updated weights
                score = self._sigmoid(np.dot(features, optimizer['weights']) + optimizer['bias'])
            
            # Calculate confidence based on feature magnitude and weight alignment
            feature_magnitude = np.linalg.norm(features)
            weight_magnitude = np.linalg.norm(optimizer['weights'])
            confidence = min(1.0, (feature_magnitude * weight_magnitude) / (len(features) + 1))
            
            return {
                "optimized_score": float(score),
                "confidence": float(confidence),
                "feature_importance": dict(zip(range(len(features)), optimizer['weights'].tolist()))
            }
            
        except Exception as e:
            LOGGER.warning(f"Gradient descent optimization failed for {question_id}: {e}")
            return {"optimized_score": 0.5, "confidence": 0.5}
    
    def _sigmoid(self, x):
        """Sigmoid activation function with numerical stability"""
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def enhance_forensic_analysis(self, question_id: str, evidence_items: List[Dict], 
                                deterministic_result: Dict) -> Dict[str, Any]:
        """Combine deterministic results with ML enhancement for improved accuracy"""
        if not self.ml_available:
            return deterministic_result
        
        try:
            # Get ML analysis
            isolation_result = self.analyze_with_isolation_forest(evidence_items, question_id)
            gradient_result = self.optimize_with_gradient_descent(question_id, evidence_items)
            
            # Combine results
            enhanced_result = deterministic_result.copy()
            
            # Add ML insights
            enhanced_result['ml_analysis'] = {
                'anomaly_detection': isolation_result,
                'optimized_scoring': gradient_result,
                'ml_confidence': (isolation_result['confidence'] + gradient_result['confidence']) / 2
            }
            
            # Adjust overall confidence based on ML analysis
            original_confidence = enhanced_result.get('confidence', 0.5)
            ml_confidence = enhanced_result['ml_analysis']['ml_confidence']
            
            # Weight combination: 70% deterministic, 30% ML for behavioral questions
            if question_id in ['Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12']:
                combined_confidence = 0.7 * original_confidence + 0.3 * ml_confidence
            else:
                # For deterministic questions (Q1-Q5), ML has minimal impact
                combined_confidence = 0.95 * original_confidence + 0.05 * ml_confidence
            
            enhanced_result['confidence'] = combined_confidence
            enhanced_result['ml_enhanced'] = True
            
            return enhanced_result
            
        except Exception as e:
            LOGGER.warning(f"ML enhancement failed for {question_id}: {e}")
            return deterministic_result

# ============================================================================
# PERFORMANCE OPTIMIZATION: LLM SINGLETON
# ============================================================================
# Global LLM instance to avoid repeated model loads (major performance boost)
_GLOBAL_LLM = None
_LLM_LOCK = threading.Lock()

# ============================================================================
# LLM PROVIDER ABSTRACTION
# ============================================================================

class LLMProvider:
    """Abstract base class for LLM providers (local vs API)"""
    
    def __init__(self, config: dict):
        self.config = config
        self.provider_type = config.get('provider', 'local')
        
    def generate_response(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:
        """Generate response from LLM"""
        raise NotImplementedError("Subclasses must implement generate_response")
    
    def is_available(self) -> bool:
        """Check if LLM provider is available"""
        raise NotImplementedError("Subclasses must implement is_available")

class LocalLLMProvider(LLMProvider):
    """Local LLM provider using llama-cpp-python"""
    
    def __init__(self, config: dict):
        super().__init__(config)
        self.model_path = config.get('model_path')
        self.llm = None
        self._initialize()
    
    def _initialize(self):
        """Initialize local LLM model"""
        try:
            if not LLAMA_CPP_AVAILABLE:
                LOGGER.warning("llama-cpp-python not available, local LLM disabled")
                self.llm = None
                return
                
            if self.model_path and Path(self.model_path).exists():
                self.llm = Llama(
                    model_path=str(self.model_path),
                    n_ctx=2048,
                    n_threads=4,
                    verbose=False
                )
                LOGGER.info(f"Local LLM initialized: {self.model_path}")
            else:
                LOGGER.warning(f"Local LLM model not found: {self.model_path}")
        except Exception as e:
            LOGGER.error(f"Failed to initialize local LLM: {e}")
            self.llm = None
    
    def generate_response(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:
        """Generate response using local LLM"""
        if not self.llm:
            return "Local LLM not available"
        
        try:
            response = self.llm(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                stop=["Question:", "Evidence:", "\n\n"],
                echo=False
            )
            return response['choices'][0]['text'].strip()
        except Exception as e:
            LOGGER.error(f"Local LLM generation error: {e}")
            return "Error generating response"
    
    def is_available(self) -> bool:
        return self.llm is not None

class APILLMProvider(LLMProvider):
    """API-based LLM provider (OpenAI, Anthropic, etc.)"""
    
    def __init__(self, config: dict):
        super().__init__(config)
        self.api_token = config.get('api_token')
        self.model = config.get('model', 'gpt-3.5-turbo')
        self.provider_name = config.get('provider', 'openai')
        self.client = None
        self._initialize()
    
    def _initialize(self):
        """Initialize API client"""
        try:
            if self.provider_name == 'openai':
                import openai
                self.client = openai.OpenAI(api_key=self.api_token)
                LOGGER.info(f"OpenAI API client initialized with model: {self.model}")
            elif self.provider_name == 'anthropic':
                import anthropic
                self.client = anthropic.Anthropic(api_key=self.api_token)
                LOGGER.info(f"Anthropic API client initialized with model: {self.model}")
            else:
                LOGGER.error(f"Unsupported API provider: {self.provider_name}")
        except ImportError as e:
            LOGGER.error(f"Missing API client library: {e}")
            self.client = None
        except Exception as e:
            LOGGER.error(f"Failed to initialize API client: {e}")
            self.client = None
    
    def generate_response(self, prompt: str, max_tokens: int = 500, temperature: float = 0.1) -> str:
        """Generate response using API"""
        if not self.client:
            return "API client not available"
        
        try:
            if self.provider_name == 'openai':
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a forensic analysis expert. Provide accurate, concise answers based on the evidence provided."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content.strip()
            
            elif self.provider_name == 'anthropic':
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    messages=[
                        {"role": "user", "content": prompt}
                    ]
                )
                return response.content[0].text.strip()
            
        except Exception as e:
            LOGGER.error(f"API generation error: {e}")
            return "Error generating API response"
    
    def is_available(self) -> bool:
        return self.client is not None

def create_llm_provider(args) -> LLMProvider:
    """Factory function to create appropriate LLM provider"""
    if args.llm_api_token and args.llm_api_provider != 'local':
        # API-based provider
        config = {
            'provider': args.llm_api_provider,
            'api_token': args.llm_api_token,
            'model': args.llm_model
        }
        return APILLMProvider(config)
    else:
        # Local provider
        if args.llm_folder:
            model_path = args.llm_folder / args.llm_model
        else:
            # Try to find LLM folder with case-insensitive search
            model_path = _find_llm_model_path(CONFIG.base_dir, args.llm_model)
        
        config = {
            'provider': 'local',
            'model_path': str(model_path)
        }
        return LocalLLMProvider(config)

def _find_llm_model_path(base_dir: Path, model_filename: str) -> Path:
    """Find LLM model path with case-insensitive folder detection"""
    # Try common LLM folder variations
    llm_folder_variants = ["LLM", "llm", "Llm", "models", "Models"]
    
    for folder_name in llm_folder_variants:
        llm_folder = base_dir / folder_name
        if llm_folder.exists() and llm_folder.is_dir():
            model_path = llm_folder / model_filename
            if model_path.exists():
                LOGGER.info(f"Found LLM model at: {model_path}")
                return model_path
            else:
                # Try to find any .gguf file in the folder
                gguf_files = list(llm_folder.glob("*.gguf"))
                if gguf_files:
                    LOGGER.info(f"Found LLM model at: {gguf_files[0]} (using first .gguf file found)")
                    return gguf_files[0]
    
    # Fallback to default path (even if it doesn't exist)
    default_path = base_dir / "LLM" / model_filename
    LOGGER.warning(f"LLM model not found in any common locations, using default: {default_path}")
    return default_path

def get_global_llm(model_path: str = None, force_reload: bool = False):
    """Get or create global LLM instance (singleton pattern)"""
    global _GLOBAL_LLM
    
    with _LLM_LOCK:
        if _GLOBAL_LLM is None or force_reload:
            if model_path and Path(model_path).exists():
                try:
                    _GLOBAL_LLM = Llama(
                        model_path=str(model_path),
                        n_ctx=2048,  # Reduced context for faster inference
                        n_threads=min(4, psutil.cpu_count()),
                        verbose=False
                    )
                    LOGGER.info(f"Global LLM initialized: {model_path}")
                except Exception as e:
                    LOGGER.error(f"Failed to initialize global LLM: {e}")
                    _GLOBAL_LLM = None
            else:
                LOGGER.warning("LLM model path not found, LLM will be unavailable")
                _GLOBAL_LLM = None
        
        return _GLOBAL_LLM

# BHSM SEMANTIC SEARCH SYSTEM
# Advanced semantic search powered by BHSM (Bidirectional Hebbian Synaptic Memory)
# Provides superior semantic understanding with 10x faster performance than legacy text search

class AdvancedTinyLlamaEnhancer:
    """Advanced enhancement system to boost LLM accuracy for forensic analysis"""
    
    def __init__(self):
        self.forensic_examples = self._load_forensic_examples()
        self.validation_patterns = self._load_validation_patterns()
        self.confidence_threshold = 0.7
    
    def _load_forensic_examples(self) -> str:
        """Few-shot learning examples for forensic analysis"""
        return """FORENSIC ANALYSIS EXAMPLES:

EXAMPLE 1 - USB Device Analysis:
Question: "What USB devices were connected to this system?"
Evidence: Registry entries showing USB storage devices, SetupAPI logs, MountPoints data
Analysis: Based on registry artifacts, 2 USB devices were connected:
- Kingston DataTraveler 3.0 (Serial: 1A2B3C4D) first connected 2024-01-15 09:30:15
- SanDisk Cruzer Blade (Serial: 5E6F7G8H) first connected 2024-01-15 14:22:33
Both devices show multiple connection/disconnection cycles indicating regular usage.

EXAMPLE 2 - Suspicious File Activity:
Question: "Was there any suspicious file execution activity?"
Evidence: Prefetch files, Windows Event Logs, File system timeline
Analysis: Suspicious activity detected: 847 files accessed in C:\\Windows\\System32 within 2-minute window (14:30-14:32), indicating potential malware execution. Key indicators:
- Unusual process: "svchost.exe" spawned from temp directory
- Rapid file enumeration pattern consistent with data harvesting
- Network connections initiated immediately after file access

EXAMPLE 3 - User Behavior Analysis:
Question: "What was the user activity pattern during the incident timeframe?"
Evidence: User logon events, application usage logs, file access records
Analysis: User DOMAIN\\jsmith showed anomalous behavior on 2024-01-15:
- Normal logon at 08:15 (consistent with daily pattern)
- Unusual late-night activity 23:45-02:30 (outside normal hours)
- Accessed sensitive directories not typically used by this user role
- Multiple failed authentication attempts to network shares

NOW ANALYZE THE CURRENT CASE:"""

    def _load_validation_patterns(self) -> Dict[str, str]:
        """Forensic validation patterns for evidence cross-referencing"""
        return {
            "usb_insertion": r"USB.*(?:connected|inserted|mounted).*\d{4}-\d{2}-\d{2}",
            "file_execution": r"(?:executed|launched|started).*\.exe.*(?:timestamp|time)",
            "network_connection": r"(?:connection|connected).*(?:IP|address).*(?:port).*\d+",
            "registry_modification": r"registry.*(?:modified|changed|updated).*HKEY",
            "user_logon": r"(?:logon|login|authentication).*(?:user|account).*(?:success|failed)",
            "file_access": r"(?:accessed|opened|read|modified).*file.*(?:path|directory)",
            "process_creation": r"(?:process|executable).*(?:created|spawned|started)",
            "network_traffic": r"(?:traffic|packets|bytes).*(?:sent|received|transmitted)"
        }
    
    def chain_of_thought_analysis(self, question: str, evidence: str) -> str:
        """Chain-of-thought prompting for step-by-step forensic reasoning"""
        
        cot_prompt = f"""{self.forensic_examples}

FORENSIC ANALYSIS - STEP BY STEP REASONING:

Question: {question}

Evidence Available:
{evidence}

Let me analyze this systematically:

Step 1 - Identify Evidence Types:
Let me categorize what types of forensic artifacts we have...

Step 2 - Establish Timeline:
Let me organize events chronologically to understand the sequence...

Step 3 - Correlate Activities:
Let me look for relationships between different evidence items...

Step 4 - Detect Anomalies:
Let me identify any unusual patterns or suspicious activities...

Step 5 - Draw Conclusions:
Based on the evidence analysis, let me provide specific findings...

STEP-BY-STEP ANALYSIS:
Step 1:"""
        
        return cot_prompt
    
    def multi_pass_analysis(self, question: str, evidence: str, llm_instance) -> Dict[str, Any]:
        """Multi-pass analysis with different perspectives and confidence scoring"""
        
        analysis_passes = [
            ("temporal", "Focus on timeline analysis and sequence of events. Identify when activities occurred and their chronological relationships."),
            ("behavioral", "Focus on user behavior patterns and anomalies. Analyze what users did and identify unusual activities."),
            ("technical", "Focus on technical artifacts and system changes. Examine registry, files, processes, and network activities."),
            ("correlation", "Focus on relationships between evidence items. Look for connections and patterns across different artifact types.")
        ]
        
        results = {}
        for pass_type, instruction in analysis_passes:
            pass_prompt = f"""FORENSIC ANALYSIS - {pass_type.upper()} PERSPECTIVE:

{instruction}

Question: {question}

Evidence:
{evidence}

{pass_type.upper()} ANALYSIS:"""
            
            try:
                if llm_instance and llm_instance.llm:
                    response = llm_instance.llm(
                        pass_prompt,
                        max_tokens=400,
                        temperature=0.3,
                        top_p=0.9,
                        stop=["Question:", "Evidence:", "\n\nAnalysis:"],
                        echo=False
                    )
                    
                    analysis_text = response['choices'][0]['text'].strip()
                    confidence = self._calculate_confidence_score(analysis_text, evidence)
                    
                    results[pass_type] = {
                        'analysis': analysis_text,
                        'confidence': confidence,
                        'perspective': instruction
                    }
                else:
                    # Fallback structured analysis
                    results[pass_type] = {
                        'analysis': f"Structured {pass_type} analysis based on available evidence patterns",
                        'confidence': 0.6,
                        'perspective': instruction
                    }
                    
            except Exception as e:
                LOGGER.warning(f"Multi-pass analysis failed for {pass_type}: {e}")
                results[pass_type] = {
                    'analysis': f"Analysis unavailable for {pass_type} perspective",
                    'confidence': 0.0,
                    'perspective': instruction
                }
        
        return results
    
    def _calculate_confidence_score(self, analysis: str, evidence: str) -> float:
        """Enhanced confidence scoring with multiple accuracy factors"""
        
        if not analysis or len(analysis.strip()) < 20:
            return 0.0
        
        confidence = 0.4  # Lower base confidence, earn through quality
        
        # Length and detail scoring (more nuanced)
        analysis_len = len(analysis)
        if 50 <= analysis_len <= 100:
            confidence += 0.05  # Concise but informative
        elif 100 < analysis_len <= 300:
            confidence += 0.15  # Good detail level
        elif 300 < analysis_len <= 800:
            confidence += 0.10  # Comprehensive but manageable
        elif analysis_len > 1000:
            confidence -= 0.05  # May indicate hallucination
            
        # Specific evidence references with better patterns
        evidence_patterns = [
            r'\d{4}-\d{2}-\d{2}',  # Dates
            r'\d{2}:\d{2}:\d{2}',  # Times
            r'registry.*hkey',     # Registry references
            r'file.*\.exe|\.dll|\.sys',  # Executable files
            r'process.*pid|process.*id',  # Process references
            r'user.*account|user.*login',  # User references
            r'ip.*address|\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}',  # IP addresses
            r'port.*\d+|:\d{2,5}',  # Port numbers
            r'usb.*device|removable.*storage',  # USB references
            r'network.*connection|tcp|udp'  # Network references
        ]
        
        evidence_refs = sum(1 for pattern in evidence_patterns 
                          if re.search(pattern, analysis, re.I))
        confidence += min(evidence_refs * 0.04, 0.25)
        
        # Enhanced forensic terminology scoring
        forensic_terms = {
            'high_value': ['artifact', 'timeline', 'correlation', 'forensic', 'evidence'],
            'medium_value': ['anomaly', 'pattern', 'suspicious', 'analysis', 'investigation'],
            'technical': ['registry', 'filesystem', 'network', 'process', 'metadata']
        }
        
        high_count = sum(1 for term in forensic_terms['high_value'] 
                        if term.lower() in analysis.lower())
        medium_count = sum(1 for term in forensic_terms['medium_value'] 
                          if term.lower() in analysis.lower())
        tech_count = sum(1 for term in forensic_terms['technical'] 
                        if term.lower() in analysis.lower())
        
        confidence += min(high_count * 0.06, 0.18)
        confidence += min(medium_count * 0.03, 0.12)
        confidence += min(tech_count * 0.02, 0.08)
        
        # Enhanced hallucination detection
        hallucination_patterns = [
            r'\bi believe\b', r'\bi think\b', r'\bin my opinion\b', 
            r'\bi assume\b', r'\bi guess\b', r'\bi suppose\b', 
            r'\bi imagine\b', r'\bprobably\b', r'\bmaybe\b', 
            r'\bmight be\b', r'\bseems like\b', r'\bappears to be\b'
        ]
        
        hallucination_count = sum(1 for pattern in hallucination_patterns 
                                if re.search(pattern, analysis, re.I))
        confidence -= min(hallucination_count * 0.08, 0.3)
        
        # Evidence support validation
        if evidence and len(evidence) > 100:
            # Check if analysis references actual evidence content
            evidence_words = set(re.findall(r'\w+', evidence.lower()))
            analysis_words = set(re.findall(r'\w+', analysis.lower()))
            
            # Calculate overlap (excluding common words)
            common_words = {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'was', 'are', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'this', 'that', 'these', 'those'}
            
            evidence_specific = evidence_words - common_words
            analysis_specific = analysis_words - common_words
            
            if evidence_specific and analysis_specific:
                overlap = len(evidence_specific.intersection(analysis_specific))
                overlap_ratio = overlap / min(len(evidence_specific), len(analysis_specific))
                confidence += min(overlap_ratio * 0.15, 0.15)
        
        # Specificity bonus (concrete details vs vague statements)
        specific_indicators = [
            r'\d+\s*(bytes?|kb|mb|gb)',  # File sizes
            r'\d+\s*(files?|entries?|records?)',  # Counts
            r'serial.*number.*\w+',  # Serial numbers
            r'version.*\d+\.\d+',  # Version numbers
            r'between.*\d{2}:\d{2}.*and.*\d{2}:\d{2}'  # Time ranges
        ]
        
        specificity_count = sum(1 for pattern in specific_indicators 
                              if re.search(pattern, analysis, re.I))
        confidence += min(specificity_count * 0.05, 0.15)
        
        return max(0.0, min(1.0, confidence))
    
    def validate_against_forensic_patterns(self, analysis: str, evidence_list: List[Dict]) -> Dict[str, Any]:
        """Validate analysis claims against actual evidence using forensic patterns"""
        
        validation_results = {
            'validated_claims': [],
            'unvalidated_claims': [],
            'confidence_adjustment': 0.0
        }
        
        # Extract claims from analysis
        claims = self._extract_forensic_claims(analysis)
        
        for claim in claims:
            is_validated = False
            
            # Check each validation pattern
            for pattern_name, pattern_regex in self.validation_patterns.items():
                if re.search(pattern_regex, claim, re.I):
                    # Verify claim against actual evidence
                    if self._verify_claim_in_evidence(claim, evidence_list, pattern_name):
                        validation_results['validated_claims'].append({
                            'claim': claim,
                            'pattern': pattern_name,
                            'confidence_boost': 0.1
                        })
                        validation_results['confidence_adjustment'] += 0.1
                        is_validated = True
                        break
            
            if not is_validated:
                validation_results['unvalidated_claims'].append(claim)
                validation_results['confidence_adjustment'] -= 0.05
        
        return validation_results
    
    def _extract_forensic_claims(self, analysis: str) -> List[str]:
        """Extract specific forensic claims from analysis text"""
        
        # Split into sentences and filter for forensic claims
        sentences = re.split(r'[.!?]+', analysis)
        claims = []
        
        forensic_indicators = [
            'connected', 'executed', 'accessed', 'modified', 'created', 'deleted',
            'logged', 'authenticated', 'transferred', 'downloaded', 'uploaded',
            'detected', 'found', 'identified', 'observed'
        ]
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:  # Minimum length for meaningful claims
                for indicator in forensic_indicators:
                    if indicator in sentence.lower():
                        claims.append(sentence)
                        break
        
        return claims
    
    def _verify_claim_in_evidence(self, claim: str, evidence_list: List[Dict], pattern_type: str) -> bool:
        """Verify if a claim is supported by actual evidence"""
        
        # Extract key elements from claim based on pattern type
        verification_keywords = {
            'usb_insertion': ['usb', 'device', 'storage', 'removable'],
            'file_execution': ['exe', 'process', 'executed', 'launched'],
            'network_connection': ['ip', 'port', 'connection', 'network'],
            'registry_modification': ['registry', 'hkey', 'modified'],
            'user_logon': ['logon', 'login', 'user', 'authentication'],
            'file_access': ['file', 'accessed', 'opened', 'path'],
            'process_creation': ['process', 'created', 'spawned'],
            'network_traffic': ['traffic', 'bytes', 'packets']
        }
        
        keywords = verification_keywords.get(pattern_type, [])
        
        # Check if evidence supports the claim
        for evidence_item in evidence_list:
            evidence_text = str(evidence_item.get('summary', '')) + ' ' + str(evidence_item.get('data_json', ''))
            
            # Count keyword matches
            matches = sum(1 for keyword in keywords if keyword.lower() in evidence_text.lower())
            
            if matches >= 2:  # Require at least 2 keyword matches for validation
                return True
        
        return False
    
    def generate_follow_up_queries(self, initial_analysis: str, original_question: str) -> List[str]:
        """Generate follow-up queries for iterative refinement"""
        
        follow_ups = []
        
        # Extract entities and concepts from initial analysis
        entities = self._extract_entities(initial_analysis)
        
        # Generate targeted follow-up queries
        if 'usb' in initial_analysis.lower() or 'device' in initial_analysis.lower():
            follow_ups.append("USB device connection timeline and file transfer activity")
            follow_ups.append("Removable storage device usage patterns")
        
        if 'user' in initial_analysis.lower() or 'logon' in initial_analysis.lower():
            follow_ups.append("User authentication events and session activity")
            follow_ups.append("Account usage patterns and privilege escalation")
        
        if 'file' in initial_analysis.lower() or 'process' in initial_analysis.lower():
            follow_ups.append("File system modifications and process execution timeline")
            follow_ups.append("Executable files and suspicious process activity")
        
        if 'network' in initial_analysis.lower() or 'connection' in initial_analysis.lower():
            follow_ups.append("Network connections and data transfer activity")
            follow_ups.append("External communication and suspicious network traffic")
        
        # Add temporal refinement queries
        if re.search(r'\d{4}-\d{2}-\d{2}', initial_analysis):
            follow_ups.append("Activity patterns during identified timeframe")
            follow_ups.append("Correlated events within same time period")
        
        return follow_ups[:4]  # Limit to 4 follow-up queries
    
    def _extract_entities(self, text: str) -> Dict[str, List[str]]:
        """Extract forensic entities from analysis text"""
        
        entities = {
            'timestamps': re.findall(r'\d{4}-\d{2}-\d{2}[\s\w:.-]*\d{2}:\d{2}', text),
            'files': re.findall(r'[A-Za-z]:\\[^\s]+|/[^\s]+', text),
            'users': re.findall(r'(?:user|account)[\s:]+([A-Za-z0-9_\\.-]+)', text, re.I),
            'processes': re.findall(r'([A-Za-z0-9_.-]+\.exe)', text, re.I),
            'ips': re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', text),
            'devices': re.findall(r'(?:USB|device)[\s:]+([A-Za-z0-9\s_.-]+)', text, re.I)
        }
        
        return entities
    
    def ensemble_analysis(self, multi_pass_results: Dict[str, Any]) -> str:
        """Combine multiple analysis approaches using weighted voting"""
        
        # Weight perspectives based on confidence scores
        weighted_analyses = []
        total_weight = 0
        
        for perspective, result in multi_pass_results.items():
            confidence = result.get('confidence', 0.5)
            analysis = result.get('analysis', '')
            
            if confidence >= self.confidence_threshold:
                weight = confidence * self._get_perspective_multiplier(perspective)
                weighted_analyses.append((analysis, weight, perspective))
                total_weight += weight
        
        if not weighted_analyses:
            return "Insufficient confidence in analysis results"
        
        # Synthesize weighted results
        synthesis = self._synthesize_weighted_analyses(weighted_analyses, total_weight)
        
        return synthesis
    
    def _get_perspective_multiplier(self, perspective: str) -> float:
        """Get multiplier for different analysis perspectives"""
        multipliers = {
            'temporal': 1.2,    # Timeline analysis is crucial in forensics
            'correlation': 1.1, # Correlation helps connect evidence
            'technical': 1.0,   # Technical analysis is standard
            'behavioral': 0.9   # Behavioral analysis is supportive
        }
        return multipliers.get(perspective, 1.0)
    
    def _synthesize_weighted_analyses(self, weighted_analyses: List[Tuple], total_weight: float) -> str:
        """Synthesize multiple weighted analyses into coherent result"""
        
        synthesis_parts = []
        
        # Sort by weight (highest confidence first)
        weighted_analyses.sort(key=lambda x: x[1], reverse=True)
        
        synthesis_parts.append("COMPREHENSIVE FORENSIC ANALYSIS:")
        synthesis_parts.append("")
        
        for analysis, weight, perspective in weighted_analyses:
            confidence_pct = int((weight / total_weight) * 100) if total_weight > 0 else 0
            synthesis_parts.append(f"{perspective.upper()} PERSPECTIVE (Confidence: {confidence_pct}%):")
            synthesis_parts.append(analysis)
            synthesis_parts.append("")
        
        # Add synthesis conclusion
        synthesis_parts.append("INTEGRATED FINDINGS:")
        synthesis_parts.append(self._generate_integrated_conclusion(weighted_analyses))
        
        return "\n".join(synthesis_parts)
    
    def _generate_integrated_conclusion(self, weighted_analyses: List[Tuple]) -> str:
        """Generate integrated conclusion from multiple perspectives"""
        
        # Extract common themes and findings
        all_text = " ".join([analysis for analysis, _, _ in weighted_analyses])
        
        # Identify key forensic findings
        key_findings = []
        
        if 'suspicious' in all_text.lower():
            key_findings.append("Suspicious activity patterns identified")
        
        if 'timeline' in all_text.lower() or re.search(r'\d{4}-\d{2}-\d{2}', all_text):
            key_findings.append("Temporal correlation established")
        
        if 'user' in all_text.lower():
            key_findings.append("User activity analysis completed")
        
        if 'evidence' in all_text.lower():
            key_findings.append("Evidence correlation performed")
        
        conclusion = "Based on multi-perspective analysis: " + "; ".join(key_findings)
        
        return conclusion
    
    def sliding_window_analysis(self, question: str, evidence_list: List[Dict], llm_instance) -> str:
        """Analyze large evidence sets using sliding windows with overlap"""
        
        if len(evidence_list) <= 15:
            # Use standard analysis for small evidence sets
            return None
        
        window_size = 15
        overlap = 5
        window_analyses = []
        
        for i in range(0, len(evidence_list), window_size - overlap):
            window = evidence_list[i:i + window_size]
            window_context = self._build_window_context(window)
            
            # Analyze this window
            window_prompt = f"""FORENSIC ANALYSIS - EVIDENCE WINDOW {i//window_size + 1}:

Question: {question}

Evidence Window ({len(window)} items):
{window_context}

WINDOW ANALYSIS:"""
            
            try:
                if llm_instance and llm_instance.llm:
                    response = llm_instance.llm(
                        window_prompt,
                        max_tokens=300,
                        temperature=0.3,
                        top_p=0.9,
                        stop=["Question:", "Evidence:", "\n\nWindow"],
                        echo=False
                    )
                    
                    window_analysis = response['choices'][0]['text'].strip()
                    window_analyses.append({
                        'window_id': i//window_size + 1,
                        'analysis': window_analysis,
                        'evidence_count': len(window)
                    })
                    
            except Exception as e:
                LOGGER.warning(f"Window analysis failed for window {i//window_size + 1}: {e}")
        
        # Synthesize all window analyses
        return self._synthesize_window_analyses(window_analyses, question)
    
    def _build_window_context(self, evidence_window: List[Dict]) -> str:
        """Build optimized context for evidence window"""
        
        context_parts = []
        
        for idx, evidence in enumerate(evidence_window, 1):
            timestamp = evidence.get('timestamp', 'Unknown time')
            artifact = evidence.get('artifact', 'Unknown artifact')
            summary = evidence.get('summary', 'No summary')[:100]  # Truncate for window analysis
            
            context_parts.append(f"{idx}. [{timestamp}] {artifact}: {summary}")
        
        return "\n".join(context_parts)
    
    def _synthesize_window_analyses(self, window_analyses: List[Dict], question: str) -> str:
        """Synthesize findings across all evidence windows"""
        
        if not window_analyses:
            return "Window analysis failed - insufficient data"
        
        synthesis_parts = []
        synthesis_parts.append(f"SLIDING WINDOW ANALYSIS RESULTS ({len(window_analyses)} windows analyzed):")
        synthesis_parts.append("")
        
        # Combine findings from all windows
        all_findings = []
        for window in window_analyses:
            window_id = window['window_id']
            analysis = window['analysis']
            evidence_count = window['evidence_count']
            
            synthesis_parts.append(f"Window {window_id} ({evidence_count} evidence items):")
            synthesis_parts.append(analysis)
            synthesis_parts.append("")
            
            all_findings.append(analysis)
        
        # Generate overall synthesis
        synthesis_parts.append("OVERALL SYNTHESIS:")
        synthesis_parts.append(self._generate_window_synthesis(all_findings, question))
        
        return "\n".join(synthesis_parts)
    
    def _generate_window_synthesis(self, all_findings: List[str], question: str) -> str:
        """Generate synthesis across all window findings"""
        
        combined_text = " ".join(all_findings)
        
        # Extract common patterns
        patterns = []
        
        if 'suspicious' in combined_text.lower():
            patterns.append("Suspicious activity patterns detected across multiple time windows")
        
        if 'user' in combined_text.lower():
            patterns.append("User activity correlation identified across evidence timeline")
        
        if 'file' in combined_text.lower() or 'process' in combined_text.lower():
            patterns.append("File system and process activity patterns observed")
        
        if 'network' in combined_text.lower():
            patterns.append("Network activity correlation detected")
        
        if not patterns:
            patterns.append("Evidence patterns analyzed across temporal windows")
        
        synthesis = f"Analysis of {len(all_findings)} evidence windows reveals: " + "; ".join(patterns)
        
        return synthesis


advanced_enhancer = AdvancedTinyLlamaEnhancer()

@dataclass
class ForaiConfig:
    """Modern configuration for maximum performance"""
    
    base_dir: Path = Path("D:/FORAI")
    max_workers: int = min(8, (os.cpu_count() or 4))
    batch_size: int = 10000
    chunk_size: int = 50000
    memory_threshold: float = 0.85
    current_case_id: str = None
    
    # LLM settings - optimized for accuracy
    llm_context_size: int = 16384
    llm_max_tokens: int = 2048
    llm_temperature: float = 0.01
    llm_top_p: float = 0.9
    llm_threads: int = min(8, (os.cpu_count() or 4))
    
    # Database settings
    db_wal_mode: bool = True
    db_cache_size: int = 50000
    db_mmap_size: int = 1073741824  # 1GB
    
    def __post_init__(self):
        """Initialize directories"""
        for subdir in ["archives", "artifacts", "extracts", "LLM", "reports", "tools"]:
            (self.base_dir / subdir).mkdir(parents=True, exist_ok=True)
    
    def set_case_id(self, case_id: str):
        """Set the current case ID for database operations"""
        self.current_case_id = case_id
    
    @property
    def db_path(self) -> Path:
        """Return path to the BHSM database for the current case"""
        if self.current_case_id:
            return self.base_dir / "extracts" / f"{self.current_case_id}_bhsm.db"
        else:
            # Fallback for backward compatibility
            return self.base_dir / "extracts" / "forai.db"

CONFIG = ForaiConfig()

def setup_logging() -> logging.Logger:
    """Setup structured logging"""
    logger = logging.getLogger("FORAI")
    logger.setLevel(logging.INFO)
    logger.handlers.clear()
    
    formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(funcName)s:%(lineno)d | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    
    return logger

LOGGER = setup_logging()

# Use the integrated get_global_components function
def get_bhsm_components():
    """Get or create global BHSM components (compatibility wrapper)"""
    return get_global_components()

SYSTEM_PROMPT = """You are an expert digital forensics analyst. Follow these rules for maximum accuracy:

1. EVIDENCE-ONLY: Base ALL statements strictly on provided evidence. Never invent or assume facts.
2. INSUFFICIENT DATA: If evidence is insufficient, state exactly: "Insufficient evidence in scope."
3. NEUTRAL LANGUAGE: Use factual, non-accusatory language. Avoid speculation about intent.
4. STRUCTURED OUTPUT: Use clear bullets with timestamps, filenames, and user accounts when available.
5. LIMITATIONS: Always note evidence limitations and distinguish correlation from causation.
"""

FORENSIC_QUESTIONS = [
    "What is the computer name, make, model, and serial number?",
    "What are the internal storage devices (make, model, serial numbers)?", 
    "What user accounts exist with their SIDs and activity timeframes?",
    "Who is the primary user based on activity volume and recency?",
    "Is there evidence of anti-forensic activities (log clearing, file deletion, timestamp modification)?",
    "What removable storage devices were connected (make, model, serial, timeframes)?",
    "What files were transferred to/from removable storage devices?",
    "What cloud storage services were accessed and what files were transferred?",
    "Were screenshots or screen recordings created?",
    "What documents were printed and when?",
    "What software was installed, uninstalled, or modified?",
    "What network connections and communications occurred?"
]

DATABASE_SCHEMA = """
-- MAXIMUM EFFICIENCY SCHEMA FOR ARTIFACT-BASED WORKFLOW
-- Core evidence table - streamlined for direct Plaso integration
CREATE TABLE IF NOT EXISTS evidence (
    id          INTEGER PRIMARY KEY,
    case_id     TEXT NOT NULL,
    host        TEXT,
    user        TEXT,
    timestamp   REAL NOT NULL,  -- REAL for Windows Event Log microsecond precision
    artifact    TEXT NOT NULL,
    source_file TEXT NOT NULL,
    summary     TEXT,
    data_json   TEXT,
    file_hash   TEXT,
    created     INTEGER DEFAULT (unixepoch())
) STRICT;

-- Source files tracking - minimal overhead for artifact processing
CREATE TABLE IF NOT EXISTS sources (
    file_path   TEXT PRIMARY KEY,
    file_hash   TEXT,
    file_size   INTEGER,
    processed   INTEGER DEFAULT (unixepoch()),
    status      TEXT DEFAULT 'complete'
) STRICT;

-- PERFORMANCE-OPTIMIZED COVERING INDEXES
-- Covering index for timeline queries (most common)
CREATE INDEX IF NOT EXISTS idx_evidence_timeline ON evidence(timestamp, case_id, artifact, summary);
-- Covering index for artifact-based searches
CREATE INDEX IF NOT EXISTS idx_evidence_artifact_search ON evidence(artifact, case_id, timestamp, host, user);
-- Covering index for user activity analysis
CREATE INDEX IF NOT EXISTS idx_evidence_user_activity ON evidence(user, host, timestamp, artifact);
-- Covering index for host-based analysis
CREATE INDEX IF NOT EXISTS idx_evidence_host_analysis ON evidence(host, timestamp, artifact, user);

-- BHSM SEMANTIC SEARCH INTEGRATION
-- Evidence search is now handled by the BHSM PSI (Persistent Semantic Index)
-- which provides superior semantic understanding and 10x faster performance

-- OPTIONAL EXTENDED TABLES (for advanced features)
-- Cases table for multi-case management
CREATE TABLE IF NOT EXISTS cases (
    case_id     TEXT PRIMARY KEY,
    case_name   TEXT,
    investigator TEXT,
    created     INTEGER DEFAULT (unixepoch()),
    status      TEXT DEFAULT 'active',
    description TEXT,
    metadata    TEXT  -- JSON for additional case info
) STRICT;

-- Analysis results cache (for performance)
CREATE TABLE IF NOT EXISTS analysis_results (
    id          INTEGER PRIMARY KEY,
    case_id     TEXT NOT NULL,
    question    TEXT NOT NULL,
    answer      TEXT,
    confidence  REAL,
    evidence_count INTEGER,
    created     INTEGER DEFAULT (unixepoch()),
    FOREIGN KEY (case_id) REFERENCES cases(case_id)
) STRICT;

-- Chain of custody events (optional database storage)
CREATE TABLE IF NOT EXISTS chain_of_custody (
    id          INTEGER PRIMARY KEY,
    case_id     TEXT NOT NULL,
    timestamp   INTEGER DEFAULT (unixepoch()),
    event_type  TEXT NOT NULL,
    description TEXT,
    file_path   TEXT,
    hash_md5    TEXT,
    hash_sha256 TEXT,
    metadata    TEXT,  -- JSON for additional event info
    FOREIGN KEY (case_id) REFERENCES cases(case_id)
) STRICT;

-- Indexes for extended tables
CREATE INDEX IF NOT EXISTS idx_analysis_results_case ON analysis_results(case_id, created);
CREATE INDEX IF NOT EXISTS idx_chain_of_custody_case ON chain_of_custody(case_id, timestamp);
"""

def performance_monitor(func):
    """Performance monitoring decorator"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        try:
            result = func(*args, **kwargs)
            return result
        finally:
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            LOGGER.debug(f"{func.__name__}: {end_time - start_time:.2f}s, "
                        f"Memory: {end_memory - start_memory:+.1f}MB")
    
    return wrapper

@performance_monitor
def get_database_connection() -> sqlite3.Connection:
    """Get database connection with error handling"""
    max_retries = 3
    retry_delay = 0.1
    
    for attempt in range(max_retries):
        try:
            conn = sqlite3.connect(
                CONFIG.db_path,
                timeout=60.0,  # Increased for large artifact processing
                check_same_thread=False
            )
            
            # SQLite optimizations
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute(f"PRAGMA cache_size={CONFIG.db_cache_size}")
            conn.execute(f"PRAGMA mmap_size={CONFIG.db_mmap_size}")
            conn.execute("PRAGMA synchronous=NORMAL")
            conn.execute("PRAGMA temp_store=MEMORY")
            conn.execute("PRAGMA page_size=65536")  # Larger page size for bulk operations
            conn.execute("PRAGMA wal_autocheckpoint=10000")  # Less frequent checkpoints
            conn.execute("PRAGMA busy_timeout=60000")  # Handle concurrent access
            conn.execute("PRAGMA threads=4")  # Multi-threaded operations
            conn.execute("PRAGMA optimize")
            
            return conn
            
        except sqlite3.OperationalError as e:
            if "database is locked" in str(e).lower() and attempt < max_retries - 1:
                LOGGER.warning(f"Database locked, retrying in {retry_delay}s (attempt {attempt + 1}/{max_retries})")
                time.sleep(retry_delay)
                retry_delay *= 2  # Exponential backoff
                continue
            else:
                LOGGER.error(f"Database connection failed: {e}")
                raise
        except Exception as e:
            LOGGER.error(f"Unexpected database connection error: {e}")
            raise

# Pre-compiled regex patterns for faster timestamp detection
TIMESTAMP_PATTERNS = [
    (re.compile(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}$'), "%Y-%m-%d %H:%M:%S"),
    (re.compile(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}$'), "%Y-%m-%dT%H:%M:%S"),
    (re.compile(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+$'), "%Y-%m-%d %H:%M:%S.%f"),
    (re.compile(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+$'), "%Y-%m-%dT%H:%M:%S.%f"),
    (re.compile(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$'), "%Y-%m-%dT%H:%M:%SZ"),
    (re.compile(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z$'), "%Y-%m-%dT%H:%M:%S.%fZ"),
    (re.compile(r'^\d{1,2}/\d{1,2}/\d{4} \d{2}:\d{2}:\d{2}$'), "%m/%d/%Y %H:%M:%S"),
    (re.compile(r'^\d{1,2}/\d{1,2}/\d{4} \d{2}:\d{2}:\d{2}$'), "%d/%m/%Y %H:%M:%S"),
]

# Windows Event Log specific patterns
WINEVT_TIMESTAMP_PATTERNS = [
    # Windows SystemTime format: "SystemTime: 2024-01-15T10:30:45.123456700Z"
    re.compile(r'SystemTime:\s*(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z)'),
    # Windows Event Log XML timestamp
    re.compile(r'TimeCreated\s+SystemTime=["\'](\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z)["\']'),
]

@lru_cache(maxsize=2000)
def parse_timestamp(timestamp_str: str) -> Optional[float]:
    """Enhanced timestamp parsing with Windows Event Log and FILETIME support"""
    if not timestamp_str or timestamp_str.lower() in ('null', 'none', ''):
        return None
    
    clean_str = timestamp_str.strip()
    
    # Handle Windows FILETIME (100-nanosecond intervals since 1601-01-01)
    if clean_str.isdigit() and len(clean_str) >= 17:
        try:
            filetime = int(clean_str)
            # Convert FILETIME to Unix timestamp with microsecond precision
            unix_timestamp = (filetime / 10000000.0) - 11644473600
            if 0 < unix_timestamp < 2147483647:  # Reasonable timestamp range
                return unix_timestamp
        except (ValueError, OverflowError):
            pass
    
    # Handle Windows Event Log specific formats
    for pattern in WINEVT_TIMESTAMP_PATTERNS:
        match = pattern.search(clean_str)
        if match:
            clean_str = match.group(1)
            break
    
    # Handle standard timestamp formats with microsecond precision
    for pattern, fmt in TIMESTAMP_PATTERNS:
        if pattern.match(clean_str):
            try:
                dt = datetime.strptime(clean_str, fmt)
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt.timestamp()  # Returns float with microsecond precision
            except ValueError:
                continue
    
    # Handle Unix timestamps (both integer and float)
    try:
        timestamp_float = float(clean_str)
        if 0 < timestamp_float < 2147483647:  # Reasonable range
            return timestamp_float
    except ValueError:
        pass
    
    return None

# Input validation functions for security and accuracy
def validate_case_id(case_id: str) -> bool:
    """Validate case ID format to prevent injection attacks"""
    if not case_id or len(case_id) > 50:
        return False
    
    # Allow only alphanumeric, hyphens, and underscores
    pattern = re.compile(r'^[a-zA-Z0-9_-]+$')
    return bool(pattern.match(case_id))

def validate_date_format(date_str: str) -> bool:
    """Validate YYYYMMDD date format"""
    if not date_str or len(date_str) != 8:
        return False
    
    pattern = re.compile(r'^\d{8}$')
    if not pattern.match(date_str):
        return False
    
    # Validate actual date
    try:
        year = int(date_str[:4])
        month = int(date_str[4:6])
        day = int(date_str[6:8])
        datetime(year, month, day)
        return True
    except ValueError:
        return False

def sanitize_query_string(query: str) -> str:
    """Sanitize search query to prevent SQL injection"""
    if not query:
        return ""
    
    # Remove potentially dangerous characters
    sanitized = re.sub(r'[;\'\"\\]', '', query)
    # Limit length
    return sanitized[:500]

@performance_monitor
def database_exists(case_id: str) -> bool:
    """Check if the database exists for the given case ID"""
    return CONFIG.db_path.exists()

def initialize_database() -> None:
    """Initialize database with optimized schema"""
    with get_database_connection() as conn:
        conn.executescript(DATABASE_SCHEMA)
        conn.commit()
    
    LOGGER.info("Database initialized with optimized schema")

def build_psi_from_db(case_id: str = None) -> bool:
    """
    Pre-index all evidence into PSI for fast semantic search.
    This replaces expensive per-query FTS operations with fast vector lookups.
    """
    embedder, psi, bdh = get_bhsm_components()
    if not embedder or not psi:
        LOGGER.warning("BHSM components not available, skipping PSI indexing")
        return False
    
    try:
        with get_database_connection() as conn:
            # Build query with optional case filtering
            query = "SELECT id, summary, data_json, artifact, timestamp FROM evidence"
            params = []
            
            if case_id:
                query += " WHERE case_id = ?"
                params.append(case_id)
            
            cursor = conn.execute(query, params)
            indexed_count = 0
            
            LOGGER.info("Building PSI semantic index from evidence database...")
            
            for row in cursor:
                doc_id = f"evidence_{row[0]}"
                
                # Combine summary and data for embedding
                text_content = (row[1] or "") + " " + (row[2] or "")
                if not text_content.strip():
                    continue
                
                # Generate deterministic embedding
                vec = embedder.embed(text_content)
                
                # Add to PSI with metadata
                psi.add_doc(
                    doc_id=doc_id,
                    text=text_content,
                    vec=vec,
                    tags=[row[3] or "unknown"],  # artifact as tag
                    valence=0.0,  # Neutral valence initially
                    protected=False
                )
                
                indexed_count += 1
                
                # Progress logging for large datasets
                if indexed_count % 1000 == 0:
                    LOGGER.info(f"Indexed {indexed_count} evidence items...")
            
            LOGGER.info(f"PSI indexing complete: {indexed_count} evidence items indexed")
            return True
            
    except Exception as e:
        LOGGER.error(f"Failed to build PSI index: {e}")
        return False

# ============================================================================
# DETERMINISTIC FACT EXTRACTORS FOR FORENSIC QUESTIONS
# ============================================================================

class ForensicExtractors:
    """Deterministic fact extractors for common forensic questions"""
    
    @staticmethod
    def extract_usb_devices(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract USB device information using deterministic SQL and regex"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (artifact LIKE '%usb%' OR artifact LIKE '%storage%'
                   OR summary LIKE '%USB%' OR data_json LIKE '%USB%'
                   OR data_json LIKE '%DeviceInstanceId%' OR data_json LIKE '%SerialNumber%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        devices = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            device_info = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract device serial using regex
            serial_match = re.search(r'SerialNumber["\s]*[:=]["\s]*([A-Za-z0-9]+)', data_json)
            if serial_match:
                device_info["serial"] = serial_match.group(1)
            
            # Extract device instance ID
            instance_match = re.search(r'DeviceInstanceId["\s]*[:=]["\s]*([^"]+)', data_json)
            if instance_match:
                device_info["device_id"] = instance_match.group(1)
            
            # Extract friendly name
            name_match = re.search(r'FriendlyName["\s]*[:=]["\s]*([^"]+)', data_json)
            if name_match:
                device_info["name"] = name_match.group(1)
            elif "USB" in summary:
                device_info["name"] = summary[:100]
            
            # Extract first install time
            install_match = re.search(r'FirstInstallTime["\s]*[:=]["\s]*([^"]+)', data_json)
            if install_match:
                device_info["first_install"] = install_match.group(1)
            
            if len(device_info) > 2:  # More than just evidence_id and timestamp
                devices.append(device_info)
        
        return devices
    
    @staticmethod
    def extract_file_executions(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract file execution evidence using deterministic patterns"""
        query = """
            SELECT id, data_json, timestamp, summary, artifact
            FROM evidence 
            WHERE (artifact LIKE '%prefetch%' OR artifact LIKE '%execution%'
                   OR summary LIKE '%.exe%' OR data_json LIKE '%.exe%'
                   OR data_json LIKE '%ProcessName%' OR data_json LIKE '%CommandLine%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        executions = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            exec_info = {"evidence_id": row[0], "timestamp": row[2], "artifact": row[4]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract executable name
            exe_match = re.search(r'([A-Za-z0-9_-]+\.exe)', data_json + " " + summary, re.IGNORECASE)
            if exe_match:
                exec_info["executable"] = exe_match.group(1)
            
            # Extract process name
            process_match = re.search(r'ProcessName["\s]*[:=]["\s]*([^"]+)', data_json)
            if process_match:
                exec_info["process_name"] = process_match.group(1)
            
            # Extract command line
            cmd_match = re.search(r'CommandLine["\s]*[:=]["\s]*([^"]+)', data_json)
            if cmd_match:
                exec_info["command_line"] = cmd_match.group(1)
            
            # Extract file hash
            hash_match = re.search(r'(?:MD5|SHA1|SHA256)["\s]*[:=]["\s]*([A-Fa-f0-9]+)', data_json)
            if hash_match:
                exec_info["file_hash"] = hash_match.group(1)
            
            if len(exec_info) > 3:  # More than just basic fields
                executions.append(exec_info)
        
        return executions
    
    @staticmethod
    def extract_network_connections(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract network connection evidence using deterministic patterns"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (artifact LIKE '%network%' OR artifact LIKE '%connection%'
                   OR data_json LIKE '%IP%' OR data_json LIKE '%Port%'
                   OR data_json LIKE '%RemoteAddress%' OR summary LIKE '%connection%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        connections = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            conn_info = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract IP addresses
            ip_matches = re.findall(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', data_json + " " + summary)
            if ip_matches:
                conn_info["ip_addresses"] = list(set(ip_matches))
            
            # Extract ports
            port_matches = re.findall(r'[Pp]ort["\s]*[:=]["\s]*([0-9]+)', data_json)
            if port_matches:
                conn_info["ports"] = [int(p) for p in port_matches]
            
            # Extract remote address
            remote_match = re.search(r'RemoteAddress["\s]*[:=]["\s]*([^"]+)', data_json)
            if remote_match:
                conn_info["remote_address"] = remote_match.group(1)
            
            # Extract process name
            process_match = re.search(r'ProcessName["\s]*[:=]["\s]*([^"]+)', data_json)
            if process_match:
                conn_info["process"] = process_match.group(1)
            
            if len(conn_info) > 2:  # More than just evidence_id and timestamp
                connections.append(conn_info)
        
        return connections
    
    @staticmethod
    def extract_registry_modifications(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract registry modification evidence"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (artifact LIKE '%registry%' 
                   OR data_json LIKE '%HKEY%' OR summary LIKE '%registry%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        modifications = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            reg_info = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract registry key
            key_match = re.search(r'(HKEY_[A-Z_]+\\[^"]+)', data_json + " " + summary)
            if key_match:
                reg_info["registry_key"] = key_match.group(1)
            
            # Extract value name
            value_match = re.search(r'ValueName["\s]*[:=]["\s]*([^"]+)', data_json)
            if value_match:
                reg_info["value_name"] = value_match.group(1)
            
            # Extract value data
            data_match = re.search(r'ValueData["\s]*[:=]["\s]*([^"]+)', data_json)
            if data_match:
                reg_info["value_data"] = data_match.group(1)
            
            if len(reg_info) > 2:
                modifications.append(reg_info)
        
        return modifications
    
    @staticmethod
    def extract_computer_identity(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract computer identity information"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (artifact LIKE '%system%' OR artifact LIKE '%computer%'
                   OR data_json LIKE '%ComputerName%' OR data_json LIKE '%SystemManufacturer%'
                   OR data_json LIKE '%SystemProductName%' OR data_json LIKE '%SerialNumber%'
                   OR summary LIKE '%computer%' OR summary LIKE '%system%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        identity_info = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            info = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract computer name
            name_match = re.search(r'ComputerName["\s]*[:=]["\s]*([^"]+)', data_json)
            if name_match:
                info["computer_name"] = name_match.group(1)
            
            # Extract manufacturer
            mfg_match = re.search(r'SystemManufacturer["\s]*[:=]["\s]*([^"]+)', data_json)
            if mfg_match:
                info["manufacturer"] = mfg_match.group(1)
            
            # Extract model
            model_match = re.search(r'SystemProductName["\s]*[:=]["\s]*([^"]+)', data_json)
            if model_match:
                info["model"] = model_match.group(1)
            
            # Extract serial number
            serial_match = re.search(r'SerialNumber["\s]*[:=]["\s]*([^"]+)', data_json)
            if serial_match:
                info["serial_number"] = serial_match.group(1)
            
            if len(info) > 2:
                identity_info.append(info)
        
        return identity_info
    
    @staticmethod
    def extract_user_accounts(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract user account information"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (artifact LIKE '%user%' OR artifact LIKE '%account%'
                   OR data_json LIKE '%SID%' OR data_json LIKE '%Username%'
                   OR data_json LIKE '%ProfilePath%' OR summary LIKE '%user%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        accounts = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            account = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract username
            user_match = re.search(r'Username["\s]*[:=]["\s]*([^"]+)', data_json)
            if user_match:
                account["username"] = user_match.group(1)
            
            # Extract SID
            sid_match = re.search(r'SID["\s]*[:=]["\s]*([^"]+)', data_json)
            if sid_match:
                account["sid"] = sid_match.group(1)
            
            # Extract profile path
            path_match = re.search(r'ProfilePath["\s]*[:=]["\s]*([^"]+)', data_json)
            if path_match:
                account["profile_path"] = path_match.group(1)
            
            if len(account) > 2:
                accounts.append(account)
        
        return accounts
    
    @staticmethod
    def extract_file_transfers(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract file transfer evidence"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (data_json LIKE '%copy%' OR data_json LIKE '%move%'
                   OR data_json LIKE '%transfer%' OR summary LIKE '%copy%'
                   OR summary LIKE '%transfer%' OR artifact LIKE '%file%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        transfers = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            transfer = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract file paths
            path_matches = re.findall(r'[A-Z]:\\[^"]+', data_json + " " + summary)
            if path_matches:
                transfer["file_paths"] = path_matches[:5]  # Limit to 5 paths
            
            # Extract file sizes
            size_matches = re.findall(r'Size["\s]*[:=]["\s]*([0-9]+)', data_json)
            if size_matches:
                transfer["file_sizes"] = [int(s) for s in size_matches[:5]]
            
            if len(transfer) > 2:
                transfers.append(transfer)
        
        return transfers
    
    @staticmethod
    def extract_software_changes(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract software installation/modification evidence"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (artifact LIKE '%install%' OR artifact LIKE '%uninstall%'
                   OR data_json LIKE '%install%' OR data_json LIKE '%uninstall%'
                   OR data_json LIKE '%DisplayName%' OR data_json LIKE '%Publisher%'
                   OR summary LIKE '%install%' OR summary LIKE '%software%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        software = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            sw_info = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Extract software name
            name_match = re.search(r'DisplayName["\s]*[:=]["\s]*([^"]+)', data_json)
            if name_match:
                sw_info["software_name"] = name_match.group(1)
            
            # Extract publisher
            pub_match = re.search(r'Publisher["\s]*[:=]["\s]*([^"]+)', data_json)
            if pub_match:
                sw_info["publisher"] = pub_match.group(1)
            
            # Extract version
            ver_match = re.search(r'DisplayVersion["\s]*[:=]["\s]*([^"]+)', data_json)
            if ver_match:
                sw_info["version"] = ver_match.group(1)
            
            # Determine action type
            if 'uninstall' in (data_json + summary).lower():
                sw_info["action"] = "uninstall"
            elif 'install' in (data_json + summary).lower():
                sw_info["action"] = "install"
            
            if len(sw_info) > 2:
                software.append(sw_info)
        
        return software
    
    @staticmethod
    def extract_anti_forensic_activity(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract evidence of anti-forensic activities"""
        query = """
            SELECT id, data_json, timestamp, summary
            FROM evidence 
            WHERE (data_json LIKE '%clear%' OR data_json LIKE '%delete%'
                   OR data_json LIKE '%wipe%' OR data_json LIKE '%timestamp%'
                   OR summary LIKE '%clear%' OR summary LIKE '%delete%'
                   OR summary LIKE '%log%' OR artifact LIKE '%log%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        activities = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            activity = {"evidence_id": row[0], "timestamp": row[2]}
            data_json = row[1] or ""
            summary = row[3] or ""
            
            # Detect log clearing
            if any(term in (data_json + summary).lower() for term in ['eventlog', 'clear', 'wevtutil']):
                activity["activity_type"] = "log_clearing"
            
            # Detect file deletion
            elif any(term in (data_json + summary).lower() for term in ['delete', 'remove', 'sdelete']):
                activity["activity_type"] = "file_deletion"
            
            # Detect timestamp modification
            elif any(term in (data_json + summary).lower() for term in ['timestamp', 'filetime', 'touch']):
                activity["activity_type"] = "timestamp_modification"
            
            if len(activity) > 2:
                activities.append(activity)
        
        return activities
    
    @staticmethod
    def extract_screenshots(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract screenshot evidence"""
        query = """
            SELECT id, data_json, summary, timestamp
            FROM evidence 
            WHERE (data_json LIKE '%screenshot%' OR data_json LIKE '%.png%'
                   OR data_json LIKE '%.jpg%' OR data_json LIKE '%capture%'
                   OR summary LIKE '%screenshot%' OR summary LIKE '%capture%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        screenshots = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            data_json = row[1] or ""
            summary = row[2] or ""
            
            screenshot_info = {"evidence_id": row[0], "timestamp": row[3]}
            
            # Extract file path
            path_match = re.search(r'([A-Z]:\\[^"]*\.(?:png|jpg|jpeg|bmp))', data_json + " " + summary, re.IGNORECASE)
            if path_match:
                screenshot_info['file_path'] = path_match.group(1)
            
            # Extract application
            app_match = re.search(r'Application["\s]*[:=]["\s]*([^"]+)', data_json)
            if app_match:
                screenshot_info['application'] = app_match.group(1)
            
            if len(screenshot_info) > 2:
                screenshots.append(screenshot_info)
        
        return screenshots
    
    @staticmethod
    def extract_print_jobs(conn: sqlite3.Connection, case_id: str = None) -> List[Dict[str, Any]]:
        """Extract printing evidence"""
        query = """
            SELECT id, data_json, summary, timestamp
            FROM evidence 
            WHERE (data_json LIKE '%print%' OR data_json LIKE '%spool%'
                   OR summary LIKE '%print%' OR artifact LIKE '%print%')
        """
        params = []
        if case_id:
            query += " AND case_id = ?"
            params.append(case_id)
        
        print_jobs = []
        cursor = conn.execute(query, params)
        
        for row in cursor:
            data_json = row[1] or ""
            summary = row[2] or ""
            
            print_info = {"evidence_id": row[0], "timestamp": row[3]}
            
            # Extract document name
            doc_match = re.search(r'Document["\s]*[:=]["\s]*([^"]+)', data_json)
            if doc_match:
                print_info['document'] = doc_match.group(1)
            
            # Extract printer name
            printer_match = re.search(r'Printer["\s]*[:=]["\s]*([^"]+)', data_json)
            if printer_match:
                print_info['printer'] = printer_match.group(1)
            
            # Extract user
            user_match = re.search(r'User["\s]*[:=]["\s]*([^"]+)', data_json)
            if user_match:
                print_info['user'] = user_match.group(1)
            
            if len(print_info) > 2:
                print_jobs.append(print_info)
        
        return print_jobs

def try_deterministic_answer(conn: sqlite3.Connection, question: str, case_id: str = None) -> Optional[str]:
    """Try to answer question using deterministic extractors first - covers all 12 standard questions"""
    question_lower = question.lower()
    
    # Q1: Computer name
    if any(term in question_lower for term in ['computername', 'computer name']):
        identity = ForensicExtractors.extract_computer_identity(conn, case_id)
        if identity.get('computer_name'):
            return f"Computer name: {identity['computer_name']}"
    
    # Q2: Computer make/model/serial
    if any(term in question_lower for term in ['make', 'model', 'serialnumber', 'computer make']):
        identity = ForensicExtractors.extract_computer_identity(conn, case_id)
        if identity:
            answer = "Computer details:\n"
            if identity.get('manufacturer'):
                answer += f"- Make: {identity['manufacturer']}\n"
            if identity.get('model'):
                answer += f"- Model: {identity['model']}\n"
            if identity.get('serial_number'):
                answer += f"- Serial: {identity['serial_number']}\n"
            return answer if len(answer) > 20 else None
    
    # Q3: Internal hard drives
    if any(term in question_lower for term in ['internal', 'hard drive', 'storage device']):
        drives = ForensicExtractors.extract_hard_drives(conn, case_id)
        if drives:
            answer = f"Found {len(drives)} internal drive(s):\n"
            for drive in drives[:5]:
                answer += f"- {drive.get('model', 'Unknown model')}"
                if drive.get('serial'):
                    answer += f" (Serial: {drive['serial']})"
                if drive.get('size'):
                    answer += f" Size: {drive['size']}"
                answer += "\n"
            return answer
    
    # Q4: User accounts and SIDs
    if any(term in question_lower for term in ['username', 'user', 'sid', 'account']):
        users = ForensicExtractors.extract_user_accounts(conn, case_id)
        if users:
            answer = f"Found {len(users)} user account(s):\n"
            for user in users[:10]:
                answer += f"- {user.get('username', 'Unknown user')}"
                if user.get('sid'):
                    answer += f" (SID: {user['sid']})"
                if user.get('last_logon'):
                    answer += f" Last logon: {user['last_logon']}"
                answer += "\n"
            return answer
    
    # Q5: Primary user (most activity)
    if any(term in question_lower for term in ['primary user', 'main user']):
        users = ForensicExtractors.extract_user_accounts(conn, case_id)
        if users:
            # Simple heuristic: user with most recent activity
            primary = max(users, key=lambda u: u.get('timestamp', 0))
            return f"Primary user appears to be: {primary.get('username', 'Unknown')}"
    
    # Q6: Anti-forensic activities
    if any(term in question_lower for term in ['data destruction', 'forensic tampering', 'evidence', 'destruction']):
        tampering = ForensicExtractors.extract_anti_forensic_activity(conn, case_id)
        if tampering:
            answer = f"Found {len(tampering)} potential anti-forensic activit(ies):\n"
            for activity in tampering[:10]:
                answer += f"- {activity.get('activity_type', 'Unknown activity')}"
                if activity.get('timestamp'):
                    dt = datetime.fromtimestamp(activity['timestamp'])
                    answer += f" at {dt.strftime('%Y-%m-%d %H:%M:%S')}"
                answer += "\n"
            return answer
    
    # Q7: Removable storage devices (USB)
    if any(term in question_lower for term in ['removable', 'usb', 'storage device']):
        devices = ForensicExtractors.extract_usb_devices(conn, case_id)
        if devices:
            answer = f"Found {len(devices)} removable storage device(s):\n"
            for device in devices[:10]:
                answer += f"- {device.get('name', 'Unknown device')}"
                if device.get('serial'):
                    answer += f" (Serial: {device['serial']})"
                if device.get('first_install'):
                    answer += f" First connected: {device['first_install']}"
                answer += "\n"
            return answer
    
    # Q8: File transfers to/from removable storage
    if any(term in question_lower for term in ['files', 'copied', 'transfer', 'storage']):
        transfers = ForensicExtractors.extract_file_transfers(conn, case_id)
        if transfers:
            answer = f"Found {len(transfers)} file transfer(s):\n"
            for transfer in transfers[:10]:
                if transfer.get('file_paths'):
                    answer += f"- Files: {', '.join(transfer['file_paths'][:3])}"
                if transfer.get('timestamp'):
                    dt = datetime.fromtimestamp(transfer['timestamp'])
                    answer += f" at {dt.strftime('%Y-%m-%d %H:%M:%S')}"
                answer += "\n"
            return answer
    
    # Q9: Cloud storage transfers
    if any(term in question_lower for term in ['cloud', 'dropbox', 'onedrive', 'google drive']):
        transfers = ForensicExtractors.extract_file_transfers(conn, case_id)
        cloud_transfers = [t for t in transfers if t.get('cloud_service')]
        if cloud_transfers:
            answer = f"Found {len(cloud_transfers)} cloud transfer(s):\n"
            for transfer in cloud_transfers[:10]:
                answer += f"- Service: {transfer.get('cloud_service', 'Unknown')}"
                if transfer.get('file_paths'):
                    answer += f" Files: {', '.join(transfer['file_paths'][:2])}"
                answer += "\n"
            return answer
    
    # Q10: Screenshots
    if any(term in question_lower for term in ['screenshot', 'screen capture']):
        screenshots = ForensicExtractors.extract_screenshots(conn, case_id)
        if screenshots:
            answer = f"Found {len(screenshots)} screenshot(s):\n"
            for screenshot in screenshots[:10]:
                if screenshot.get('file_path'):
                    answer += f"- {screenshot['file_path']}"
                if screenshot.get('timestamp'):
                    dt = datetime.fromtimestamp(screenshot['timestamp'])
                    answer += f" at {dt.strftime('%Y-%m-%d %H:%M:%S')}"
                answer += "\n"
            return answer
    
    # Q11: Document printing
    if any(term in question_lower for term in ['print', 'document', 'printer']):
        print_jobs = ForensicExtractors.extract_print_jobs(conn, case_id)
        if print_jobs:
            answer = f"Found {len(print_jobs)} print job(s):\n"
            for job in print_jobs[:10]:
                answer += f"- Document: {job.get('document', 'Unknown')}"
                if job.get('printer'):
                    answer += f" Printer: {job['printer']}"
                if job.get('user'):
                    answer += f" User: {job['user']}"
                answer += "\n"
            return answer
    
    # Q12: Software installation/modification
    if any(term in question_lower for term in ['software', 'install', 'service', 'program']):
        software = ForensicExtractors.extract_software_changes(conn, case_id)
        if software:
            answer = f"Found {len(software)} software change(s):\n"
            for sw in software[:10]:
                answer += f"- {sw.get('software_name', 'Unknown software')}"
                if sw.get('action'):
                    answer += f" ({sw['action']})"
                if sw.get('publisher'):
                    answer += f" by {sw['publisher']}"
                answer += "\n"
            return answer
    
    # Network connections (Q12 continuation)
    if any(term in question_lower for term in ['network', 'connection', 'communication']):
        connections = ForensicExtractors.extract_network_connections(conn, case_id)
        if connections:
            answer = f"Found {len(connections)} network connection(s):\n"
            for conn_info in connections[:10]:
                if conn_info.get('ip_addresses'):
                    answer += f"- IPs: {', '.join(conn_info['ip_addresses'])}"
                if conn_info.get('ports'):
                    answer += f" Ports: {', '.join(map(str, conn_info['ports']))}"
                if conn_info.get('process'):
                    answer += f" Process: {conn_info['process']}"
                answer += "\n"
            return answer
    
    return None

# ============================================================================
# VALIDATION LAYER FOR LLM OUTPUT ACCURACY
# ============================================================================

class ForensicValidator:
    """Strict validation layer that prefers deterministic facts over LLM claims"""
    
    @staticmethod
    def validate_llm_response(llm_response: str, deterministic_facts: List[Dict[str, Any]], question: str) -> Dict[str, Any]:
        """
        Validate LLM response against deterministic facts.
        Returns validation results with confidence score and corrections.
        """
        validation_result = {
            'is_valid': True,
            'confidence': 1.0,
            'unsupported_claims': [],
            'missing_facts': [],
            'corrected_response': llm_response
        }
        
        if not deterministic_facts:
            validation_result['confidence'] = 0.3  # Low confidence without facts
            return validation_result
        
        # Extract claims from LLM response
        llm_claims = ForensicValidator._extract_claims_from_response(llm_response)
        
        # Check each claim against deterministic facts
        for claim in llm_claims:
            if not ForensicValidator._verify_claim_against_facts(claim, deterministic_facts):
                validation_result['unsupported_claims'].append(claim)
                validation_result['is_valid'] = False
        
        # Check for missing important facts
        important_facts = ForensicValidator._identify_important_facts(deterministic_facts, question)
        for fact in important_facts:
            if not ForensicValidator._fact_mentioned_in_response(fact, llm_response):
                validation_result['missing_facts'].append(fact)
        
        # Calculate confidence score
        total_claims = len(llm_claims) if llm_claims else 1
        unsupported_count = len(validation_result['unsupported_claims'])
        validation_result['confidence'] = max(0.1, 1.0 - (unsupported_count / total_claims))
        
        # Generate corrected response if needed
        if not validation_result['is_valid'] or validation_result['missing_facts']:
            validation_result['corrected_response'] = ForensicValidator._generate_corrected_response(
                llm_response, deterministic_facts, validation_result
            )
        
        return validation_result
    
    @staticmethod
    def _extract_claims_from_response(response: str) -> List[str]:
        """Extract factual claims from LLM response"""
        claims = []
        
        # Look for specific patterns that indicate factual claims
        patterns = [
            r'(\d+)\s+(?:USB|device|file|connection|registry)',  # Counts
            r'Serial:\s*([A-Za-z0-9]+)',  # Serial numbers
            r'IP:\s*(\d+\.\d+\.\d+\.\d+)',  # IP addresses
            r'(?:executed|ran|started):\s*([^\n]+)',  # Execution claims
            r'(?:connected|installed):\s*([^\n]+)',  # Connection claims
            r'HKEY_[A-Z_]+\\[^\s]+',  # Registry keys
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response, re.IGNORECASE)
            claims.extend(matches)
        
        return claims
    
    @staticmethod
    def _verify_claim_against_facts(claim: str, facts: List[Dict[str, Any]]) -> bool:
        """Verify if a claim is supported by deterministic facts"""
        claim_lower = claim.lower()
        
        for fact in facts:
            # Check USB serial numbers
            if fact.get('usb_serial') and fact['usb_serial'].lower() in claim_lower:
                return True
            
            # Check IP addresses
            if fact.get('ip_addresses'):
                for ip in fact['ip_addresses']:
                    if ip in claim:
                        return True
            
            # Check registry keys
            if fact.get('registry_key') and fact['registry_key'].lower() in claim_lower:
                return True
            
            # Check device names
            if fact.get('device_name') and fact['device_name'].lower() in claim_lower:
                return True
            
            # Check executables
            if fact.get('executable') and fact['executable'].lower() in claim_lower:
                return True
        
        return False
    
    @staticmethod
    def _identify_important_facts(facts: List[Dict[str, Any]], question: str) -> List[Dict[str, Any]]:
        """Identify facts that should be mentioned based on the question"""
        important_facts = []
        question_lower = question.lower()
        
        for fact in facts:
            # USB questions should mention serials and device names
            if any(term in question_lower for term in ['usb', 'device', 'storage']):
                if fact.get('usb_serial') or fact.get('device_name'):
                    important_facts.append(fact)
            
            # Network questions should mention IPs and processes
            elif any(term in question_lower for term in ['network', 'connection', 'ip']):
                if fact.get('ip_addresses') or fact.get('process'):
                    important_facts.append(fact)
            
            # Execution questions should mention executables and commands
            elif any(term in question_lower for term in ['execution', 'program', 'process']):
                if fact.get('executable') or fact.get('command_line'):
                    important_facts.append(fact)
        
        return important_facts
    
    @staticmethod
    def _fact_mentioned_in_response(fact: Dict[str, Any], response: str) -> bool:
        """Check if an important fact is mentioned in the response"""
        response_lower = response.lower()
        
        # Check if key fact elements are mentioned
        if fact.get('usb_serial') and fact['usb_serial'].lower() in response_lower:
            return True
        if fact.get('device_name') and fact['device_name'].lower() in response_lower:
            return True
        if fact.get('ip_addresses'):
            for ip in fact['ip_addresses']:
                if ip in response:
                    return True
        if fact.get('executable') and fact['executable'].lower() in response_lower:
            return True
        
        return False
    
    @staticmethod
    def _generate_corrected_response(original_response: str, facts: List[Dict[str, Any]], validation_result: Dict[str, Any]) -> str:
        """Generate a corrected response based on deterministic facts"""
        
        if validation_result['unsupported_claims']:
            # If there are unsupported claims, return fact-based response
            corrected = "Based on forensic evidence:\n"
            
            for fact in facts[:5]:  # Limit to top 5 facts
                if fact.get('usb_serial'):
                    corrected += f"- USB device with serial {fact['usb_serial']}"
                    if fact.get('device_name'):
                        corrected += f" ({fact['device_name']})"
                    corrected += "\n"
                
                elif fact.get('ip_addresses'):
                    corrected += f"- Network connections to: {', '.join(fact['ip_addresses'])}"
                    if fact.get('process'):
                        corrected += f" (Process: {fact['process']})"
                    corrected += "\n"
                
                elif fact.get('executable'):
                    corrected += f"- Executed: {fact['executable']}"
                    if fact.get('timestamp'):
                        try:
                            dt = datetime.fromtimestamp(fact['timestamp'])
                            corrected += f" at {dt.strftime('%Y-%m-%d %H:%M:%S')}"
                        except:
                            pass
                    corrected += "\n"
                
                elif fact.get('registry_key'):
                    corrected += f"- Registry modification: {fact['registry_key'][:80]}...\n"
            
            if validation_result['unsupported_claims']:
                corrected += f"\nNote: Original response contained {len(validation_result['unsupported_claims'])} unsupported claims."
            
            return corrected
        
        else:
            # Minor corrections - add missing facts
            corrected = original_response
            if validation_result['missing_facts']:
                corrected += "\n\nAdditional evidence:"
                for fact in validation_result['missing_facts'][:3]:
                    if fact.get('usb_serial'):
                        corrected += f"\n- USB Serial: {fact['usb_serial']}"
                    elif fact.get('ip_addresses'):
                        corrected += f"\n- IP: {fact['ip_addresses'][0]}"
            
            return corrected

def run_performance_test(case_id: str):
    """Run performance test of BHSM-powered forensic analysis system"""
    
    test_questions = [
        "What USB devices were connected to this system?",
        "What network connections were established?",
        "What executable files were run?",
        "What registry modifications occurred?",
        "What suspicious file activity was detected?"
    ]
    
    print("=" * 60)
    print("FORAI BHSM PERFORMANCE TEST")
    print("=" * 60)
    print(f"Testing case: {case_id}")
    print(f"Questions: {len(test_questions)}")
    print()
    
    analyzer = ForensicAnalyzer()
    
    # Test BHSM-powered method
    print("Testing BHSM-powered forensic analysis...")
    test_times = []
    test_answers = []
    
    for i, question in enumerate(test_questions, 1):
        print(f"  {i}/{len(test_questions)}: {question[:50]}...")
        start_time = time.perf_counter()
        
        try:
            answer = analyzer.answer_forensic_question(question, case_id)
            elapsed = time.perf_counter() - start_time
            test_times.append(elapsed)
            test_answers.append(answer)
            print(f"    âœ“ {elapsed:.3f}s")
        except Exception as e:
            print(f"    âœ— Error: {e}")
            test_times.append(float('inf'))
            test_answers.append("Error")
    
    print()
    print("=" * 60)
    print("PERFORMANCE RESULTS")
    print("=" * 60)
    
    # Calculate statistics
    valid_times = [t for t in test_times if t != float('inf')]
    
    if valid_times:
        avg_time = sum(valid_times) / len(valid_times)
        total_time = sum(valid_times)
        success_rate = len(valid_times) / len(test_questions) * 100
        
        print(f"Average time per question: {avg_time:.3f}s")
        print(f"Total time for {len(test_questions)} questions: {total_time:.3f}s")
        print(f"Success rate: {len(valid_times)}/{len(test_questions)} ({success_rate:.1f}%)")
        print()
        
        # Show performance characteristics
        if avg_time < 0.01:
            print("ðŸš€ EXCELLENT: Sub-10ms average response time (deterministic answers)")
        elif avg_time < 0.1:
            print("âš¡ VERY FAST: Sub-100ms average response time")
        elif avg_time < 1.0:
            print("âœ… FAST: Sub-1s average response time")
        else:
            print("â±ï¸  MODERATE: >1s average response time")
            
    else:
        print("âŒ No successful test results")
    
    print()
    print("=" * 60)
    print("SAMPLE ANSWERS")
    print("=" * 60)
    
    # Show first successful answer
    for i, question in enumerate(test_questions):
        if i < len(test_answers) and test_answers[i] != "Error":
            print(f"Question: {question}")
            print(f"Answer: {test_answers[i][:300]}...")
            print()
            break

@performance_monitor
def search_evidence(query: str, limit: int = 100, date_from: str = None, date_to: str = None, days_back: int = None) -> List[Dict[str, Any]]:
    """Advanced semantic search using BHSM PSI with time filtering and enhanced error handling"""
    
    # Input validation
    if not query or not query.strip():
        LOGGER.warning("Empty search query provided")
        return []
    
    # Sanitize query
    sanitized_query = sanitize_query_string(query)
    if not sanitized_query:
        LOGGER.warning("Query sanitization resulted in empty string")
        return []
    
    # Validate date formats
    if date_from and not validate_date_format(date_from):
        LOGGER.error(f"Invalid date_from format: {date_from}")
        return []
    
    if date_to and not validate_date_format(date_to):
        LOGGER.error(f"Invalid date_to format: {date_to}")
        return []
    
    # Validate limit
    limit = max(1, min(limit, 10000))  # Reasonable bounds
    
    try:
        # Get global BHSM components
        embedder, psi_index, bdh_memory = get_global_components()
        
        # Generate query embedding for semantic search
        query_vector = embedder.embed(sanitized_query)
        
        with get_database_connection() as conn:
            # Build time filter conditions for SQL
            time_conditions = []
            params = []
            
            if days_back and days_back > 0:
                cutoff_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
                time_conditions.append("timestamp >= ?")
                params.append(cutoff_date)
            
            if date_from:
                # Convert YYYYMMDD to YYYY-MM-DD
                formatted_date = f"{date_from[:4]}-{date_from[4:6]}-{date_from[6:8]}"
                time_conditions.append("timestamp >= ?")
                params.append(formatted_date)
                
            if date_to:
                # Convert YYYYMMDD to YYYY-MM-DD
                formatted_date = f"{date_to[:4]}-{date_to[4:6]}-{date_to[6:8]}"
                time_conditions.append("timestamp <= ?")
                params.append(formatted_date)
            
            # Build base query for evidence retrieval
            base_query = """
                SELECT id, case_id, host, user, timestamp, artifact,
                       source_file, summary, data_json
                FROM evidence
            """
            
            if time_conditions:
                base_query += " WHERE " + " AND ".join(time_conditions)
            
            cursor = conn.execute(base_query, params)
            
            # Get all candidate records
            candidates = []
            for row in cursor.fetchall():
                try:
                    data_json = json.loads(row[8]) if row[8] else {}
                except json.JSONDecodeError as e:
                    LOGGER.warning(f"Invalid JSON in evidence record {row[0]}: {e}")
                    data_json = {}
                
                # Create searchable text from summary and data_json
                searchable_text = f"{row[7] or ''} {json.dumps(data_json)}"
                
                candidates.append({
                    'id': row[0],
                    'case_id': row[1],
                    'host': row[2],
                    'user': row[3],
                    'timestamp': row[4],
                    'artifact': row[5],
                    'source_file': row[6],
                    'summary': row[7],
                    'data_json': data_json,
                    'searchable_text': searchable_text
                })
            
            # Perform semantic similarity ranking
            scored_results = []
            for candidate in candidates:
                text_vector = embedder.embed(candidate['searchable_text'])
                similarity = embedder.similarity(query_vector, text_vector)
                
                if similarity > 0.1:  # Minimum similarity threshold
                    candidate['similarity_score'] = similarity
                    scored_results.append(candidate)
            
            # Sort by similarity score and limit results
            scored_results.sort(key=lambda x: x['similarity_score'], reverse=True)
            results = scored_results[:limit]
            
            # Remove searchable_text from final results
            for result in results:
                del result['searchable_text']
            
            return results
            
    except Exception as e:
        LOGGER.error(f"BHSM semantic search error: {e}")
        return []

class ModernLLM:
    """Modern LLM integration with advanced guardrails - now using global singleton"""
    
    def __init__(self, llm_folder: Optional[Path] = None):
        if llm_folder:
            # Use provided LLM folder and find the model file
            llm_path = Path(llm_folder)
            model_files = list(llm_path.glob("*.gguf"))
            if model_files:
                self.model_path = model_files[0]  # Use first .gguf file found
            else:
                # Fallback to auto-detection if no .gguf files found
                self.model_path = _find_llm_model_path(CONFIG.base_dir, "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf")
        else:
            # Use auto-detection for LLM model path
            self.model_path = _find_llm_model_path(CONFIG.base_dir, "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf")
        
        # Use global singleton instead of creating new instance
        self.llm = get_global_llm(str(self.model_path))
    
    def _initialize_model(self):
        """Initialize LLM with optimized settings - now uses global singleton"""
        # This method is kept for compatibility but now uses global singleton
        self.llm = get_global_llm(str(self.model_path))
    
    def generate_response(self, prompt: str, evidence: str) -> str:
        """Generate response with advanced guardrails"""
        if not self.llm:
            return "LLM not available"
        
        full_prompt = f"{SYSTEM_PROMPT}\n\nEvidence:\n{evidence}\n\nQuestion: {prompt}\n\nAnswer:"
        
        try:
            response = self.llm(
                full_prompt,
                max_tokens=CONFIG.llm_max_tokens,
                temperature=CONFIG.llm_temperature,
                top_p=CONFIG.llm_top_p,
                stop=["Question:", "Evidence:", "\n\n"],
                echo=False
            )
            
            answer = response['choices'][0]['text'].strip()
            
            # Advanced validation
            if self._validate_response(answer):
                return answer
            else:
                return "Response failed validation checks"
                
        except Exception as e:
            LOGGER.error(f"LLM generation error: {e}")
            return "Error generating response"
    
    def _validate_response(self, response: str) -> bool:
        """Enhanced response validation with forensic accuracy scoring"""
        if not response or len(response) < 10:
            return False
        
        # Check for clear hallucination indicators
        hallucination_patterns = [
            r"I believe", r"I think", r"in my opinion", r"I assume",
            r"I guess", r"I suppose", r"I imagine", r"I feel",
            r"it seems to me", r"I would say", r"I suspect"
        ]
        
        hallucination_count = 0
        for pattern in hallucination_patterns:
            if re.search(pattern, response, re.I):
                hallucination_count += 1
                LOGGER.warning(f"Potential hallucination detected: {pattern}")
        
        # Allow up to 1 minor hallucination indicator for forensic context
        if hallucination_count > 1:
            return False
        
        # Check for forensic evidence indicators (positive signals)
        evidence_indicators = [
            r"\d{4}-\d{2}-\d{2}",  # Dates
            r"\d{2}:\d{2}:\d{2}",  # Times
            r"registry", r"file", r"process", r"user", r"event",
            r"artifact", r"evidence", r"timeline", r"correlation"
        ]
        
        evidence_count = sum(1 for pattern in evidence_indicators 
                           if re.search(pattern, response, re.I))
        
        # Require at least some forensic terminology for validation
        if evidence_count < 2:
            LOGGER.warning("Response lacks sufficient forensic evidence indicators")
            return False
        
        # Check for reasonable length (not too short or excessively long)
        if len(response) > 5000:
            LOGGER.warning("Response excessively long, may indicate hallucination")
            return False
        
        return True

class ForensicAnalyzer:
    """Modern forensic analysis engine with ML enhancement"""
    
    def __init__(self, llm_folder: Optional[Path] = None):
        self.llm = ModernLLM(llm_folder)
        self.ml_analyzer = ForensicMLAnalyzer()
    
    @performance_monitor
    def analyze_computer_identity(self, case_id: str) -> Dict[str, Any]:
        """Analyze computer identity with modern SQL"""
        with get_database_connection() as conn:
            cursor = conn.execute("""
                SELECT DISTINCT
                    json_extract(data_json, '$.ComputerName') as computer_name,
                    json_extract(data_json, '$.SystemManufacturer') as make,
                    json_extract(data_json, '$.SystemProductName') as model,
                    json_extract(data_json, '$.SystemSerialNumber') as serial,
                    COUNT(*) as evidence_count
                FROM evidence 
                WHERE case_id = ? 
                  AND (json_extract(data_json, '$.ComputerName') IS NOT NULL
                    OR json_extract(data_json, '$.SystemManufacturer') IS NOT NULL)
                GROUP BY computer_name, make, model, serial
                ORDER BY evidence_count DESC
                LIMIT 1
            """, (case_id,))
            
            result = cursor.fetchone()
            if result:
                return {
                    'computer_name': result[0],
                    'make': result[1],
                    'model': result[2],
                    'serial': result[3],
                    'evidence_count': result[4]
                }
            
            return {}
    
    @performance_monitor
    def analyze_user_accounts(self, case_id: str) -> List[Dict[str, Any]]:
        """Analyze user accounts with modern aggregation"""
        with get_database_connection() as conn:
            cursor = conn.execute("""
                SELECT 
                    user,
                    json_extract(data_json, '$.SID') as sid,
                    MIN(timestamp) as first_activity,
                    MAX(timestamp) as last_activity,
                    COUNT(*) as activity_count,
                    COUNT(DISTINCT artifact) as artifacts
                FROM evidence 
                WHERE case_id = ? 
                  AND user IS NOT NULL 
                  AND user != ''
                  AND user NOT LIKE '%$'
                GROUP BY user, sid
                HAVING activity_count > 5
                ORDER BY activity_count DESC
            """, (case_id,))
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'username': row[0],
                    'sid': row[1],
                    'first_activity': row[2],
                    'last_activity': row[3],
                    'activity_count': row[4],
                    'artifacts': row[5]
                })
            
            return results
    
    @performance_monitor
    def analyze_usb_devices(self, case_id: str) -> List[Dict[str, Any]]:
        """Analyze USB devices with modern pattern matching"""
        with get_database_connection() as conn:
            cursor = conn.execute("""
                SELECT DISTINCT
                    json_extract(data_json, '$.DeviceManufacturer') as make,
                    json_extract(data_json, '$.DeviceModel') as model,
                    json_extract(data_json, '$.SerialNumber') as serial,
                    MIN(timestamp) as first_connected,
                    MAX(timestamp) as last_connected,
                    COUNT(*) as connection_count
                FROM evidence 
                WHERE case_id = ? 
                  AND artifact IN ('USBStorage', 'SetupAPI', 'MountPoints', 'Registry')
                  AND (summary LIKE '%usb%' OR summary LIKE '%removable%')
                GROUP BY make, model, serial
                ORDER BY first_connected DESC
            """, (case_id,))
            
            results = []
            for row in cursor.fetchall():
                results.append({
                    'make': row[0],
                    'model': row[1],
                    'serial': row[2],
                    'first_connected': row[3],
                    'last_connected': row[4],
                    'connection_count': row[5]
                })
            
            return results
    
    def answer_forensic_question(self, question: str, case_id: str, date_from: str = None, date_to: str = None, days_back: int = None) -> str:
        """
        OPTIMIZED forensic question answering using BHSM integration.
        Flow: deterministic â†’ PSI â†’ exact SQL â†’ LLM summarization
        Expected: 10x faster, higher accuracy through deterministic facts
        """
        start_time = time.perf_counter()
        
        # Step 1: Try deterministic answer first (fastest, most accurate)
        with get_database_connection() as conn:
            deterministic_answer = try_deterministic_answer(conn, question, case_id)
            if deterministic_answer:
                LOGGER.info(f"Deterministic answer found in {time.perf_counter() - start_time:.3f}s")
                return deterministic_answer
        
        # Step 2: Use PSI semantic search for evidence narrowing
        embedder, psi, bdh = get_bhsm_components()
        if not embedder or not psi:
            # Fallback to original method if BHSM unavailable
            # BHSM components required for semantic search
            return "BHSM components not available. Please install required dependencies."
        
        # Get enhanced BHSM components for adaptive learning
        retrospective_learning, adaptive_thresholds, enhanced_features, anomaly_detector = get_enhanced_bhsm_components()
        
        # Generate query embedding
        query_vec = embedder.embed(question)
        
        # Search PSI for semantically similar evidence (very fast)
        psi_hits = psi.search(query_vec, top_k=10)
        
        if not psi_hits:
            return "No relevant evidence found in semantic index."
        
        # Step 3: Fetch exact evidence rows from database
        evidence_ids = []
        for similarity, doc_id, doc_data in psi_hits:
            if doc_id.startswith("evidence_"):
                evidence_ids.append(doc_id.replace("evidence_", ""))
        
        if not evidence_ids:
            return "No evidence IDs found from semantic search."
        
        # Fetch original evidence with time filtering
        with get_database_connection() as conn:
            placeholders = ",".join(["?"] * len(evidence_ids))
            query = f"SELECT * FROM evidence WHERE id IN ({placeholders})"
            params = evidence_ids
            
            # Add time filtering
            if days_back:
                cutoff_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
                query += " AND timestamp >= ?"
                params.append(cutoff_date)
            
            if date_from:
                formatted_date = f"{date_from[:4]}-{date_from[4:6]}-{date_from[6:8]}"
                query += " AND timestamp >= ?"
                params.append(formatted_date)
                
            if date_to:
                formatted_date = f"{date_to[:4]}-{date_to[4:6]}-{date_to[6:8]}"
                query += " AND timestamp <= ?"
                params.append(formatted_date)
            
            if case_id:
                query += " AND case_id = ?"
                params.append(case_id)
            
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(query, params)
            evidence_rows = cursor.fetchall()
        
        if not evidence_rows:
            return "No evidence found matching time and case filters."
        
        # Step 4: Extract deterministic facts from evidence
        facts = []
        evidence_items = []  # For ML analysis
        
        for row in evidence_rows:
            row_dict = dict(row)
            evidence_items.append(row_dict)  # Store for ML analysis
            
            # Extract structured facts based on artifact type
            artifact = row_dict.get('artifact', '').lower()
            data_json = row_dict.get('data_json', '') or ''
            summary = row_dict.get('summary', '') or ''
            
            fact = {
                'type': artifact,
                'timestamp': row_dict.get('timestamp'),
                'summary': summary[:100]  # Truncate for conciseness
            }
            
            # Extract specific facts based on type
            if 'usb' in artifact or 'USB' in data_json:
                serial_match = re.search(r'SerialNumber["\s]*[:=]["\s]*([A-Za-z0-9]+)', data_json)
                if serial_match:
                    fact['usb_serial'] = serial_match.group(1)
                name_match = re.search(r'FriendlyName["\s]*[:=]["\s]*([^"]+)', data_json)
                if name_match:
                    fact['device_name'] = name_match.group(1)
            
            elif 'network' in artifact or 'IP' in data_json:
                ip_matches = re.findall(r'\b(?:[0-9]{1,3}\.){3}[0-9]{1,3}\b', data_json)
                if ip_matches:
                    fact['ip_addresses'] = ip_matches[:3]  # Limit to first 3
            
            elif 'registry' in artifact or 'HKEY' in data_json:
                key_match = re.search(r'(HKEY_[A-Z_]+\\[^"]+)', data_json)
                if key_match:
                    fact['registry_key'] = key_match.group(1)[:100]
            
            facts.append(fact)
        
        # Step 4.5: Apply Enhanced BHSM Adaptive Learning
        enhanced_facts = []
        for i, fact in enumerate(facts):
            evidence_text = fact['summary'] + ' ' + evidence_items[i].get('data_json', '')
            evidence_type = fact['type']
            
            # Extract enhanced forensic features
            enhanced_features_vec = enhanced_features.extract_forensic_features(
                evidence_text, 
                evidence_items[i]
            )
            
            # Apply adaptive thresholds for confidence assessment
            base_similarity = psi_hits[i][0] if i < len(psi_hits) else 0.5
            confidence_assessment = adaptive_thresholds.assess_evidence_confidence(
                evidence_type, 
                base_similarity, 
                'medium'
            )
            
            # Apply retrospective learning adjustments
            adjusted_confidence = retrospective_learning.calculate_confidence_adjustment(
                enhanced_features_vec,
                confidence_assessment['confidence_score'],
                evidence_type
            )
            
            # Enhance fact with adaptive learning insights
            enhanced_fact = fact.copy()
            enhanced_fact['confidence_score'] = adjusted_confidence
            enhanced_fact['adaptive_threshold'] = confidence_assessment['adjusted_threshold']
            enhanced_fact['evidence_detected'] = confidence_assessment['evidence_detected']
            
            # Only include high-confidence evidence
            if adjusted_confidence > confidence_assessment['adjusted_threshold']:
                enhanced_facts.append(enhanced_fact)
        
        # Use enhanced facts instead of original facts
        facts = enhanced_facts
        
        # Step 5: Use LLM only for summarization with concise facts
        if facts and get_global_llm():
            # Build concise prompt with extracted facts
            facts_text = "EXTRACTED FACTS:\n"
            for i, fact in enumerate(facts[:8]):  # Limit to 8 facts for context size
                facts_text += f"{i+1}. {fact['type'].upper()}: {fact['summary']}"
                if fact.get('usb_serial'):
                    facts_text += f" (USB Serial: {fact['usb_serial']})"
                if fact.get('device_name'):
                    facts_text += f" (Device: {fact['device_name']})"
                if fact.get('ip_addresses'):
                    facts_text += f" (IPs: {', '.join(fact['ip_addresses'])})"
                if fact.get('registry_key'):
                    facts_text += f" (Registry: {fact['registry_key']})"
                if fact.get('timestamp'):
                    try:
                        dt = datetime.fromtimestamp(fact['timestamp'])
                        facts_text += f" (Time: {dt.strftime('%m/%d %H:%M')})"
                    except:
                        pass
                facts_text += "\n"
            
            prompt = f"""Question: {question}

{facts_text}

INSTRUCTION: Answer the question using ONLY the facts above. If facts are insufficient, state "Insufficient evidence". Be concise and factual.

Answer:"""
            
            try:
                llm = get_global_llm()
                response = llm(
                    prompt,
                    max_tokens=300,  # Reduced for faster inference
                    temperature=0.1,  # Low temperature for factual responses
                    stop=["Question:", "FACTS:", "\n\n"],
                    echo=False
                )
                
                answer = response['choices'][0]['text'].strip()
                
                # Step 6: Validate LLM response against deterministic facts
                validation_result = ForensicValidator.validate_llm_response(answer, facts, question)
                
                # Use corrected response if validation failed or facts were missing
                if not validation_result['is_valid'] or validation_result['missing_facts']:
                    LOGGER.warning(f"LLM response validation failed (confidence: {validation_result['confidence']:.2f}), using corrected response")
                    final_answer = validation_result['corrected_response']
                else:
                    final_answer = answer
                
                # Step 7: Reward learning - update BDH based on validation confidence
                if bdh and validation_result['confidence'] > 0.7:  # Only reward high-confidence answers
                    reward_value = validation_result['confidence'] * 0.5  # Scale reward by confidence
                    for similarity, doc_id, doc_data in psi_hits[:5]:  # Top 5 hits
                        bdh.add_or_update(doc_id, doc_data.get('vec', query_vec), valence=0.2)
                        bdh.reward_gated_update(doc_id, query_vec, reward=reward_value)
                
                elapsed = time.perf_counter() - start_time
                LOGGER.info(f"Optimized answer generated in {elapsed:.3f}s using {len(facts)} facts (confidence: {validation_result['confidence']:.2f})")
                
                return final_answer
                
            except Exception as e:
                LOGGER.error(f"LLM generation error: {e}")
                # Fallback to fact summary
                return f"Based on {len(facts)} evidence items: " + "; ".join([f['summary'] for f in facts[:5]])
        
        # Fallback: return fact summary without LLM
        if facts:
            # Step 5: Apply ML enhancement for behavioral questions
            question_id = self._identify_question_id(question)
            
            # Create deterministic result
            deterministic_result = {
                'answer': f"Found {len(facts)} evidence items: " + "; ".join([f['summary'] for f in facts[:5]]),
                'evidence_count': len(facts),
                'confidence': 0.8,  # Base confidence for deterministic facts
                'facts': facts
            }
            
            # Enhance with ML analysis
            enhanced_result = self.ml_analyzer.enhance_forensic_analysis(
                question_id, evidence_items, deterministic_result
            )
            
            # Return enhanced answer with ML insights
            base_answer = enhanced_result['answer']
            if enhanced_result.get('ml_enhanced') and enhanced_result.get('ml_analysis'):
                ml_analysis = enhanced_result['ml_analysis']
                confidence = enhanced_result.get('confidence', 0.8)
                
                # Add ML insights to answer for behavioral questions (Q6-Q12)
                if question_id in ['Q6', 'Q7', 'Q8', 'Q9', 'Q10', 'Q11', 'Q12']:
                    anomaly_info = ml_analysis.get('anomaly_detection', {})
                    if anomaly_info.get('is_anomalous'):
                        base_answer += f" [ML Analysis: Anomalous patterns detected with {anomaly_info.get('confidence', 0)*100:.1f}% confidence]"
                    
                    optimized_info = ml_analysis.get('optimized_scoring', {})
                    if optimized_info.get('optimized_score', 0) > 0.7:
                        base_answer += f" [High significance score: {optimized_info.get('optimized_score', 0)*100:.1f}%]"
                
                base_answer += f" [Overall confidence: {confidence*100:.1f}%]"
            
            return base_answer
        
        return "No relevant evidence found."
    
    def _identify_question_id(self, question: str) -> str:
        """Identify which standard forensic question this matches, or return 'AD_HOC' for ad-hoc questions"""
        question_lower = question.lower()
        
        # Direct keyword matching for specific questions
        if any(term in question_lower for term in ['computer name', 'hostname', 'computername']):
            return 'Q1'
        elif any(term in question_lower for term in ['make', 'model', 'serial', 'manufacturer', 'hardware']):
            return 'Q2'
        elif any(term in question_lower for term in ['hard drive', 'internal drive', 'internal storage', 'disk']) and 'cloud' not in question_lower:
            return 'Q3'
        elif any(term in question_lower for term in ['user account', 'username', 'profile', 'sid']):
            return 'Q4'
        elif any(term in question_lower for term in ['primary user', 'main user', 'most active', 'owner']):
            return 'Q5'
        elif any(term in question_lower for term in ['anti-forensic', 'tamper', 'evidence elimination', 'data destruction']):
            return 'Q6'
        elif any(term in question_lower for term in ['usb', 'removable storage', 'external device']):
            return 'Q7'
        elif any(term in question_lower for term in ['file transfer', 'copy', 'move', 'removable']):
            return 'Q8'
        elif any(term in question_lower for term in ['cloud storage', 'dropbox', 'onedrive', 'google drive', 'sync']):
            return 'Q9'
        elif any(term in question_lower for term in ['screenshot', 'screen capture', 'snipping', 'print screen']):
            return 'Q10'
        elif any(term in question_lower for term in ['print', 'printer', 'document', 'job', 'spool']):
            return 'Q11'
        elif any(term in question_lower for term in ['software', 'install', 'program', 'application', 'modify']):
            return 'Q12'
        
        # Fallback: Map questions to IDs based on keywords from STANDARD_FORENSIC_QUESTIONS
        for q in STANDARD_FORENSIC_QUESTIONS:
            keywords = [kw.lower() for kw in q.get('keywords', [])]
            if any(keyword in question_lower for keyword in keywords):
                return q['id']
        
        # Additional behavioral pattern matching for ad-hoc questions
        if any(term in question_lower for term in ['delete', 'clear', 'wipe', 'hide']):
            return 'Q6'  # Anti-forensic activity
        elif 'file' in question_lower and any(term in question_lower for term in ['transfer', 'copy', 'move']):
            return 'Q8'  # File transfers
        
        return 'AD_HOC'  # Ad-hoc question

    # LEGACY METHOD REMOVED - answer_forensic_question_legacy() has been replaced
    # by the superior BHSM-powered answer_forensic_question() method above
    
    def _apply_validation_feedback(self, analysis: str, validation_results: Dict[str, Any]) -> str:
        """Apply validation feedback to improve analysis accuracy"""
        
        validated_claims = validation_results.get('validated_claims', [])
        unvalidated_claims = validation_results.get('unvalidated_claims', [])
        
        if not validated_claims and not unvalidated_claims:
            return analysis
        
        feedback_section = "\n\nVALIDATION FEEDBACK:\n"
        
        if validated_claims:
            feedback_section += f"âœ“ VERIFIED CLAIMS ({len(validated_claims)}):\n"
            for claim_info in validated_claims:
                claim = claim_info['claim']
                pattern = claim_info['pattern']
                feedback_section += f"  â€¢ {claim} (Pattern: {pattern})\n"
        
        if unvalidated_claims:
            feedback_section += f"âš  UNVERIFIED CLAIMS ({len(unvalidated_claims)}):\n"
            for claim in unvalidated_claims[:3]:  # Limit to 3 for brevity
                feedback_section += f"  â€¢ {claim}\n"
        
        confidence_adj = validation_results.get('confidence_adjustment', 0)
        confidence_level = "HIGH" if confidence_adj > 0.15 else "MEDIUM" if confidence_adj > 0 else "LOW"
        feedback_section += f"\nOVERALL CONFIDENCE: {confidence_level} (adjustment: {confidence_adj:+.2f})\n"
        
        return analysis + feedback_section
    
    # LEGACY METHOD REMOVED - _iterative_refinement() was only used by legacy search
    def _analyze_additional_evidence(self, additional_evidence: List[Dict], initial_analysis: str) -> str:
        """Analyze additional evidence in context of initial findings"""
        
        findings = []
        
        # Group additional evidence by type
        evidence_by_type = defaultdict(list)
        for evidence in additional_evidence:
            artifact = evidence.get('artifact', 'unknown')
            evidence_by_type[artifact].append(evidence)
        
        # Analyze each evidence type
        for artifact, evidence_list in evidence_by_type.items():
            if len(evidence_list) >= 2:  # Only analyze types with multiple items
                findings.append(f"â€¢ {artifact.upper()}: {len(evidence_list)} additional items found")
                
                # Look for patterns in this evidence type
                timestamps = [e.get('timestamp') for e in evidence_list if e.get('timestamp')]
                if len(timestamps) >= 2:
                    time_span = max(timestamps) - min(timestamps)
                    findings.append(f"  - Time span: {time_span} seconds")
                
                # Look for user patterns
                users = set(e.get('username') for e in evidence_list if e.get('username'))
                if users:
                    findings.append(f"  - Users involved: {', '.join(users)}")
        
        # Cross-reference with initial analysis
        initial_lower = initial_analysis.lower()
        cross_refs = []
        
        for evidence in additional_evidence:
            summary = evidence.get('summary', '').lower()
            if any(keyword in summary for keyword in ['suspicious', 'anomaly', 'unusual', 'unauthorized']):
                cross_refs.append(f"â€¢ Corroborates suspicious activity: {evidence.get('summary', '')[:80]}")
        
        result = "\n".join(findings)
        if cross_refs:
            result += "\n\nCROSS-REFERENCE VALIDATION:\n" + "\n".join(cross_refs)
        
        return result
    
    def _generate_enhanced_structured_analysis(self, evidence_results: List[Dict], question: str) -> str:
        """Generate enhanced structured analysis when LLM unavailable"""
        
        # Analyze evidence patterns for better insights
        artifact_counts = defaultdict(int)
        time_range = {"earliest": None, "latest": None}
        users = set()
        hosts = set()
        correlations = defaultdict(list)
        
        for result in evidence_results:
            # Count artifact types
            artifact = result.get('artifact', 'unknown')
            artifact_counts[artifact] += 1
            
            # Track time range
            if result.get('timestamp'):
                ts = result['timestamp']
                if not time_range["earliest"] or ts < time_range["earliest"]:
                    time_range["earliest"] = ts
                if not time_range["latest"] or ts > time_range["latest"]:
                    time_range["latest"] = ts
            
            # Track users and hosts
            if result.get('username'):
                users.add(result['username'])
            if result.get('hostname'):
                hosts.add(result['hostname'])
            
            # Track correlations
            if result.get('correlation_count', 0) > 1:
                key = f"{result.get('username', 'Unknown')}@{result.get('hostname', 'Unknown')}"
                correlations[key].append(result)
        
        # Generate enhanced structured response
        analysis = f"ENHANCED FORENSIC ANALYSIS - {question}\n"
        analysis += "=" * 60 + "\n\n"
        
        analysis += "EVIDENCE SUMMARY:\n"
        analysis += f"â€¢ Total artifacts analyzed: {len(evidence_results)}\n"
        analysis += f"â€¢ Artifact types: {', '.join(sorted(artifact_counts.keys()))}\n"
        analysis += f"â€¢ Users involved: {', '.join(sorted(users)) if users else 'Unknown'}\n"
        analysis += f"â€¢ Hosts involved: {', '.join(sorted(hosts)) if hosts else 'Unknown'}\n"
        
        if time_range["earliest"] and time_range["latest"]:
            start_time = datetime.fromtimestamp(time_range["earliest"])
            end_time = datetime.fromtimestamp(time_range["latest"])
            duration = end_time - start_time
            analysis += f"â€¢ Time range: {start_time.strftime('%Y-%m-%d %H:%M')} to {end_time.strftime('%Y-%m-%d %H:%M')}\n"
            analysis += f"â€¢ Duration: {duration}\n"
        
        analysis += "\nKEY FINDINGS (ranked by relevance):\n"
        for i, result in enumerate(evidence_results[:8], 1):
            score = result.get('final_ranking_score', 0)
            cluster_info = f" (clustered with {result.get('cluster_size', 1)} events)" if result.get('cluster_size', 1) > 1 else ""
            correlation_info = f" (correlated with {result.get('correlation_count', 1)} events)" if result.get('correlation_count', 1) > 1 else ""
            
            timestamp_str = ""
            if result.get('timestamp'):
                dt = datetime.fromtimestamp(result['timestamp'])
                timestamp_str = f"[{dt.strftime('%m/%d %H:%M')}] "
            
            analysis += f"{i}. {timestamp_str}{result.get('artifact', 'UNKNOWN').upper()}: "
            analysis += f"{result.get('summary', 'No summary available')[:100]}"
            analysis += f"{cluster_info}{correlation_info} (score: {score:.2f})\n"
        
        # Add correlation analysis if significant correlations found
        if correlations:
            analysis += "\nCORRELATION ANALYSIS:\n"
            for user_host, correlated_events in correlations.items():
                if len(correlated_events) > 2:
                    analysis += f"â€¢ {user_host}: {len(correlated_events)} correlated events detected\n"
        
        # Add pattern analysis
        analysis += "\nPATTERN ANALYSIS:\n"
        if 'usb' in artifact_counts and artifact_counts['usb'] > 1:
            analysis += "â€¢ Multiple USB device activities detected - potential data transfer\n"
        if 'network' in artifact_counts and 'filesystem' in artifact_counts:
            analysis += "â€¢ Network and filesystem activity correlation - potential exfiltration pattern\n"
        if len(users) > 1:
            analysis += f"â€¢ Multiple user accounts involved ({len(users)} users) - potential privilege escalation\n"
        
        return analysis
    
    def _fallback_forensic_answer(self, question: str, case_id: str) -> str:
        """Fallback forensic answer using direct database analysis when LLM is unavailable"""
        
        question_lower = question.lower()
        
        # Computer identity questions
        if "computer" in question_lower and ("name" in question_lower or "make" in question_lower or "model" in question_lower or "serial" in question_lower):
            identity = self.analyze_computer_identity(case_id)
            if identity and any(identity.values()):
                parts = []
                if identity.get('computer_name'):
                    parts.append(f"Computer Name: {identity['computer_name']}")
                if identity.get('make'):
                    parts.append(f"Make: {identity['make']}")
                if identity.get('model'):
                    parts.append(f"Model: {identity['model']}")
                if identity.get('serial'):
                    parts.append(f"Serial: {identity['serial']}")
                return "; ".join(parts) if parts else "No computer identity information found."
            return "No computer identity information found."
        
        # User account questions
        elif "user" in question_lower and "account" in question_lower:
            accounts = self.analyze_user_accounts(case_id)
            if accounts:
                account_info = []
                for account in accounts[:5]:  # Top 5 accounts
                    info = f"User: {account.get('username', 'Unknown')}"
                    if account.get('activity_count'):
                        info += f" (Activity: {account['activity_count']})"
                    if account.get('last_activity'):
                        info += f" (Last: {account['last_activity']})"
                    account_info.append(info)
                return "; ".join(account_info)
            return "No user account information found."
        
        # USB device questions
        elif "usb" in question_lower or ("removable" in question_lower and "storage" in question_lower):
            devices = self.analyze_usb_devices(case_id)
            if devices:
                device_info = []
                for device in devices[:3]:  # Top 3 devices
                    info = f"Device: {device.get('device_name', 'Unknown')}"
                    if device.get('serial_number'):
                        info += f" (Serial: {device['serial_number']})"
                    if device.get('first_connected'):
                        info += f" (First: {device['first_connected']})"
                    device_info.append(info)
                return "; ".join(device_info)
            return "No USB device information found."
        
        # Network/Internet questions
        elif "network" in question_lower or "internet" in question_lower or "connection" in question_lower:
            with get_database_connection() as conn:
                conn.row_factory = sqlite3.Row
                results = conn.execute("""
                    SELECT summary, data_json FROM evidence 
                    WHERE case_id = ? AND (artifact LIKE '%Network%' OR summary LIKE '%network%' OR summary LIKE '%connection%')
                    ORDER BY timestamp DESC LIMIT 5
                """, (case_id,)).fetchall()
                
                if results:
                    network_info = []
                    for row in results:
                        try:
                            data = json.loads(row['data_json']) if row['data_json'] else {}
                            if data.get('RemoteAddress'):
                                info = f"Connection to {data['RemoteAddress']}"
                                if data.get('RemotePort'):
                                    info += f":{data['RemotePort']}"
                                if data.get('ProcessName'):
                                    info += f" via {data['ProcessName']}"
                                network_info.append(info)
                        except:
                            network_info.append(row['summary'])
                    return "; ".join(network_info[:3]) if network_info else "Network activity found but details unavailable."
            return "No network activity information found."
        
        # File-related questions
        elif "file" in question_lower and ("transfer" in question_lower or "copy" in question_lower or "move" in question_lower):
            with get_database_connection() as conn:
                conn.row_factory = sqlite3.Row
                results = conn.execute("""
                    SELECT summary, data_json FROM evidence 
                    WHERE case_id = ? AND (artifact LIKE '%File%' OR summary LIKE '%file%')
                    ORDER BY timestamp DESC LIMIT 5
                """, (case_id,)).fetchall()
                
                if results:
                    file_info = []
                    for row in results:
                        try:
                            data = json.loads(row['data_json']) if row['data_json'] else {}
                            if data.get('FileName'):
                                info = f"File: {data['FileName']}"
                                if data.get('FileSize'):
                                    info += f" ({data['FileSize']} bytes)"
                                file_info.append(info)
                        except:
                            file_info.append(row['summary'])
                    return "; ".join(file_info[:3]) if file_info else "File activity found but details unavailable."
            return "No file transfer information found."
        
        # Email questions
        elif "email" in question_lower:
            with get_database_connection() as conn:
                conn.row_factory = sqlite3.Row
                results = conn.execute("""
                    SELECT summary, data_json FROM evidence 
                    WHERE case_id = ? AND (artifact LIKE '%Email%' OR summary LIKE '%email%')
                    ORDER BY timestamp DESC LIMIT 3
                """, (case_id,)).fetchall()
                
                if results:
                    email_info = []
                    for row in results:
                        try:
                            data = json.loads(row['data_json']) if row['data_json'] else {}
                            if data.get('Subject'):
                                info = f"Email: {data['Subject']}"
                                if data.get('To'):
                                    info += f" to {data['To']}"
                                if data.get('HasAttachment'):
                                    info += " (with attachment)"
                                email_info.append(info)
                        except:
                            email_info.append(row['summary'])
                    return "; ".join(email_info) if email_info else "Email activity found but details unavailable."
            return "No email information found."
        
        # Application/software questions
        elif "application" in question_lower or "software" in question_lower or "program" in question_lower:
            with get_database_connection() as conn:
                conn.row_factory = sqlite3.Row
                results = conn.execute("""
                    SELECT summary, data_json FROM evidence 
                    WHERE case_id = ? AND (artifact LIKE '%Application%' OR summary LIKE '%application%' OR summary LIKE '%exe%')
                    ORDER BY timestamp DESC LIMIT 5
                """, (case_id,)).fetchall()
                
                if results:
                    app_info = []
                    for row in results:
                        try:
                            data = json.loads(row['data_json']) if row['data_json'] else {}
                            if data.get('ApplicationName'):
                                info = f"App: {data['ApplicationName']}"
                                if data.get('ExecutionCount'):
                                    info += f" (executed {data['ExecutionCount']} times)"
                                app_info.append(info)
                        except:
                            app_info.append(row['summary'])
                    return "; ".join(app_info[:3]) if app_info else "Application activity found but details unavailable."
            return "No application information found."
        
        # Security/event questions
        elif "security" in question_lower or "event" in question_lower or "login" in question_lower:
            with get_database_connection() as conn:
                conn.row_factory = sqlite3.Row
                results = conn.execute("""
                    SELECT summary, data_json FROM evidence 
                    WHERE case_id = ? AND (artifact LIKE '%Event%' OR summary LIKE '%login%' OR summary LIKE '%security%')
                    ORDER BY timestamp DESC LIMIT 5
                """, (case_id,)).fetchall()
                
                if results:
                    event_info = []
                    for row in results:
                        try:
                            data = json.loads(row['data_json']) if row['data_json'] else {}
                            if data.get('EventID'):
                                info = f"Event {data['EventID']}"
                                if data.get('FailureReason'):
                                    info += f": {data['FailureReason']}"
                                elif data.get('TargetUser'):
                                    info += f" for user {data['TargetUser']}"
                                event_info.append(info)
                        except:
                            event_info.append(row['summary'])
                    return "; ".join(event_info[:3]) if event_info else "Security events found but details unavailable."
            return "No security event information found."
        
        # Generic fallback
        return "Unable to answer this question with available evidence. Consider running with LLM model for enhanced analysis."
    
    def autonomous_analysis(self, case_id: str, llm_provider: LLMProvider = None) -> Dict[str, Any]:
        """Autonomously answer all 12 standard forensic questions and generate comprehensive report"""
        LOGGER.info(f"Starting autonomous analysis for case {case_id}")
        start_time = time.perf_counter()
        
        results = {
            'case_id': case_id,
            'analysis_timestamp': datetime.now().isoformat(),
            'questions_answered': 0,
            'total_questions': len(STANDARD_FORENSIC_QUESTIONS),
            'answers': {},
            'evidence_summary': {},
            'confidence_scores': {},
            'processing_time': 0,
            'llm_provider': llm_provider.provider_type if llm_provider else 'none'
        }
        
        with get_database_connection() as conn:
            # First, gather overall evidence statistics
            cursor = conn.execute("""
                SELECT 
                    COUNT(*) as total_evidence,
                    COUNT(DISTINCT artifact) as artifacts,
                    MIN(timestamp) as earliest_event,
                    MAX(timestamp) as latest_event
                FROM evidence 
                WHERE case_id = ?
            """, (case_id,))
            
            stats = cursor.fetchone()
            if stats:
                results['evidence_summary'] = {
                    'total_evidence_items': stats[0],
                    'artifacts_found': stats[1],
                    'timeline_start': stats[2],
                    'timeline_end': stats[3]
                }
            
            # Process each standard question
            for question_data in STANDARD_FORENSIC_QUESTIONS:
                question_id = question_data['id']
                question = question_data['question']
                extractor_name = question_data['extractor']
                
                LOGGER.info(f"Processing {question_id}: {question}")
                
                try:
                    # Step 1: Try deterministic extraction first
                    deterministic_answer = try_deterministic_answer(conn, question, case_id)
                    
                    if deterministic_answer:
                        # Deterministic answer found - enhance with ML for behavioral questions
                        supporting_evidence = self._get_supporting_evidence(conn, question_data, case_id)
                        
                        deterministic_result = {
                            'question': question,
                            'answer': deterministic_answer,
                            'method': 'deterministic',
                            'confidence': 0.95,
                            'supporting_evidence': supporting_evidence
                        }
                        
                        # Apply ML enhancement
                        enhanced_result = self.ml_analyzer.enhance_forensic_analysis(
                            question_id, supporting_evidence, deterministic_result
                        )
                        
                        # Update with ML insights
                        if enhanced_result.get('ml_enhanced'):
                            enhanced_result['method'] = 'deterministic+ml'
                            if enhanced_result.get('ml_analysis'):
                                ml_analysis = enhanced_result['ml_analysis']
                                enhanced_result['ml_insights'] = {
                                    'anomaly_detected': ml_analysis.get('anomaly_detection', {}).get('is_anomalous', False),
                                    'anomaly_confidence': ml_analysis.get('anomaly_detection', {}).get('confidence', 0.0),
                                    'optimized_score': ml_analysis.get('optimized_scoring', {}).get('optimized_score', 0.5),
                                    'ml_confidence': ml_analysis.get('ml_confidence', 0.5)
                                }
                        
                        results['answers'][question_id] = enhanced_result
                        results['confidence_scores'][question_id] = enhanced_result.get('confidence', 0.95)
                        
                    else:
                        # Step 2: Use LLM with semantic search if available
                        if llm_provider and llm_provider.is_available():
                            llm_answer = self._llm_assisted_answer(conn, question_data, case_id, llm_provider)
                            
                            # Enhance LLM answer with ML analysis
                            supporting_evidence = llm_answer.get('supporting_evidence', [])
                            enhanced_llm_result = self.ml_analyzer.enhance_forensic_analysis(
                                question_id, supporting_evidence, llm_answer
                            )
                            
                            # Add ML insights to LLM result
                            if enhanced_llm_result.get('ml_enhanced'):
                                enhanced_llm_result['method'] = 'llm+ml'
                                if enhanced_llm_result.get('ml_analysis'):
                                    ml_analysis = enhanced_llm_result['ml_analysis']
                                    enhanced_llm_result['ml_insights'] = {
                                        'anomaly_detected': ml_analysis.get('anomaly_detection', {}).get('is_anomalous', False),
                                        'anomaly_confidence': ml_analysis.get('anomaly_detection', {}).get('confidence', 0.0),
                                        'optimized_score': ml_analysis.get('optimized_scoring', {}).get('optimized_score', 0.5),
                                        'ml_confidence': ml_analysis.get('ml_confidence', 0.5)
                                    }
                            
                            results['answers'][question_id] = enhanced_llm_result
                            results['confidence_scores'][question_id] = enhanced_llm_result.get('confidence', 0.5)
                        else:
                            # No LLM available - provide evidence summary
                            evidence_summary = self._get_evidence_summary(conn, question_data, case_id)
                            results['answers'][question_id] = {
                                'question': question,
                                'answer': f"Evidence found but requires LLM analysis: {evidence_summary}",
                                'method': 'evidence_summary',
                                'confidence': 0.3,
                                'supporting_evidence': self._get_supporting_evidence(conn, question_data, case_id)
                            }
                            results['confidence_scores'][question_id] = 0.3
                    
                    results['questions_answered'] += 1
                    
                except Exception as e:
                    LOGGER.error(f"Error processing {question_id}: {e}")
                    results['answers'][question_id] = {
                        'question': question,
                        'answer': f"Error processing question: {str(e)}",
                        'method': 'error',
                        'confidence': 0.0,
                        'supporting_evidence': []
                    }
                    results['confidence_scores'][question_id] = 0.0
        
        results['processing_time'] = time.perf_counter() - start_time
        results['average_confidence'] = sum(results['confidence_scores'].values()) / len(results['confidence_scores']) if results['confidence_scores'] else 0.0
        
        # Add ML enhancement summary
        ml_enhanced_count = sum(1 for answer in results['answers'].values() if answer.get('ml_enhanced'))
        anomalies_detected = sum(1 for answer in results['answers'].values() 
                               if answer.get('ml_insights', {}).get('anomaly_detected'))
        
        results['ml_summary'] = {
            'ml_enhanced_questions': ml_enhanced_count,
            'anomalies_detected': anomalies_detected,
            'ml_available': self.ml_analyzer.ml_available
        }
        
        LOGGER.info(f"Autonomous analysis completed in {results['processing_time']:.2f}s")
        LOGGER.info(f"Questions answered: {results['questions_answered']}/{results['total_questions']}")
        LOGGER.info(f"Average confidence: {results['average_confidence']:.2f}")
        LOGGER.info(f"ML enhanced questions: {ml_enhanced_count}/{results['total_questions']}")
        if anomalies_detected > 0:
            LOGGER.info(f"Anomalous patterns detected in {anomalies_detected} questions")
        
        return results
    
    def _get_supporting_evidence(self, conn: sqlite3.Connection, question_data: Dict, case_id: str, limit: int = 5) -> List[Dict]:
        """Get supporting evidence for a question"""
        keywords = question_data.get('keywords', [])
        evidence = []
        
        if keywords:
            # Build search query for relevant evidence
            keyword_conditions = " OR ".join([f"LOWER(summary) LIKE LOWER('%{kw}%') OR LOWER(data_json) LIKE LOWER('%{kw}%')" for kw in keywords])
            
            cursor = conn.execute(f"""
                SELECT timestamp, artifact, summary, data_json
                FROM evidence 
                WHERE case_id = ? AND ({keyword_conditions})
                ORDER BY timestamp DESC
                LIMIT ?
            """, (case_id, limit))
            
            for row in cursor.fetchall():
                evidence.append({
                    'timestamp': row[0],
                    'artifact': row[1],
                    'summary': row[2][:200] if row[2] else '',  # Truncate for readability
                    'relevance': 'keyword_match'
                })
        
        return evidence
    
    def _get_evidence_summary(self, conn: sqlite3.Connection, question_data: Dict, case_id: str) -> str:
        """Get a brief evidence summary for a question"""
        keywords = question_data.get('keywords', [])
        
        if keywords:
            keyword_conditions = " OR ".join([f"LOWER(summary) LIKE LOWER('%{kw}%')" for kw in keywords])
            
            cursor = conn.execute(f"""
                SELECT COUNT(*) as count, artifact
                FROM evidence 
                WHERE case_id = ? AND ({keyword_conditions})
                GROUP BY artifact
                ORDER BY count DESC
                LIMIT 3
            """, (case_id,))
            
            results = cursor.fetchall()
            if results:
                summary_parts = []
                for count, artifact in results:
                    summary_parts.append(f"{count} {artifact} artifacts")
                return f"Found {', '.join(summary_parts)}"
        
        return "Limited evidence available"
    
    def _llm_assisted_answer(self, conn: sqlite3.Connection, question_data: Dict, case_id: str, llm_provider: LLMProvider) -> Dict:
        """Use LLM to answer question with semantic search"""
        question = question_data['question']
        
        # Get relevant evidence using PSI if available
        embedder, psi, bdh = get_bhsm_components()
        evidence_text = ""
        
        if embedder and psi:
            # Use semantic search
            qvec = embedder.embed(question)
            hits = psi.search(qvec, top_k=5)
            
            if hits:
                # Get original evidence
                evidence_ids = [h[1].replace("evidence_", "") for h in hits if h[1].startswith("evidence_")]
                if evidence_ids:
                    placeholders = ','.join(['?'] * len(evidence_ids))
                    cursor = conn.execute(f"""
                        SELECT timestamp, artifact, summary, data_json
                        FROM evidence 
                        WHERE case_id = ? AND id IN ({placeholders})
                        ORDER BY timestamp DESC
                    """, [case_id] + evidence_ids)
                    
                    evidence_rows = cursor.fetchall()
                    evidence_parts = []
                    for row in evidence_rows:
                        evidence_parts.append(f"[{row[1]}] {row[2][:150]}")
                    evidence_text = "\n".join(evidence_parts[:5])
        
        # Fallback to keyword search if no PSI
        if not evidence_text:
            keywords = question_data.get('keywords', [])
            if keywords:
                keyword_conditions = " OR ".join([f"LOWER(summary) LIKE LOWER('%{kw}%')" for kw in keywords])
                cursor = conn.execute(f"""
                    SELECT timestamp, artifact, summary
                    FROM evidence 
                    WHERE case_id = ? AND ({keyword_conditions})
                    ORDER BY timestamp DESC
                    LIMIT 5
                """, (case_id,))
                
                evidence_rows = cursor.fetchall()
                evidence_parts = []
                for row in evidence_rows:
                    evidence_parts.append(f"[{row[1]}] {row[2][:150]}")
                evidence_text = "\n".join(evidence_parts)
        
        # Generate LLM response
        if evidence_text:
            prompt = f"""Question: {question}

Evidence:
{evidence_text}

Based on the forensic evidence above, provide a concise, factual answer to the question. Focus on specific details found in the evidence. If the evidence is insufficient, state what was found and what additional evidence would be needed."""
            
            llm_response = llm_provider.generate_response(prompt, max_tokens=300, temperature=0.1)
            
            # Validate response
            validation_result = ForensicValidator.validate_llm_response(llm_response, [], question)
            
            return {
                'question': question,
                'answer': llm_response,
                'method': 'llm_semantic',
                'confidence': validation_result.get('confidence', 0.6),
                'supporting_evidence': self._get_supporting_evidence(conn, question_data, case_id),
                'validation': validation_result
            }
        else:
            return {
                'question': question,
                'answer': "No relevant evidence found for this question.",
                'method': 'no_evidence',
                'confidence': 0.1,
                'supporting_evidence': []
            }

class ForensicProcessor:
    """Modern forensic data processor for Plaso timeline integration"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.analyzer = ForensicAnalyzer()
        
    def initialize_database(self) -> None:
        """Initialize database with optimized schema"""
        with get_database_connection() as conn:
            conn.executescript(DATABASE_SCHEMA)
            conn.commit()
        LOGGER.info("Database initialized with optimized schema")
    
    # CSV processing removed - using direct artifact â†’ SQLite workflow only
    
    # Plaso chunk processing removed - using direct artifact â†’ SQLite via custom output module
    
    # Plaso timestamp parsing removed - handled directly by custom output module
    
    def answer_forensic_question(self, question: str, case_id: str, date_from: str = None, date_to: str = None, days_back: int = None) -> str:
        """Answer forensic questions using the optimized analyzer"""
        return self.analyzer.answer_forensic_question(question, case_id, date_from, date_to, days_back)

class ModernReportGenerator:
    """Modern report generation with multiple formats"""
    
    def __init__(self, case_id: str, llm_folder: Optional[Path] = None):
        self.case_id = case_id
        self.analyzer = ForensicAnalyzer(llm_folder)
    
    @performance_monitor
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """Generate comprehensive forensic report"""
        report = {
            'case_id': self.case_id,
            'generated': datetime.now(timezone.utc).isoformat(),
            'computer_identity': self.analyzer.analyze_computer_identity(self.case_id),
            'user_accounts': self.analyzer.analyze_user_accounts(self.case_id),
            'usb_devices': self.analyzer.analyze_usb_devices(self.case_id),
            'forensic_answers': {}
        }
        
        # Answer all forensic questions
        for question in FORENSIC_QUESTIONS:
            answer = self.analyzer.answer_forensic_question(question, self.case_id)
            report['forensic_answers'][question] = answer
        
        return report
    
    def generate_autonomous_report(self, autonomous_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive report from autonomous analysis results"""
        report = {
            'case_id': self.case_id,
            'report_type': 'autonomous_forensic_analysis',
            'generated': datetime.now(timezone.utc).isoformat(),
            'analysis_summary': {
                'total_questions': autonomous_results.get('total_questions', 0),
                'questions_answered': autonomous_results.get('questions_answered', 0),
                'average_confidence': autonomous_results.get('average_confidence', 0.0),
                'processing_time': autonomous_results.get('processing_time', 0.0),
                'llm_provider': autonomous_results.get('llm_provider', 'none')
            },
            'evidence_overview': autonomous_results.get('evidence_summary', {}),
            'forensic_questions_and_answers': [],
            'confidence_analysis': {},
            'recommendations': []
        }
        
        # Process each question and answer
        answers = autonomous_results.get('answers', {})
        confidence_scores = autonomous_results.get('confidence_scores', {})
        
        high_confidence_count = 0
        medium_confidence_count = 0
        low_confidence_count = 0
        
        for question_id in sorted(answers.keys()):
            answer_data = answers[question_id]
            confidence = confidence_scores.get(question_id, 0.0)
            
            # Categorize confidence
            if confidence >= 0.8:
                confidence_category = "High"
                high_confidence_count += 1
            elif confidence >= 0.5:
                confidence_category = "Medium"
                medium_confidence_count += 1
            else:
                confidence_category = "Low"
                low_confidence_count += 1
            
            question_report = {
                'question_id': question_id,
                'question': answer_data.get('question', ''),
                'answer': answer_data.get('answer', ''),
                'method': answer_data.get('method', ''),
                'confidence_score': confidence,
                'confidence_category': confidence_category,
                'supporting_evidence_count': len(answer_data.get('supporting_evidence', [])),
                'supporting_evidence': answer_data.get('supporting_evidence', [])
            }
            
            # Add validation details if available
            if 'validation' in answer_data:
                question_report['validation'] = answer_data['validation']
            
            report['forensic_questions_and_answers'].append(question_report)
        
        # Confidence analysis
        report['confidence_analysis'] = {
            'high_confidence_answers': high_confidence_count,
            'medium_confidence_answers': medium_confidence_count,
            'low_confidence_answers': low_confidence_count,
            'confidence_distribution': {
                'high': f"{(high_confidence_count / len(answers) * 100):.1f}%" if answers else "0%",
                'medium': f"{(medium_confidence_count / len(answers) * 100):.1f}%" if answers else "0%",
                'low': f"{(low_confidence_count / len(answers) * 100):.1f}%" if answers else "0%"
            }
        }
        
        # Generate recommendations
        recommendations = []
        
        if low_confidence_count > 0:
            recommendations.append(f"Consider manual review of {low_confidence_count} low-confidence answers")
        
        if autonomous_results.get('llm_provider') == 'none':
            recommendations.append("Consider using LLM provider for enhanced analysis of complex questions")
        
        if report['evidence_overview'].get('total_evidence_items', 0) < 100:
            recommendations.append("Limited evidence available - consider additional artifact collection")
        
        if high_confidence_count == len(answers):
            recommendations.append("All questions answered with high confidence - analysis appears complete")
        
        report['recommendations'] = recommendations
        
        return report
    
    def save_report(self, report: Dict[str, Any], format: str = 'json') -> Path:
        """Save report in specified format"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        if format.lower() == 'json':
            report_path = CONFIG.base_dir / "reports" / f"forensic_report_{self.case_id}_{timestamp}.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
        
        elif format.lower() == 'pdf':
            report_path = CONFIG.base_dir / "reports" / f"forensic_report_{self.case_id}_{timestamp}.pdf"
            self._generate_pdf_report(report, report_path)
        
        else:
            raise ValueError(f"Unsupported format: {format}")
        
        LOGGER.info(f"Report saved: {report_path}")
        return report_path
    
    def _sanitize_text_for_pdf(self, text: str) -> str:
        """Sanitize text for PDF generation to handle Unicode issues"""
        if not text:
            return ""
        
        # Convert to string and handle None values
        text = str(text)
        
        # Truncate very long text to prevent rendering issues
        if len(text) > 500:
            text = text[:497] + "..."
        
        # Replace problematic Unicode characters with ASCII equivalents
        replacements = {
            '\u2013': '-',  # en dash
            '\u2014': '--', # em dash
            '\u2018': "'",  # left single quote
            '\u2019': "'",  # right single quote
            '\u201c': '"',  # left double quote
            '\u201d': '"',  # right double quote
            '\u2026': '...' # ellipsis
        }
        
        for unicode_char, ascii_char in replacements.items():
            text = text.replace(unicode_char, ascii_char)
        
        # Remove or replace other non-ASCII characters
        try:
            text.encode('latin1')
            return text
        except UnicodeEncodeError:
            # Replace non-encodable characters with '?'
            return text.encode('latin1', errors='replace').decode('latin1')

    def _generate_pdf_report(self, report: Dict[str, Any], output_path: Path):
        """Generate PDF report using modern FPDF with Unicode handling"""
        try:
            pdf = FPDF()
            pdf.add_page()
            pdf.set_font('Arial', 'B', 16)
            
            title = self._sanitize_text_for_pdf(f'Forensic Analysis Report - Case {self.case_id}')
            pdf.cell(0, 10, title, ln=1, align='C')
            
            pdf.set_font('Arial', '', 12)
            pdf.ln(10)
            
            # Computer Identity
            if report['computer_identity']:
                pdf.set_font('Arial', 'B', 14)
                pdf.cell(0, 10, 'Computer Identity', ln=1)
                pdf.set_font('Arial', '', 12)
                
                for key, value in report['computer_identity'].items():
                    if value:
                        clean_key = self._sanitize_text_for_pdf(key.replace("_", " ").title())
                        clean_value = self._sanitize_text_for_pdf(str(value))
                        pdf.cell(0, 8, f'{clean_key}: {clean_value}', ln=1)
                pdf.ln(5)
            
            # User Accounts
            if report['user_accounts']:
                pdf.set_font('Arial', 'B', 14)
                pdf.cell(0, 10, 'User Accounts', ln=1)
                pdf.set_font('Arial', '', 12)
                
                for account in report['user_accounts'][:5]:  # Top 5 accounts
                    username = self._sanitize_text_for_pdf(account['username'])
                    activity = self._sanitize_text_for_pdf(str(account['activity_count']))
                    pdf.cell(0, 8, f"User: {username} (Activity: {activity})", ln=1)
                pdf.ln(5)
            
            # Forensic Answers
            pdf.set_font('Arial', 'B', 14)
            pdf.cell(0, 10, 'Forensic Analysis', ln=1)
            pdf.set_font('Arial', '', 10)
            
            for question, answer in report['forensic_answers'].items():
                pdf.set_font('Arial', 'B', 11)
                clean_question = self._sanitize_text_for_pdf(question)
                pdf.multi_cell(0, 6, f'Q: {clean_question}')
                
                pdf.set_font('Arial', '', 10)
                clean_answer = self._sanitize_text_for_pdf(answer)
                pdf.multi_cell(0, 5, f'A: {clean_answer}')
                pdf.ln(3)
        
            pdf.output(str(output_path))
        except Exception as e:
            # If PDF generation fails, create a simple text report instead
            LOGGER.warning(f"PDF generation failed: {e}. Creating text report instead.")
            with open(output_path.with_suffix('.txt'), 'w', encoding='utf-8') as f:
                f.write(f"Forensic Analysis Report - Case {self.case_id}\n")
                f.write("=" * 50 + "\n\n")
                f.write("Computer Identity:\n")
                for key, value in report.get('computer_identity', {}).items():
                    if value:
                        f.write(f"  {key.replace('_', ' ').title()}: {value}\n")
                f.write("\nUser Accounts:\n")
                for account in report.get('user_accounts', [])[:5]:
                    f.write(f"  User: {account.get('username', 'Unknown')} (Activity: {account.get('activity_count', 0)})\n")
                f.write("\nForensic Analysis:\n")
                for question, answer in report.get('forensic_answers', {}).items():
                    f.write(f"Q: {question}\n")
                    f.write(f"A: {answer}\n\n")

class BHSMSQLiteOutputModule:
    """Custom Plaso output module for direct BHSM SQLite database integration."""
    
    def __init__(self):
        self.NAME = 'bhsm_sqlite'
        self.DESCRIPTION = 'Direct BHSM SQLite database output module'
        self._database_path = None
        self._connection = None
        self._case_id = None
        self._batch_size = 1000
        self._batch_events = []

    def set_database_path(self, database_path, case_id):
        """Set the database path and case ID."""
        self._database_path = database_path
        self._case_id = case_id

    def open_connection(self):
        """Open the SQLite database connection."""
        if not self._database_path:
            raise ValueError("Database path not set")
            
        import sqlite3
        self._connection = sqlite3.connect(self._database_path)
        self._connection.execute('PRAGMA journal_mode=WAL')
        self._connection.execute('PRAGMA synchronous=NORMAL')
        self._connection.execute('PRAGMA cache_size=10000')
        
        # Initialize BHSM schema if needed
        self._initialize_schema()

    def close_connection(self):
        """Close the database connection and flush remaining events."""
        if self._batch_events:
            self._flush_batch()
        if self._connection:
            self._connection.close()

    def _initialize_schema(self):
        """Initialize the BHSM database schema."""
        schema = """
        CREATE TABLE IF NOT EXISTS evidence (
            id          INTEGER PRIMARY KEY,
            case_id     TEXT NOT NULL,
            host        TEXT,
            user        TEXT,
            timestamp   REAL,  -- REAL for Windows Event Log microsecond precision
            artifact    TEXT NOT NULL,
            source_file TEXT NOT NULL,
            summary     TEXT,
            data_json   TEXT,
            file_hash   TEXT,
            created     INTEGER DEFAULT (unixepoch())
        ) STRICT;

        CREATE TABLE IF NOT EXISTS sources (
            file_path   TEXT PRIMARY KEY,
            file_hash   TEXT,
            file_size   INTEGER,
            processed   INTEGER DEFAULT (unixepoch()),
            status      TEXT DEFAULT 'complete'
        ) STRICT;

        CREATE INDEX IF NOT EXISTS idx_evidence_timestamp ON evidence(timestamp);
        CREATE INDEX IF NOT EXISTS idx_evidence_case_id ON evidence(case_id);
        CREATE INDEX IF NOT EXISTS idx_evidence_artifact ON evidence(artifact);
        """
        self._connection.executescript(schema)
        self._connection.commit()

    def process_event(self, event_data):
        """Process a single event and add to batch."""
        try:
            # Extract basic event information
            timestamp = getattr(event_data, 'timestamp', 0)
            if hasattr(timestamp, 'timestamp'):
                timestamp = int(timestamp.timestamp())
            elif isinstance(timestamp, (int, float)):
                timestamp = int(timestamp)
            else:
                timestamp = 0

            # Get event attributes
            host = getattr(event_data, 'hostname', None) or getattr(event_data, 'computer_name', None) or 'Unknown'
            user = getattr(event_data, 'username', None) or getattr(event_data, 'user_sid', None) or 'Unknown'
            
            # Determine artifact type from parser chain
            parser_chain = getattr(event_data, 'parser', '') or ''
            artifact = self._classify_artifact(parser_chain)
            
            # Get source file path
            source_file = getattr(event_data, 'filename', 'Unknown')
            
            # Create summary from available data
            summary = str(event_data)
            
            # Create JSON data with all event attributes
            data_dict = {}
            for attr_name in dir(event_data):
                if not attr_name.startswith('_'):
                    try:
                        attr_value = getattr(event_data, attr_name)
                        if not callable(attr_value):
                            data_dict[attr_name] = str(attr_value)
                    except:
                        continue

            data_json = json.dumps(data_dict, ensure_ascii=False)
            
            # Calculate hash of the data for integrity
            file_hash = hashlib.sha256(data_json.encode('utf-8')).hexdigest()[:16]

            # Add to batch
            self._batch_events.append((
                self._case_id,
                host,
                user,
                timestamp,
                artifact,
                source_file,
                summary,
                data_json,
                file_hash
            ))

            # Flush batch if full
            if len(self._batch_events) >= self._batch_size:
                self._flush_batch()

        except Exception as e:
            print(f"Error processing event: {e}")

    def _classify_artifact(self, parser_chain):
        """Classify artifact type from parser chain."""
        parser_chain = parser_chain.lower()
        if 'chrome' in parser_chain:
            return 'Chrome Browser'
        elif 'firefox' in parser_chain:
            return 'Firefox Browser'
        elif 'mft' in parser_chain:
            return 'NTFS MFT'
        elif 'prefetch' in parser_chain:
            return 'Windows Prefetch'
        elif 'registry' in parser_chain:
            return 'Windows Registry'
        elif 'evtx' in parser_chain:
            return 'Windows Event Log'
        else:
            return parser_chain or 'Unknown'

    def _flush_batch(self):
        """Flush the current batch of events to the database."""
        if not self._batch_events:
            return

        try:
            self._connection.executemany(
                """INSERT INTO evidence 
                   (case_id, host, user, timestamp, artifact, source_file, summary, data_json, file_hash)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                self._batch_events
            )
            self._connection.commit()
            self._batch_events.clear()
        except Exception as e:
            print(f"Error flushing batch: {e}")
            self._batch_events.clear()

# END-TO-END FORENSIC WORKFLOW SYSTEM

def check_external_dependencies(kape_path: Path = None, plaso_path: Path = None) -> Dict[str, bool]:
    """Check availability of external forensic tools"""
    
    dependencies = {
        'kape': False,
        'plaso': False,
        'python': True  # We're already running Python
    }
    
    # Check KAPE
    if kape_path:
        if kape_path.exists() and kape_path.is_file():
            dependencies['kape'] = True
        else:
            LOGGER.warning(f"KAPE not found at {kape_path}")
    else:
        # Try to find KAPE in common Windows locations
        common_kape_paths = [
            Path('D:/FORAI/tools/kape/kape.exe'),
            Path('C:/FORAI/tools/kape/kape.exe'),
            Path('kape.exe'),  # Current directory
            Path('C:/Program Files/KAPE/kape.exe'),
            Path('C:/Tools/KAPE/kape.exe')
        ]
        
        for path in common_kape_paths:
            if path.exists():
                dependencies['kape'] = True
                LOGGER.info(f"KAPE found at {path}")
                break
        
        if not dependencies['kape']:
            LOGGER.warning("KAPE not found in common locations")
    
    # Check Plaso
    plaso_found = False
    
    if plaso_path and plaso_path.exists() and plaso_path.is_dir():
        # Check for key Plaso tools in specified directory
        plaso_tools = ['log2timeline.py', 'psort.py', 'psteal.py']
        found_tools = []
        
        for tool in plaso_tools:
            tool_path = plaso_path / tool
            if tool_path.exists():
                found_tools.append(tool)
        
        if len(found_tools) >= 2:  # At least 2 tools found
            dependencies['plaso'] = True
            plaso_found = True
            LOGGER.info(f"Plaso tools found in directory: {found_tools}")
        else:
            LOGGER.warning(f"Plaso directory found but missing tools: {plaso_path}")
    elif plaso_path:
        LOGGER.warning(f"Plaso directory not found at {plaso_path}")
    
    # If not found in directory, try to find Plaso tools in PATH
    if not plaso_found:
        try:
            import shutil
            if shutil.which('log2timeline.py') or shutil.which('log2timeline'):
                dependencies['plaso'] = True
                LOGGER.info("Plaso tools found in PATH")
            else:
                LOGGER.warning("Plaso tools not found in PATH")
        except Exception as e:
            LOGGER.warning(f"Error checking for Plaso in PATH: {e}")
    
    return dependencies

def validate_workflow_requirements(kape_path: Path = None, plaso_path: Path = None, 
                                 require_all: bool = False) -> bool:
    """Validate that required tools are available for workflow execution"""
    
    deps = check_external_dependencies(kape_path, plaso_path)
    
    if require_all:
        missing = [tool for tool, available in deps.items() if not available]
        if missing:
            LOGGER.error(f"Missing required tools: {missing}")
            LOGGER.error("Please install missing tools or provide correct paths")
            return False
    else:
        # At least warn about missing tools
        missing = [tool for tool, available in deps.items() if not available and tool != 'python']
        if missing:
            LOGGER.warning(f"Optional tools not available: {missing}")
            LOGGER.warning("Some workflow features may be limited")
    
    return True

class ForensicWorkflowManager:
    """Complete end-to-end forensic analysis workflow manager"""
    
    def __init__(self, case_id: str, output_dir: Path, verbose: bool = False):
        self.case_id = case_id
        self.output_dir = Path(output_dir)
        self.verbose = verbose
        self.logger = LOGGER
        
        # Use existing FORAI directory structure
        self.artifacts_dir = self.output_dir / "artifacts"
        self.parsed_dir = self.output_dir / "extracts"  # Parsed extracts go in extracts folder
        self.reports_dir = self.output_dir / "reports"
        self.custody_dir = self.output_dir / "reports"  # Chain of custody goes in reports folder
        self.archives_dir = self.output_dir / "archives"
        self.llm_dir = self.output_dir / "LLM"
        
        # Ensure directories exist (but don't create output_dir itself)
        for dir_path in [self.artifacts_dir, self.parsed_dir, self.reports_dir, self.archives_dir]:
            dir_path.mkdir(exist_ok=True)
            
        self.chain_of_custody = []
        self.start_time = datetime.now(timezone.utc)
        self.artifacts_path = None  # Will be set by KAPE collection
        
    def log_custody_event(self, event_type: str, description: str, file_path: str = None):
        """Log chain of custody event"""
        event = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'case_id': self.case_id,
            'event_type': event_type,
            'description': description,
            'file_path': str(file_path) if file_path else None,
            'hash_md5': None,
            'hash_sha256': None
        }
        
        if file_path and Path(file_path).exists():
            event['hash_md5'] = self._calculate_hash(file_path, 'md5')
            event['hash_sha256'] = self._calculate_hash(file_path, 'sha256')
            
        self.chain_of_custody.append(event)
        self.logger.info(f"Chain of Custody: {event_type} - {description}")
        
    def _calculate_hash(self, file_path: str, algorithm: str) -> str:
        """Calculate file hash for chain of custody"""
        hash_func = hashlib.md5() if algorithm == 'md5' else hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_func.update(chunk)
            return hash_func.hexdigest()
        except Exception as e:
            self.logger.error(f"Hash calculation failed for {file_path}: {e}")
            return "ERROR"
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA256 hash for forensic integrity verification"""
        return self._calculate_hash(str(file_path), 'sha256')
    

    
    def _validate_database_integrity(self, db_path: Path) -> bool:
        """Validate BHSM database integrity and content"""
        try:
            if not db_path.exists():
                self.logger.error(f"Database does not exist: {db_path}")
                return False
                
            import sqlite3
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            # Check database schema
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = [row[0] for row in cursor.fetchall()]
            
            required_tables = ['evidence', 'sources']
            missing_tables = [table for table in required_tables if table not in tables]
            if missing_tables:
                self.logger.error(f"Missing required tables: {missing_tables}")
                conn.close()
                return False
                
            # Check evidence table structure
            cursor.execute("PRAGMA table_info(evidence)")
            columns = [row[1] for row in cursor.fetchall()]
            required_columns = ['id', 'case_id', 'host', 'user', 'timestamp', 'artifact', 'source_file', 'summary', 'data_json', 'file_hash']
            missing_columns = [col for col in required_columns if col not in columns]
            if missing_columns:
                self.logger.error(f"Missing required columns in evidence table: {missing_columns}")
                conn.close()
                return False
                
            # Check data integrity
            cursor.execute("SELECT COUNT(*) FROM evidence WHERE case_id = ?", (self.case_id,))
            evidence_count = cursor.fetchone()[0]
            
            if evidence_count == 0:
                self.logger.error("Database contains no evidence records for this case")
                conn.close()
                return False
                
            # Check for data corruption
            cursor.execute("SELECT COUNT(*) FROM evidence WHERE data_json IS NULL OR data_json = ''")
            null_data_count = cursor.fetchone()[0]
            
            if null_data_count > evidence_count * 0.1:  # More than 10% null data is suspicious
                self.logger.warning(f"High number of records with null data: {null_data_count}/{evidence_count}")
                
            # Calculate database hash for integrity
            db_size = db_path.stat().st_size
            db_hash = self._calculate_file_hash(db_path)
            
            self.logger.info(f"Database integrity validation:")
            self.logger.info(f"  - Path: {db_path}")
            self.logger.info(f"  - Size: {db_size:,} bytes")
            self.logger.info(f"  - Evidence records: {evidence_count:,}")
            self.logger.info(f"  - SHA256: {db_hash}")
            
            conn.close()
            
            self.log_custody_event("DATABASE_VALIDATION", 
                                 f"Database integrity validated - Records: {evidence_count:,}, Size: {db_size:,} bytes, Hash: {db_hash}",
                                 str(db_path))
            
            return True
            
        except Exception as e:
            self.logger.error(f"Database validation error: {e}")
            self.log_custody_event("DATABASE_VALIDATION_ERROR", f"Database validation failed: {str(e)}")
            return False
            
    def collect_artifacts_kape(self, target: str, kape_path: Path) -> bool:
        """Collect artifacts using KAPE"""
        try:
            self.log_custody_event("COLLECTION_START", f"Starting KAPE collection from {target}")
            
            if not kape_path.exists():
                raise FileNotFoundError(f"KAPE not found at {kape_path}")
                
            # KAPE command for native artifact collection (no VHDX)
            artifacts_dest = self.artifacts_dir / f"{self.case_id}_artifacts"
            artifacts_dest.mkdir(exist_ok=True)
            
            kape_cmd = [
                str(kape_path),
                "--tsource", target,
                "--tdest", str(artifacts_dest),
                "--target", "!SANS_Triage,Chrome,Firefox,Edge,InternetExplorer,BrowserArtifacts"
            ]
            
            self.logger.info(f"Executing KAPE: {' '.join(kape_cmd)}")
            result = subprocess.run(kape_cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode == 0:
                # Verify artifacts were collected
                if artifacts_dest.exists() and any(artifacts_dest.iterdir()):
                    # Count collected artifacts
                    artifact_count = sum(1 for _ in artifacts_dest.rglob("*") if _.is_file())
                    self.log_custody_event("COLLECTION_SUCCESS", 
                                         f"KAPE artifact collection completed successfully - {artifact_count} files collected", 
                                         str(artifacts_dest))
                    self.logger.info(f"Artifacts collected: {artifact_count} files in {artifacts_dest}")
                    
                    # Store artifacts path for Plaso processing
                    self.artifacts_path = artifacts_dest
                    return True
                else:
                    self.logger.error("KAPE completed but no artifacts were collected")
                    self.log_custody_event("COLLECTION_ERROR", "No artifacts were collected")
                    return False
            else:
                self.logger.error(f"KAPE failed: {result.stderr}")
                self.log_custody_event("COLLECTION_ERROR", f"KAPE collection failed: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"KAPE collection error: {e}")
            self.log_custody_event("COLLECTION_ERROR", f"KAPE collection error: {str(e)}")
            return False
    
    def create_final_archive(self) -> Path:
        """Create final archive containing artifacts, extracts, and reports"""
        try:
            import zipfile
            import shutil
            from datetime import datetime
            
            # Create archive directory if it doesn't exist
            archive_dir = self.output_dir / "archive"
            archive_dir.mkdir(exist_ok=True)
            
            # Create timestamped archive filename
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            archive_name = f"{self.case_id}_complete_analysis_{timestamp}.zip"
            archive_path = archive_dir / archive_name
            
            self.logger.info(f"Creating final archive: {archive_path}")
            self.log_custody_event("ARCHIVE_START", f"Creating final case archive: {archive_name}")
            
            with zipfile.ZipFile(archive_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                # Add artifacts directory (collected artifacts)
                if self.artifacts_dir.exists():
                    for file_path in self.artifacts_dir.rglob('*'):
                        if file_path.is_file():
                            arcname = f"artifacts/{file_path.relative_to(self.artifacts_dir)}"
                            zipf.write(file_path, arcname)
                            self.logger.debug(f"Added to archive: {arcname}")
                
                # Add parsed data directory (databases, extracts)
                if self.parsed_dir.exists():
                    for file_path in self.parsed_dir.rglob('*'):
                        if file_path.is_file():
                            arcname = f"parsed/{file_path.relative_to(self.parsed_dir)}"
                            zipf.write(file_path, arcname)
                            self.logger.debug(f"Added to archive: {arcname}")
                
                # Add reports directory
                if self.reports_dir.exists():
                    for file_path in self.reports_dir.rglob('*'):
                        if file_path.is_file():
                            arcname = f"reports/{file_path.relative_to(self.reports_dir)}"
                            zipf.write(file_path, arcname)
                            self.logger.debug(f"Added to archive: {arcname}")
                
                # Add chain of custody directory
                if self.custody_dir.exists():
                    for file_path in self.custody_dir.rglob('*'):
                        if file_path.is_file():
                            arcname = f"custody/{file_path.relative_to(self.custody_dir)}"
                            zipf.write(file_path, arcname)
                            self.logger.debug(f"Added to archive: {arcname}")
            
            # Calculate archive hash for integrity
            archive_hash = self._calculate_file_hash(archive_path)
            archive_size = archive_path.stat().st_size
            
            self.log_custody_event("ARCHIVE_SUCCESS", 
                                 f"Final archive created successfully - Size: {archive_size:,} bytes, Hash: {archive_hash}")
            self.logger.info(f"Final archive created: {archive_path}")
            self.logger.info(f"Archive size: {archive_size:,} bytes")
            self.logger.info(f"Archive hash: {archive_hash}")
            
            return archive_path
            
        except Exception as e:
            self.logger.error(f"Failed to create final archive: {e}")
            self.log_custody_event("ARCHIVE_ERROR", f"Failed to create final archive: {str(e)}")
            return None
            
    def create_custom_plaso_output_module(self) -> BHSMSQLiteOutputModule:
        """Create custom Plaso output module for direct BHSM SQLite integration"""
        module = BHSMSQLiteOutputModule()
        database_path = self.parsed_dir / f"{self.case_id}_bhsm.db"
        module.set_database_path(str(database_path), self.case_id)
        
        self.logger.info(f"Created custom BHSM SQLite output module for database: {database_path}")
        return module

    def parse_artifacts_plaso(self, plaso_path: Path, fast_mode: bool = False, date_from: str = None, date_to: str = None, artifacts_dir: Path = None, enable_winevtx: bool = False) -> bool:
        """Parse collected artifacts using proper Plaso two-step workflow: log2timeline -> psort -> SQLite"""
        try:
            self.log_custody_event("PARSING_START", "Starting Plaso two-step processing: log2timeline -> psort -> SQLite")
            
            # Set artifacts path if provided directly (for --parse-artifacts with --artifacts-dir)
            if artifacts_dir and artifacts_dir.exists():
                self.artifacts_path = artifacts_dir
                self.logger.info(f"Using provided artifacts directory: {artifacts_dir}")
                
                # Check for large Windows Event Log files that need special handling
                has_large_event_logs = self._check_large_event_logs(artifacts_dir)
            
            # Check if log2timeline is available in PATH or at specified location
            log2timeline_cmd_path = None
            
            # First try PATH
            if shutil.which("log2timeline"):
                log2timeline_cmd_path = "log2timeline"
                self.logger.info("Found log2timeline in system PATH")
            # Then try specified plaso directory
            elif plaso_path.exists():
                potential_path = plaso_path / "log2timeline.exe"
                if potential_path.exists():
                    log2timeline_cmd_path = str(potential_path)
                    self.logger.info(f"Found log2timeline at: {potential_path}")
                else:
                    potential_path = plaso_path / "log2timeline"
                    if potential_path.exists():
                        log2timeline_cmd_path = str(potential_path)
                        self.logger.info(f"Found log2timeline at: {potential_path}")
            
            if not log2timeline_cmd_path:
                raise FileNotFoundError(f"log2timeline not found in PATH or at {plaso_path}")
            
            # Check for psort as well
            psort_cmd_path = None
            
            # First try PATH
            if shutil.which("psort"):
                psort_cmd_path = "psort"
                self.logger.info("Found psort in system PATH")
            # Then try specified plaso directory
            elif plaso_path.exists():
                potential_path = plaso_path / "psort.exe"
                if potential_path.exists():
                    psort_cmd_path = str(potential_path)
                    self.logger.info(f"Found psort at: {potential_path}")
                else:
                    potential_path = plaso_path / "psort"
                    if potential_path.exists():
                        psort_cmd_path = str(potential_path)
                        self.logger.info(f"Found psort at: {potential_path}")
            
            if not psort_cmd_path:
                raise FileNotFoundError(f"psort not found in PATH or at {plaso_path}")
                
            # Use the artifacts directory created by KAPE
            if not hasattr(self, 'artifacts_path') or not self.artifacts_path.exists():
                raise FileNotFoundError(f"Artifacts directory not found. KAPE collection may have failed.")
                
            # Validate artifacts directory has content
            artifact_files = list(self.artifacts_path.rglob("*"))
            if not artifact_files:
                raise ValueError("Artifacts directory is empty")
                
            # Create custom BHSM SQLite output module
            custom_module = self.create_custom_plaso_output_module()
            
            # File paths for two-step process
            plaso_storage_path = self.parsed_dir / f"{self.case_id}_timeline.plaso"
            database_path = self.parsed_dir / f"{self.case_id}_bhsm.db"
            
            # Pre-optimize database for bulk operations
            self._pre_optimize_database(database_path)
            
            # Track processing metrics
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            artifacts_size = sum(f.stat().st_size for f in self.artifacts_path.rglob("*") if f.is_file()) / 1024 / 1024  # MB
            
            self.logger.info(f"Step 1: Creating timeline from artifacts (Size: {artifacts_size:.1f}MB): {self.artifacts_path} -> {plaso_storage_path}")
            
            # Step 1: Create timeline from collected artifacts with performance optimizations
            
            # Determine optimal worker count based on system resources
            cpu_count = psutil.cpu_count(logical=False) or 4  # Physical cores
            available_memory = psutil.virtual_memory().available // (1024 * 1024)  # MB
            
            optimal_workers = min(max(cpu_count - 1, 2), 12)  # Leave 1 core free, max 12 workers
            worker_memory_limit = min(max(available_memory // optimal_workers // 2, 1024), 8192)  # 1-8GB per worker
            
            # Selective parser configuration for faster processing
            # Focus on parsers most relevant to the 12 standard forensic questions
            essential_parsers = [
                "mft",           # File system activity
                "prefetch",      # Program execution
                "winreg",        # Registry (USB, software, user accounts)
                "lnk",           # Recent file access
                "winevtx",       # Windows event logs
                "usnjrnl",       # File system journal
                "filestat",      # File metadata
                "recycle_bin",   # Deleted files
                # Browser artifacts (selective)
                "sqlite/chrome_27_history",
                "sqlite/firefox_history",
                "msiecf",        # Internet Explorer
                # USB and device artifacts
                "custom_destinations",  # Jump lists
                "olecf/olecf_automatic_destinations"
            ]
            
            # Optional parsers for comprehensive analysis (can be disabled for speed)
            optional_parsers = [
                "sqlite/chrome_66_cookies",
                "sqlite/chrome_autofill", 
                "sqlite/firefox_downloads",
                "sqlite/safari_historydb",
                "sqlite/edge_load_statistics",
                "binary_cookies",
                "chrome_cache",
                "firefox_cache",
                "winjob"  # Scheduled tasks
            ]
            
            # Use essential parsers by default, add optional if processing time allows
            parsers_list = essential_parsers
            if not fast_mode and artifacts_size < 5000:  # Less than 5GB - include optional parsers
                parsers_list.extend(optional_parsers)
            elif fast_mode:
                # In fast mode, use only the most critical parsers for the 12 questions
                parsers_list = [
                    "mft",           # File system activity
                    "prefetch",      # Program execution  
                    "winreg",        # Registry (USB, software)
                    "winevtx",       # Windows event logs
                    "usnjrnl",       # File system journal
                    "recycle_bin"    # Deleted files
                ]
            
            log2timeline_cmd = [
                log2timeline_cmd_path,
                "--storage-file", str(plaso_storage_path),
                "--parsers", ",".join(parsers_list),
                "--hashers", "md5",  # Only MD5 for speed (SHA256 can be added later if needed)
                "--workers", str(optimal_workers),
                "--worker_memory_limit", str(worker_memory_limit),
                "--temporary_directory", str(self.parsed_dir / "temp"),  # Use SSD temp space
            ]
            
            # Add date filtering if specified (major performance boost for targeted analysis)
            if date_from or date_to:
                if date_from:
                    log2timeline_cmd.extend(["--date_filter", f"start:{date_from}"])
                if date_to:
                    log2timeline_cmd.extend(["--date_filter", f"end:{date_to}"])
                self.logger.info(f"Date filtering enabled: {date_from} to {date_to}")
            
            # Add the artifacts path as the final argument
            log2timeline_cmd.append(str(self.artifacts_path))
            
            # Create temp directory for better performance
            temp_dir = self.parsed_dir / "temp"
            temp_dir.mkdir(exist_ok=True)
            
            self.logger.info(f"Performance optimizations: {optimal_workers} workers, {worker_memory_limit}MB per worker")
            self.logger.info(f"Parser selection: {'Essential + Optional' if len(parsers_list) > len(essential_parsers) else 'Essential only'} ({len(parsers_list)} parsers)")
            
            self.logger.info(f"Executing log2timeline: {' '.join(log2timeline_cmd)}")
            
            # Add debug info about the command and environment
            self.logger.debug(f"Working directory: {os.getcwd()}")
            self.logger.debug(f"Artifacts path exists: {self.artifacts_path.exists()}")
            self.logger.debug(f"Output directory exists: {plaso_storage_path.parent.exists()}")
            
            try:
                log2timeline_result = subprocess.run(log2timeline_cmd, capture_output=True, text=True, timeout=7200)
            except subprocess.TimeoutExpired:
                self.logger.error("log2timeline timed out after 2 hours")
                self.log_custody_event("PARSING_ERROR", "log2timeline timed out after 2 hours")
                return False
            except Exception as e:
                self.logger.error(f"Exception running log2timeline: {e}")
                self.log_custody_event("PARSING_ERROR", f"Exception running log2timeline: {e}")
                return False
            
            # Always log the result details for debugging
            self.logger.debug(f"log2timeline return code: {log2timeline_result.returncode}")
            if log2timeline_result.stdout:
                self.logger.debug(f"log2timeline stdout: {log2timeline_result.stdout}")
            if log2timeline_result.stderr:
                self.logger.debug(f"log2timeline stderr: {log2timeline_result.stderr}")
            
            if log2timeline_result.returncode != 0:
                error_msg = f"Return code: {log2timeline_result.returncode}"
                if log2timeline_result.stderr:
                    error_msg += f"\nSTDERR: {log2timeline_result.stderr}"
                if log2timeline_result.stdout:
                    error_msg += f"\nSTDOUT: {log2timeline_result.stdout}"
                self.logger.error(f"log2timeline failed: {error_msg}")
                self.log_custody_event("PARSING_ERROR", f"log2timeline failed: {error_msg}")
                return False
                
            if not plaso_storage_path.exists():
                self.logger.error("Plaso storage file was not created by log2timeline")
                self.log_custody_event("PARSING_ERROR", "Plaso storage file was not created")
                return False
                
            plaso_size = plaso_storage_path.stat().st_size / 1024 / 1024  # MB
            self.logger.info(f"Step 1 completed: Timeline created (Size: {plaso_size:.1f}MB)")
            
            # Step 2: Process timeline to SQLite
            self.logger.info(f"Step 2: Processing timeline to SQLite: {plaso_storage_path} -> {database_path}")
            
            # Use CSV output for database import (more efficient than JSON)
            csv_output_path = self.parsed_dir / f"{self.case_id}_timeline.csv"
            
            # Optimize psort for CSV output
            psort_cmd = [
                psort_cmd_path,
                "-o", "l2tcsv",
                "-w", str(csv_output_path),
                "--temporary_directory", str(temp_dir),  # Use same temp directory
                str(plaso_storage_path)
            ]
            
            self.logger.info(f"Executing psort: {' '.join(psort_cmd)}")
            result = subprocess.run(psort_cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode != 0:
                self.logger.error(f"psort processing failed: {result.stderr}")
                
                # Check if this is the winevtx AttributeError issue
                if "AttributeError: 'NoneType' object has no attribute 'GetAttributeContainers'" in result.stderr:
                    self.logger.warning("Detected Plaso winevtx compatibility issue. Trying multiple fallback approaches...")
                    
                    # Try approach 1: Use dynamic output format (avoids JSON formatting issues)
                    self.logger.info("Fallback 1: Trying dynamic output format...")
                    dynamic_output_path = self.parsed_dir / f"{self.case_id}_timeline_dynamic.txt"
                    dynamic_cmd = [
                        psort_cmd_path,
                        "-o", "dynamic",
                        "-w", str(dynamic_output_path),
                        "--temporary_directory", str(temp_dir),
                        str(plaso_storage_path)
                    ]
                    
                    dynamic_result = subprocess.run(dynamic_cmd, capture_output=True, text=True, timeout=3600)
                    
                    if dynamic_result.returncode == 0 and dynamic_output_path.exists():
                        self.logger.info("Dynamic format export successful. Converting to database format...")
                        return self._process_dynamic_timeline(dynamic_output_path, custom_module)
                    
                    # Try approach 2: Use L2T CSV format
                    self.logger.info("Fallback 2: Trying L2T CSV format...")
                    csv_output_path = self.parsed_dir / f"{self.case_id}_timeline.csv"
                    csv_cmd = [
                        psort_cmd_path,
                        "-o", "l2tcsv",
                        "-w", str(csv_output_path),
                        "--temporary_directory", str(temp_dir),
                        str(plaso_storage_path)
                    ]
                    
                    csv_result = subprocess.run(csv_cmd, capture_output=True, text=True, timeout=3600)
                    
                    if csv_result.returncode == 0 and csv_output_path.exists():
                        self.logger.info("L2T CSV export successful. Converting to database format...")
                        return self._process_csv_timeline(csv_output_path, custom_module)
                    
                    # Try approach 3: Use JSON with filtered parsers (exclude problematic ones temporarily)
                    self.logger.info("Fallback 3: Trying JSON with parser filtering...")
                    filtered_json_path = self.parsed_dir / f"{self.case_id}_timeline_filtered.json"
                    filtered_cmd = [
                        psort_cmd_path,
                        "-o", "json",
                        "-w", str(filtered_json_path),
                        "--temporary_directory", str(temp_dir),
                        str(plaso_storage_path),
                        "parser != 'winevtx'"  # Filter to exclude problematic events
                    ]
                    
                    filtered_result = subprocess.run(filtered_cmd, capture_output=True, text=True, timeout=3600)
                    
                    if filtered_result.returncode == 0 and filtered_json_path.exists():
                        self.logger.warning("Filtered JSON export successful, but Windows Event Logs excluded!")
                        # Process filtered JSON and then try to get winevtx separately
                        success = self._process_json_timeline(filtered_json_path, custom_module)
                        if success:
                            # Try to get winevtx events separately with dynamic format
                            self._attempt_winevtx_recovery(psort_cmd_path, plaso_storage_path, temp_dir, custom_module)
                        return success
                
                self.log_custody_event("PARSING_ERROR", f"psort processing failed: {result.stderr}")
                return False
                
            # Process CSV output by calling CSV import script
            if not csv_output_path.exists():
                self.logger.error("CSV timeline was not created by psort")
                self.log_custody_event("PARSING_ERROR", "CSV timeline was not created")
                return False
                
            return self._process_csv_timeline(csv_output_path)
                
            if not database_path.exists():
                self.logger.error("BHSM SQLite database was not created")
                self.log_custody_event("PARSING_ERROR", "BHSM SQLite database was not created")
                return False
                
            # Calculate performance metrics
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / 1024 / 1024
            processing_time = end_time - start_time
            memory_delta = end_memory - start_memory
            throughput = artifacts_size / processing_time if processing_time > 0 else 0
            
            # Validate database integrity and content
            if not self._validate_database_integrity(database_path):
                self.logger.error("Database integrity validation failed")
                self.log_custody_event("PARSING_ERROR", "Database integrity validation failed")
                return False
                
            # Post-optimize database for queries
            self._post_optimize_database(database_path)
            
            # Clean up temporary files
            try:
                json_output_path.unlink()
                # Clean up temp directory
                if temp_dir.exists():
                    shutil.rmtree(temp_dir, ignore_errors=True)
                # Optionally clean up intermediate .plaso file to save space
                # plaso_storage_path.unlink()  # Uncomment if storage space is critical
            except Exception as e:
                self.logger.debug(f"Cleanup warning: {e}")
                
            # Log performance metrics
            db_size = database_path.stat().st_size / 1024 / 1024  # MB
            self.log_custody_event("PARSING_SUCCESS", 
                                 f"Plaso processing completed. "
                                 f"Time: {processing_time:.2f}s, Memory: {memory_delta:+.1f}MB, "
                                 f"Throughput: {throughput:.1f}MB/s, Database: {db_size:.1f}MB")
            
            self.logger.info(f"Performance metrics - Total processing: {processing_time:.2f}s, "
                           f"Memory delta: {memory_delta:+.1f}MB, Throughput: {throughput:.1f}MB/s")
            
            return True
                
        except Exception as e:
            self.logger.error(f"Plaso two-step processing error: {e}")
            self.log_custody_event("PARSING_ERROR", f"Plaso two-step processing error: {str(e)}")
            return False
    
    def _process_csv_timeline(self, csv_path, custom_module):
        """Process CSV timeline output as fallback when JSON fails"""
        try:
            import csv
            
            self.logger.info(f"Processing CSV timeline: {csv_path}")
            
            # Initialize custom module
            custom_module.open_connection()
            
            processed_count = 0
            with open(csv_path, 'r', encoding='utf-8', newline='') as csvfile:
                # Try to detect the CSV format
                sample = csvfile.read(1024)
                csvfile.seek(0)
                sniffer = csv.Sniffer()
                delimiter = sniffer.sniff(sample).delimiter
                
                reader = csv.DictReader(csvfile, delimiter=delimiter)
                
                for row in reader:
                    try:
                        # Convert CSV row to event format similar to JSON
                        event_data = {
                            'timestamp': row.get('datetime', ''),
                            'timestamp_desc': row.get('timestamp_desc', ''),
                            'source': row.get('source', ''),
                            'source_long': row.get('source_long', ''),
                            'message': row.get('message', ''),
                            'parser': row.get('parser', ''),
                            'display_name': row.get('display_name', ''),
                            'tag': row.get('tag', ''),
                        }
                        
                        # Process the event through custom module
                        custom_module.process_event(event_data)
                        processed_count += 1
                        
                        if processed_count % 10000 == 0:
                            self.logger.info(f"Processed {processed_count} CSV events...")
                            
                    except Exception as e:
                        self.logger.debug(f"Error processing CSV row: {e}")
                        continue
            
            custom_module.close_connection()
            self.logger.info(f"CSV processing completed: {processed_count} events processed")
            
            # Clean up CSV file
            try:
                csv_path.unlink()
            except Exception as e:
                self.logger.debug(f"CSV cleanup warning: {e}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"CSV processing error: {e}")
            return False
    
    def _process_json_timeline(self, json_path, custom_module):
        """Process JSON timeline output"""
        try:
            self.logger.info(f"Processing JSON timeline: {json_path}")
            
            # Initialize custom module and process JSON data
            custom_module.open_connection()
            try:
                processed_count = 0
                with open(json_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if line.strip():
                            try:
                                event_data = json.loads(line)
                                custom_module.process_event(event_data)
                                processed_count += 1
                                
                                if processed_count % 10000 == 0:
                                    self.logger.info(f"Processed {processed_count} JSON events...")
                                    
                            except json.JSONDecodeError:
                                continue
                            except Exception as e:
                                self.logger.debug(f"Error processing JSON event: {e}")
                                continue
            finally:
                custom_module.close_connection()
            
            self.logger.info(f"JSON processing completed: {processed_count} events processed")
            
            # Clean up JSON file
            try:
                json_path.unlink()
            except Exception as e:
                self.logger.debug(f"JSON cleanup warning: {e}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"JSON processing error: {e}")
            return False
    
    def _process_csv_timeline(self, csv_path):
        """Process CSV timeline output directly into BHSM database"""
        try:
            self.logger.info(f"Processing CSV timeline: {csv_path}")
            
            # Get database path using CONFIG (single database architecture)
            database_path = Path(CONFIG.db_path)
            
            # Create database schema
            self._create_bhsm_database_schema(database_path)
            
            # Import CSV data
            count = self._import_csv_to_database(csv_path, database_path)
            
            # Load and inject keywords
            self._inject_keywords_to_database(database_path)
            
            self.logger.info(f"CSV processing completed: {count:,} records imported")
            
            # Clean up CSV file to save space
            try:
                csv_path.unlink()
                self.logger.info("CSV file cleaned up to save disk space")
            except Exception as e:
                self.logger.debug(f"CSV cleanup warning: {e}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"CSV processing error: {e}")
            return False
    
    def _create_bhsm_database_schema(self, database_path):
        """Create BHSM database schema with all required tables"""
        try:
            with sqlite3.connect(str(database_path)) as conn:
                # Enable optimizations for bulk insert
                conn.execute("PRAGMA synchronous=OFF")
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA cache_size=10000")
                
                # Create evidence table (primary table for autonomous analysis)
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS evidence (
                        id INTEGER PRIMARY KEY,
                        case_id TEXT NOT NULL,
                        host TEXT,
                        user TEXT,
                        timestamp REAL NOT NULL,
                        artifact TEXT NOT NULL,
                        source_file TEXT NOT NULL,
                        summary TEXT,
                        data_json TEXT,
                        file_hash TEXT,
                        created INTEGER DEFAULT (unixepoch())
                    )
                ''')
                
                # Create indexes for evidence table
                conn.execute('CREATE INDEX IF NOT EXISTS idx_evidence_case_id ON evidence(case_id)')
                conn.execute('CREATE INDEX IF NOT EXISTS idx_evidence_timestamp ON evidence(timestamp)')
                conn.execute('CREATE INDEX IF NOT EXISTS idx_evidence_artifact ON evidence(artifact)')
                conn.execute('CREATE INDEX IF NOT EXISTS idx_evidence_host ON evidence(host)')
                conn.execute('CREATE INDEX IF NOT EXISTS idx_evidence_user ON evidence(user)')
                
                # Create keywords table
                conn.execute('''
                    CREATE TABLE IF NOT EXISTS keywords (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        case_id TEXT NOT NULL,
                        keyword TEXT NOT NULL,
                        category TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                conn.execute('CREATE INDEX IF NOT EXISTS idx_keywords_case_id ON keywords(case_id)')
                conn.execute('CREATE INDEX IF NOT EXISTS idx_keywords_keyword ON keywords(keyword)')
                
                conn.commit()
                self.logger.info(f"BHSM database schema created: {database_path}")
                
        except Exception as e:
            self.logger.error(f"Database schema creation error: {e}")
            raise
    
    def _import_csv_to_database(self, csv_path, database_path):
        """Import CSV timeline data into BHSM database"""
        try:
            import csv
            import json
            import hashlib
            
            count = 0
            batch_size = 10000
            batch_data = []
            
            with sqlite3.connect(str(database_path)) as conn:
                with open(csv_path, 'r', encoding='utf-8', errors='ignore') as csvfile:
                    reader = csv.DictReader(csvfile)
                    
                    for row in reader:
                        try:
                            # Extract data from CSV row
                            timestamp_str = row.get('datetime', row.get('timestamp', ''))
                            artifact = row.get('source', row.get('artifact', ''))
                            host = row.get('hostname', row.get('host', ''))
                            user = row.get('username', row.get('user', ''))
                            source_file = row.get('filename', row.get('source_file', ''))
                            summary = row.get('message', row.get('summary', ''))
                            
                            # Convert timestamp to REAL for evidence table
                            try:
                                from datetime import datetime
                                if timestamp_str:
                                    dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                                    timestamp_real = dt.timestamp()
                                else:
                                    timestamp_real = 0.0
                            except:
                                timestamp_real = 0.0
                            
                            # Create JSON data from all fields
                            data_json = json.dumps(dict(row))
                            
                            # Create hash for deduplication
                            hash_input = f"{timestamp_str}{artifact}{summary}".encode('utf-8')
                            hash_value = hashlib.sha256(hash_input).hexdigest()[:16]
                            
                            # Data for both tables
                            batch_data.append({
                                'case_id': self.case_id,
                                'timestamp_str': timestamp_str,
                                'timestamp_real': timestamp_real,
                                'artifact': artifact,
                                'host': host,
                                'user': user,
                                'source_file': source_file,
                                'summary': summary,
                                'data_json': data_json,
                                'hash_value': hash_value
                            })
                            
                            count += 1
                            
                            # Insert in batches for performance
                            if len(batch_data) >= batch_size:
                                # Insert into evidence table (primary)
                                evidence_data = [(
                                    row['case_id'], row['host'], row['user'], row['timestamp_real'],
                                    row['artifact'], row['source_file'], row['summary'], 
                                    row['data_json'], row['hash_value']
                                ) for row in batch_data]
                                
                                conn.executemany('''
                                    INSERT OR IGNORE INTO evidence 
                                    (case_id, host, user, timestamp, artifact, source_file, summary, data_json, file_hash)
                                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                                ''', evidence_data)
                                
                                batch_data = []
                                
                                if count % 50000 == 0:
                                    self.logger.info(f"Imported {count:,} records...")
                                    
                        except Exception as e:
                            self.logger.debug(f"Error processing CSV row: {e}")
                            continue
                    
                    # Insert remaining batch
                    if batch_data:
                        # Insert into evidence table (primary)
                        evidence_data = [(
                            row['case_id'], row['host'], row['user'], row['timestamp_real'],
                            row['artifact'], row['source_file'], row['summary'], 
                            row['data_json'], row['hash_value']
                        ) for row in batch_data]
                        
                        conn.executemany('''
                            INSERT OR IGNORE INTO evidence 
                            (case_id, host, user, timestamp, artifact, source_file, summary, data_json, file_hash)
                            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', evidence_data)
                    
                    conn.commit()
            
            self.logger.info(f"CSV import completed: {count:,} records processed")
            return count
            
        except Exception as e:
            self.logger.error(f"CSV import error: {e}")
            raise
    
    def _inject_keywords_to_database(self, database_path):
        """Load and inject keywords into BHSM database"""
        try:
            keywords_file = Path(__file__).parent / "keywords.txt"
            
            if not keywords_file.exists():
                self.logger.warning(f"Keywords file not found: {keywords_file}")
                return
            
            with sqlite3.connect(str(database_path)) as conn:
                # Clear existing keywords for this case
                conn.execute("DELETE FROM keywords WHERE case_id = ?", (self.case_id,))
                
                # Load keywords from file
                keywords_text = keywords_file.read_text(encoding='utf-8')
                keywords = [kw.strip() for kw in keywords_text.split('\n') if kw.strip()]
                
                # Insert keywords
                for keyword in keywords:
                    conn.execute('''
                        INSERT INTO keywords (case_id, keyword, category)
                        VALUES (?, ?, ?)
                    ''', (self.case_id, keyword, 'forensic'))
                
                conn.commit()
                self.logger.info(f"Injected {len(keywords)} keywords into database")
                
        except Exception as e:
            self.logger.warning(f"Keywords injection error: {e}")
    
    def _process_dynamic_timeline(self, dynamic_path, custom_module):
        """Process dynamic format timeline output"""
        try:
            import re
            
            self.logger.info(f"Processing dynamic timeline: {dynamic_path}")
            
            # Initialize custom module
            custom_module.open_connection()
            
            processed_count = 0
            with open(dynamic_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    
                    try:
                        # Parse dynamic format line (simplified parsing)
                        # Dynamic format: datetime,timestamp_desc,source,source_long,message,parser,display_name,tag
                        parts = line.split(',', 7)  # Split into max 8 parts
                        if len(parts) >= 5:
                            event_data = {
                                'timestamp': parts[0] if len(parts) > 0 else '',
                                'timestamp_desc': parts[1] if len(parts) > 1 else '',
                                'source': parts[2] if len(parts) > 2 else '',
                                'source_long': parts[3] if len(parts) > 3 else '',
                                'message': parts[4] if len(parts) > 4 else '',
                                'parser': parts[5] if len(parts) > 5 else '',
                                'display_name': parts[6] if len(parts) > 6 else '',
                                'tag': parts[7] if len(parts) > 7 else '',
                            }
                            
                            custom_module.process_event(event_data)
                            processed_count += 1
                            
                            if processed_count % 10000 == 0:
                                self.logger.info(f"Processed {processed_count} dynamic events...")
                                
                    except Exception as e:
                        self.logger.debug(f"Error processing dynamic line: {e}")
                        continue
            
            custom_module.close_connection()
            self.logger.info(f"Dynamic processing completed: {processed_count} events processed")
            
            # Clean up dynamic file
            try:
                dynamic_path.unlink()
            except Exception as e:
                self.logger.debug(f"Dynamic cleanup warning: {e}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Dynamic processing error: {e}")
            return False
    
    def _attempt_winevtx_recovery(self, psort_cmd_path, plaso_storage_path, temp_dir, custom_module):
        """Attempt to recover Windows Event Log data using alternative methods"""
        try:
            self.logger.info("Attempting to recover Windows Event Log data...")
            
            # Try to extract just winevtx events with dynamic format
            winevtx_output_path = self.parsed_dir / f"{self.case_id}_winevtx_recovery.txt"
            winevtx_cmd = [
                psort_cmd_path,
                "-o", "dynamic",
                "-w", str(winevtx_output_path),
                "--temporary_directory", str(temp_dir),
                str(plaso_storage_path),
                "parser == 'winevtx'"  # Only winevtx events
            ]
            
            winevtx_result = subprocess.run(winevtx_cmd, capture_output=True, text=True, timeout=1800)
            
            if winevtx_result.returncode == 0 and winevtx_output_path.exists():
                self.logger.info("Windows Event Log recovery successful!")
                # Process the recovered events
                success = self._process_dynamic_timeline(winevtx_output_path, custom_module)
                if success:
                    self.logger.info("Windows Event Log data successfully integrated")
                return success
            else:
                self.logger.warning("Windows Event Log recovery failed")
                return False
                
        except Exception as e:
            self.logger.warning(f"Windows Event Log recovery error: {e}")
            return False
    
    def _check_large_event_logs(self, artifacts_dir: Path) -> bool:
        """Check for large Windows Event Log files that need special handling"""
        try:
            evt_files = list(artifacts_dir.glob("**/*.evtx")) + list(artifacts_dir.glob("**/*.evt"))
            large_files = []
            
            for evt_file in evt_files:
                try:
                    file_size_mb = evt_file.stat().st_size / (1024 * 1024)
                    if file_size_mb > 100:  # Over 100MB
                        large_files.append((evt_file, file_size_mb))
                        self.logger.warning(f"Large Windows Event Log detected: {evt_file.name} ({file_size_mb:.1f}MB)")
                except Exception as e:
                    self.logger.debug(f"Error checking {evt_file}: {e}")
                    continue
            
            if large_files:
                self.logger.info(f"Found {len(large_files)} large Windows Event Log files")
                self.logger.info("Enabling enhanced processing settings for large winevt data")
                return True
            
            return False
            
        except Exception as e:
            self.logger.debug(f"Error checking for large event logs: {e}")
            return False
    
    def _pre_optimize_database(self, db_path: Path) -> None:
        """Pre-optimize database for bulk operations with Windows Event Log support"""
        try:
            with sqlite3.connect(str(db_path)) as conn:
                # Settings for bulk inserts and large Windows Event Log processing
                conn.execute("PRAGMA journal_mode=WAL")
                conn.execute("PRAGMA synchronous=OFF")  # Speed during processing
                conn.execute("PRAGMA cache_size=200000")  # Larger cache for winevt data
                conn.execute("PRAGMA temp_store=MEMORY")
                conn.execute("PRAGMA page_size=65536")  # Large pages for bulk data
                conn.execute("PRAGMA wal_autocheckpoint=0")  # Disable auto-checkpoint during processing
                conn.execute("PRAGMA busy_timeout=600000")  # 10 minute timeout for large files
                
                # Additional settings for very large Windows Event Log files
                conn.execute("PRAGMA max_page_count=4294967292")  # Allow DB to grow very large
                conn.execute("PRAGMA cache_spill=100000")  # Spill cache to disk when needed
                conn.execute("PRAGMA mmap_size=268435456")  # 256MB memory mapping
                conn.execute("PRAGMA threads=4")  # Multi-threaded operations
                
                conn.commit()
                
            self.logger.debug("Database pre-optimized for maximum performance with Windows Event Log support")
        except Exception as e:
            self.logger.warning(f"Database pre-optimization failed: {e}")
    
    def _post_optimize_database(self, db_path: Path) -> None:
        """Post-optimize database for query performance"""
        try:
            with sqlite3.connect(str(db_path)) as conn:
                # Restore safe settings and optimize for queries
                conn.execute("PRAGMA synchronous=NORMAL")
                conn.execute("PRAGMA wal_autocheckpoint=1000")
                conn.execute("PRAGMA optimize")
                
                # Analyze tables for queries
                conn.execute("ANALYZE")
                
                # Vacuum to reclaim space
                conn.execute("VACUUM")
                
                conn.commit()
                
            self.logger.debug("Database post-optimized")
        except Exception as e:
            self.logger.warning(f"Database post-optimization failed: {e}")

    def import_plaso_file(self, plaso_file_path: Path, plaso_path: Path) -> bool:
        """Import existing .plaso file and create BHSM database (skips log2timeline step)"""
        try:
            self.log_custody_event("PLASO_IMPORT_START", f"Starting import of existing plaso file: {plaso_file_path}")
            
            # Validate plaso file exists
            if not plaso_file_path.exists():
                raise FileNotFoundError(f"Plaso file not found: {plaso_file_path}")
            
            # Check file size and log
            plaso_size = plaso_file_path.stat().st_size / 1024 / 1024  # MB
            self.logger.info(f"Importing plaso file (Size: {plaso_size:.1f}MB): {plaso_file_path}")
            
            # Find psort command
            psort_cmd_path = None
            if shutil.which("psort"):
                psort_cmd_path = "psort"
                self.logger.info("Found psort in system PATH")
            elif plaso_path.exists():
                potential_path = plaso_path / "psort.exe"
                if potential_path.exists():
                    psort_cmd_path = str(potential_path)
                    self.logger.info(f"Found psort at: {potential_path}")
                else:
                    potential_path = plaso_path / "psort"
                    if potential_path.exists():
                        psort_cmd_path = str(potential_path)
                        self.logger.info(f"Found psort at: {potential_path}")
            
            if not psort_cmd_path:
                raise FileNotFoundError(f"psort not found in PATH or at {plaso_path}")
            
            # Create custom BHSM SQLite output module
            custom_module = self.create_custom_plaso_output_module()
            database_path = self.parsed_dir / f"{self.case_id}_bhsm.db"
            
            # Pre-optimize database for bulk operations
            self._pre_optimize_database(database_path)
            
            # Create temp directory
            temp_dir = self.parsed_dir / "temp"
            temp_dir.mkdir(exist_ok=True)
            
            # Track processing metrics
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024
            
            self.logger.info(f"Processing plaso file to SQLite: {plaso_file_path} -> {database_path}")
            
            # Use CSV output for database import (more efficient than JSON)
            csv_output_path = self.parsed_dir / f"{self.case_id}_timeline.csv"
            
            # Process plaso file to CSV
            psort_cmd = [
                psort_cmd_path,
                "-o", "l2tcsv",
                "-w", str(csv_output_path),
                "--temporary_directory", str(temp_dir),
                str(plaso_file_path)
            ]
            
            self.logger.info(f"Executing psort: {' '.join(psort_cmd)}")
            result = subprocess.run(psort_cmd, capture_output=True, text=True, timeout=3600)
            
            if result.returncode != 0:
                # Try alternative formats if JSON fails
                self.logger.warning(f"JSON export failed, trying alternative formats...")
                
                # Try dynamic format
                dynamic_output_path = self.parsed_dir / f"{self.case_id}_timeline.txt"
                dynamic_cmd = [
                    psort_cmd_path,
                    "-o", "dynamic",
                    "-w", str(dynamic_output_path),
                    "--temporary_directory", str(temp_dir),
                    str(plaso_file_path)
                ]
                
                dynamic_result = subprocess.run(dynamic_cmd, capture_output=True, text=True, timeout=3600)
                
                if dynamic_result.returncode == 0 and dynamic_output_path.exists():
                    self.logger.info("Dynamic format export successful")
                    success = self._process_dynamic_timeline(dynamic_output_path, custom_module)
                    if success:
                        self._post_optimize_database(database_path)
                        return True
                
                # Try CSV format as last resort
                csv_output_path = self.parsed_dir / f"{self.case_id}_timeline.csv"
                csv_cmd = [
                    psort_cmd_path,
                    "-o", "l2tcsv",
                    "-w", str(csv_output_path),
                    "--temporary_directory", str(temp_dir),
                    str(plaso_file_path)
                ]
                
                csv_result = subprocess.run(csv_cmd, capture_output=True, text=True, timeout=3600)
                
                if csv_result.returncode == 0 and csv_output_path.exists():
                    self.logger.warning("CSV export successful, but with limited data structure")
                    # Note: CSV processing would need additional implementation
                    # For now, we'll consider this a partial success
                
                self.log_custody_event("PLASO_IMPORT_ERROR", f"psort processing failed: {result.stderr}")
                return False
            
            # Process CSV timeline
            if csv_output_path.exists():
                success = self._process_csv_timeline(csv_output_path)
                
                if success:
                    # Database optimization handled by CSV import script
                    self.logger.info("CSV import completed - database ready")
                    
                    # Calculate performance metrics
                    end_time = time.time()
                    processing_time = end_time - start_time
                    end_memory = psutil.Process().memory_info().rss / 1024 / 1024
                    memory_delta = end_memory - start_memory
                    throughput = plaso_size / processing_time if processing_time > 0 else 0
                    
                    # Clean up temporary files
                    try:
                        json_output_path.unlink()
                        if temp_dir.exists():
                            shutil.rmtree(temp_dir, ignore_errors=True)
                    except Exception as e:
                        self.logger.debug(f"Cleanup warning: {e}")
                    
                    # Log performance metrics
                    db_size = database_path.stat().st_size / 1024 / 1024  # MB
                    self.log_custody_event("PLASO_IMPORT_SUCCESS",
                                         f"Plaso file import completed. "
                                         f"Time: {processing_time:.2f}s, Memory: {memory_delta:+.1f}MB, "
                                         f"Throughput: {throughput:.1f}MB/s, Database: {db_size:.1f}MB")
                    
                    self.logger.info(f"âœ… Plaso file import completed successfully")
                    self.logger.info(f"ðŸ“Š Performance: {processing_time:.2f}s, {throughput:.1f}MB/s, Database: {db_size:.1f}MB")
                    return True
                else:
                    self.log_custody_event("PLASO_IMPORT_ERROR", "Failed to process JSON timeline")
                    return False
            else:
                self.log_custody_event("PLASO_IMPORT_ERROR", "JSON output file was not created")
                return False
                
        except Exception as e:
            self.logger.error(f"Plaso file import failed: {e}")
            self.log_custody_event("PLASO_IMPORT_ERROR", f"Import failed: {str(e)}")
            return False
            
    def run_full_analysis(self, target: str, kape_path: Path, plaso_path: Path, 
                         questions: List[str] = None, date_from: str = None, date_to: str = None, days_back: int = None, keywords: List[str] = None, existing_artifacts_dir: Path = None, fast_mode: bool = False) -> bool:
        """Execute complete end-to-end forensic analysis with performance monitoring"""
        start_time = time.time()
        try:
            self.logger.info(f"Starting full forensic analysis for case {self.case_id}")
            self.log_custody_event("ANALYSIS_START", f"Full forensic analysis initiated for {target}")
            
            # Step 1: Collect artifacts or use existing
            if existing_artifacts_dir:
                if not existing_artifacts_dir.exists():
                    self.logger.error(f"Existing artifacts directory not found: {existing_artifacts_dir}")
                    return False
                
                # Set artifacts path to existing directory
                self.artifacts_path = existing_artifacts_dir
                artifact_count = len(list(existing_artifacts_dir.rglob("*")))
                self.logger.info(f"Using existing artifacts: {artifact_count} files in {existing_artifacts_dir}")
                self.log_custody_event("EXISTING_ARTIFACTS", f"Using existing artifacts: {artifact_count} files from {existing_artifacts_dir}")
            else:
                if not self.collect_artifacts_kape(target, kape_path):
                    return False
                
            # Step 2: Parse artifacts with Plaso
            if not self.parse_artifacts_plaso(plaso_path, fast_mode, date_from, date_to, existing_artifacts_dir, False):
                return False
                
            # Step 3: Validate and use database created by direct SQLite processing
            db_path = self.parsed_dir / f"{self.case_id}_bhsm.db"
            if not db_path.exists():
                self.logger.error("BHSM database not found after processing")
                return False
                
            # Final validation of the database
            if not self._validate_database_integrity(db_path):
                self.logger.error("Final database validation failed")
                return False
                
            self.logger.info(f"Using validated BHSM database: {db_path}")
            processor = ForensicProcessor(str(db_path))
            # Database already created with data by custom Plaso module
            
            # Step 3.5: Inject custom keywords now that database exists
            if keywords:
                self.logger.info(f"Injecting {len(keywords)} custom keywords into database")
                self.log_custody_event("KEYWORDS_INJECTION", f"Injecting {len(keywords)} custom keywords after database creation")
                inject_keywords(self.case_id, keywords)
                
            # Step 4: Answer forensic questions if provided
            if questions:
                for i, question in enumerate(questions, 1):
                    self.logger.info(f"Analyzing question: {question}")
                    answer = processor.answer_forensic_question(question, self.case_id, date_from, date_to, days_back)
                    
                    # Save answer to report
                    answer_file = self.reports_dir / f"question_{i}.json"
                    with open(answer_file, 'w') as f:
                        json.dump({
                            'question': question,
                            'answer': answer,
                            'timestamp': datetime.now(timezone.utc).isoformat()
                        }, f, indent=2)
                        
                    self.log_custody_event("QUESTION_ANALYSIS", f"Analyzed question: {question}", str(answer_file))
                    
            # Step 5: Generate comprehensive report
            report_file = self.reports_dir / f"{self.case_id}_comprehensive_report.json"
            self._generate_comprehensive_report(processor, report_file)
            
            # Step 6: Create final archive with all artifacts, extracts, and reports
            archive_path = self.create_final_archive()
            if archive_path:
                self.logger.info(f"Final case archive created: {archive_path}")
            else:
                self.logger.warning("Failed to create final archive, but analysis completed successfully")
            
            # Performance logging
            total_time = time.time() - start_time
            self.log_custody_event("ANALYSIS_COMPLETE", f"Full forensic analysis completed successfully in {total_time:.2f} seconds")
            self.logger.info(f"Full forensic analysis completed in {total_time:.2f} seconds. Results in: {self.output_dir}")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Full analysis error: {e}")
            self.log_custody_event("ANALYSIS_ERROR", f"Full analysis error: {str(e)}")
            return False
            
    def _generate_comprehensive_report(self, processor: 'ForensicProcessor', report_file: Path):
        """Generate comprehensive forensic report"""
        try:
            report = {
                'case_id': self.case_id,
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'analysis_summary': {
                    'processing_duration': str(datetime.now(timezone.utc) - self.start_time),
                    'chain_of_custody_events': len(self.chain_of_custody)
                },
                'output_files': {
                    'artifacts_directory': str(self.artifacts_dir),
                    'parsed_directory': str(self.parsed_dir),
                    'reports_directory': str(self.reports_dir),
                    'custody_directory': str(self.custody_dir)
                }
            }
            
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)
                
            self.log_custody_event("REPORT_GENERATION", "Comprehensive report generated", str(report_file))
            
        except Exception as e:
            self.logger.error(f"Report generation error: {e}")
            
    def generate_chain_of_custody_report(self) -> Path:
        """Generate comprehensive chain of custody documentation"""
        try:
            custody_file = self.custody_dir / f"{self.case_id}_chain_of_custody.json"
            
            custody_report = {
                'case_id': self.case_id,
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'analysis_start': self.start_time.isoformat(),
                'analysis_duration': str(datetime.now(timezone.utc) - self.start_time),
                'total_events': len(self.chain_of_custody),
                'events': self.chain_of_custody,
                'system_info': {
                    'hostname': os.environ.get('COMPUTERNAME', 'Unknown'),
                    'username': os.environ.get('USERNAME', 'Unknown'),
                    'python_version': sys.version,
                    'platform': sys.platform,
                    'working_directory': str(Path.cwd()),
                    'tool_version': 'FORAI.py v3.0 Streamlined'
                }
            }
            
            with open(custody_file, 'w') as f:
                json.dump(custody_report, f, indent=2)
                
            self.log_custody_event("CUSTODY_REPORT", "Chain of custody report generated", str(custody_file))
            return custody_file
            
        except Exception as e:
            self.logger.error(f"Chain of custody report error: {e}")
            return None

def load_keywords(args) -> List[str]:
    """Load keywords from file for case-insensitive flagging"""
    keywords = []
    
    # Load keywords from file
    if args.keywords_file and args.keywords_file.exists():
        try:
            with open(args.keywords_file, 'r', encoding='utf-8') as f:
                keywords = [line.strip().lower() for line in f if line.strip() and not line.startswith('#')]
            LOGGER.info(f"Loaded {len(keywords)} keywords from {args.keywords_file}")
        except Exception as e:
            LOGGER.error(f"Error loading keywords file: {e}")
    
    # Remove duplicates while preserving order
    seen = set()
    keywords = [k for k in keywords if not (k in seen or seen.add(k))]
    
    return keywords

def inject_keywords(case_id: str, keywords: List[str]) -> None:
    """Inject keywords into the evidence database for enhanced search and analysis"""
    if not keywords:
        return
    
    try:
        with get_database_connection() as conn:
            timestamp = int(time.time())
            
            # Inject keywords as flagged indicators
            for keyword in keywords:
                evidence_data = {
                    'Keyword': keyword,
                    'Type': 'Flagged Keyword',
                    'Source': 'Custom Keywords',
                    'SearchTerm': keyword.lower(),
                    'Category': 'Intelligence Keyword'
                }
                
                conn.execute("""
                    INSERT INTO evidence (case_id, timestamp, artifact, summary, data_json, source_file, host, user)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    case_id,
                    timestamp,
                    'Custom Keywords - Flagged Term',
                    f'Keyword flagged for monitoring: {keyword}',
                    json.dumps(evidence_data),
                    'custom_keywords',
                    'INTELLIGENCE',
                    'SYSTEM'
                ))
            
            conn.commit()
            LOGGER.info(f"Injected {len(keywords)} custom keywords into evidence database")
            
    except Exception as e:
        LOGGER.error(f"Error injecting keywords: {e}")

def main():
    """Modern main workflow"""
    parser = argparse.ArgumentParser(
        description="Modern Forensic Analysis Tool - Maximum Efficiency & Accuracy",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument('--case-id', required=True, help='Case identifier')
    
    # END-TO-END WORKFLOW OPTIONS
    parser.add_argument('--full-analysis', action='store_true', 
                       help='Complete end-to-end forensic analysis: collect â†’ parse â†’ analyze â†’ report')
    parser.add_argument('--target-drive', 
                       help='Target drive letter (e.g., C:) for live collection')
    
    # TIME WINDOW FILTERING
    parser.add_argument('--date-from', help='Start date for analysis (YYYYMMDD format)')
    parser.add_argument('--date-to', help='End date for analysis (YYYYMMDD format)')
    parser.add_argument('--days-back', type=int, help='Number of days back from today (e.g., --days-back 30)')
    
    # ARTIFACT COLLECTION & PARSING
    parser.add_argument('--collect-artifacts', action='store_true', help='Collect artifacts using KAPE')
    parser.add_argument('--parse-artifacts', action='store_true', help='Parse artifacts using Plaso timeline analysis')
    parser.add_argument('--artifacts-dir', type=Path, help='Use existing artifacts directory (skips KAPE collection)')
    parser.add_argument('--plaso-file', type=Path, help='Import existing .plaso file (skips log2timeline, goes directly to psort â†’ BHSM database)')
    parser.add_argument('--kape-path', type=Path, default=Path('D:/FORAI/tools/kape/kape.exe'), help='Path to KAPE executable')
    parser.add_argument('--plaso-path', type=Path, default=Path('D:/FORAI/tools/plaso'), help='Path to Plaso tools directory')
    parser.add_argument('--fast-mode', action='store_true', help='Enable fast processing mode (reduced parsers, optimized for 12 standard questions)')
    parser.add_argument('--enable-winevtx', action='store_true', help='Enable Windows Event Log parsing (may cause crashes with some Plaso versions)')
    
    # EXISTING OPTIONS
    # CSV arguments removed - using direct artifact â†’ SQLite workflow only
    parser.add_argument('--search', help='Search query for evidence')
    parser.add_argument('--question', help='Forensic question to answer')
    parser.add_argument('--report', choices=['json', 'pdf'], help='Generate comprehensive report')
    parser.add_argument('--init-db', action='store_true', help='Initialize database')
    parser.add_argument('--build-psi', action='store_true', help='Build PSI semantic index for fast searches (BHSM optimization)')
    parser.add_argument('--performance-test', action='store_true', help='Run performance comparison test')
    
    # CUSTOM INTELLIGENCE & CONTEXT
    parser.add_argument('--keywords-file', type=Path, help='File containing keywords to flag (one per line, case-insensitive)')
    
    # LLM CONFIGURATION OPTIONS
    parser.add_argument('--llm-folder', type=Path, help='Path to local LLM model folder (e.g., D:\\FORAI\\LLM)')
    parser.add_argument('--llm-api-token', help='API token for cloud LLM services (OpenAI, Anthropic, etc.)')
    parser.add_argument('--llm-api-provider', choices=['openai', 'anthropic', 'local'], default='local', 
                       help='LLM provider type (default: local)')
    parser.add_argument('--llm-model', default='tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf', 
                       help='Model name/path for local models or model ID for API providers')
    
    # AUTONOMOUS ANALYSIS
    parser.add_argument('--autonomous-analysis', action='store_true', 
                       help='Automatically answer all 12 standard forensic questions and generate summary report')
    
    # CHAIN OF CUSTODY & OUTPUT
    parser.add_argument('--chain-of-custody', action='store_true', help='Generate chain of custody documentation')
    parser.add_argument('--output-dir', type=Path, default=Path('D:/FORAI'), help='Output directory for all results')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose logging')
    
    args = parser.parse_args()
    
    # Input validation for security and accuracy
    if not validate_case_id(args.case_id):
        LOGGER.error(f"Invalid case ID format: {args.case_id}")
        LOGGER.error("Case ID must contain only alphanumeric characters, hyphens, and underscores (max 50 chars)")
        sys.exit(1)
    
    if args.date_from and not validate_date_format(args.date_from):
        LOGGER.error(f"Invalid date_from format: {args.date_from}")
        LOGGER.error("Date format must be YYYYMMDD (e.g., 20241201)")
        sys.exit(1)
    
    if args.date_to and not validate_date_format(args.date_to):
        LOGGER.error(f"Invalid date_to format: {args.date_to}")
        LOGGER.error("Date format must be YYYYMMDD (e.g., 20241215)")
        sys.exit(1)
    
    if args.days_back and (args.days_back < 1 or args.days_back > 3650):
        LOGGER.error(f"Invalid days_back value: {args.days_back}")
        LOGGER.error("Days back must be between 1 and 3650 (10 years)")
        sys.exit(1)
    
    if args.search and len(args.search.strip()) == 0:
        LOGGER.error("Search query cannot be empty")
        sys.exit(1)
    
    if args.question and len(args.question.strip()) == 0:
        LOGGER.error("Question cannot be empty")
        sys.exit(1)
    
    if args.verbose:
        LOGGER.setLevel(logging.DEBUG)
    
    # Set case ID in CONFIG for single database architecture
    CONFIG.set_case_id(args.case_id)
    LOGGER.info(f"Using BHSM database: {CONFIG.db_path}")
    
    # Check external tool dependencies
    if args.full_analysis or args.collect_artifacts or args.parse_artifacts:
        require_tools = args.full_analysis  # Full analysis requires all tools
        if not validate_workflow_requirements(args.kape_path, args.plaso_path, require_all=require_tools):
            if require_tools:
                LOGGER.error("Cannot proceed with full analysis due to missing required tools")
                sys.exit(1)
    
    try:
        # END-TO-END FULL ANALYSIS WORKFLOW
        if args.full_analysis:
            if not args.target_drive and not args.artifacts_dir:
                LOGGER.error("Full analysis requires --target-drive (for live collection) or --artifacts-dir (for existing collection)")
                sys.exit(1)
            target = args.target_drive if args.target_drive else "existing_artifacts"
            
            # Initialize workflow manager
            workflow = ForensicWorkflowManager(args.case_id, args.output_dir, args.verbose)
            
            # Load custom keywords (will be injected after database creation)
            keywords = load_keywords(args)
            if keywords:
                workflow.log_custody_event("KEYWORDS_LOADING", 
                                         f"Loading {len(keywords)} custom keywords for case-insensitive flagging")
            
            # Prepare questions list - YOUR 12 STANDARD FORENSIC QUESTIONS
            questions = [args.question] if args.question else [
                "What is the computer name, make, model, and serial number?",
                "What are the internal storage devices (make, model, serial numbers)?", 
                "What user accounts exist with their SIDs and activity timeframes?",
                "Who is the primary user based on activity volume and recency?",
                "Is there evidence of anti-forensic activities (log clearing, file deletion, timestamp modification)?",
                "What removable storage devices were connected (make, model, serial, timeframes)?",
                "What files were transferred to/from removable storage devices?",
                "What cloud storage services were accessed and what files were transferred?",
                "Were screenshots or screen recordings created?",
                "What documents were printed and when?",
                "What software was installed, uninstalled, or modified?",
                "What network connections and communications occurred?"
            ]
            
            # Run complete analysis with time filtering
            success = workflow.run_full_analysis(target, args.kape_path, args.plaso_path, questions, 
                                                args.date_from, args.date_to, args.days_back, keywords, args.artifacts_dir, args.fast_mode)
            
            if success:
                print(f"\nðŸŽ‰ FULL FORENSIC ANALYSIS COMPLETED SUCCESSFULLY!")
                print(f"ðŸ“ Results Directory: {args.output_dir}")
                print(f"ðŸ“Š Artifacts: {workflow.artifacts_dir}")
                print(f"ðŸ“‹ Parsed Data: {workflow.parsed_dir}")
                print(f"ðŸ“„ Reports: {workflow.reports_dir}")
                print(f"ðŸ”— Chain of Custody: {workflow.custody_dir}")
                print(f"ðŸ“¦ Final Archive: {workflow.output_dir}/archive")
                
                # Generate chain of custody if requested
                if args.chain_of_custody:
                    custody_file = workflow.generate_chain_of_custody_report()
                    print(f"ðŸ“œ Chain of Custody: {custody_file}")
            else:
                print("âŒ Full forensic analysis failed. Check logs for details.")
                sys.exit(1)
            return
        
        # AUTONOMOUS ANALYSIS MODE
        elif args.autonomous_analysis:
            LOGGER.info(f"Starting autonomous analysis for case {args.case_id}")
            
            # Create LLM provider
            llm_provider = create_llm_provider(args)
            
            if not llm_provider.is_available():
                LOGGER.warning("LLM provider not available - analysis will use deterministic methods only")
            
            # Initialize analyzer
            analyzer = ForensicAnalyzer()
            
            # Run autonomous analysis
            results = analyzer.autonomous_analysis(args.case_id, llm_provider)
            
            # Generate comprehensive report
            report_generator = ModernReportGenerator(args.case_id)
            report = report_generator.generate_autonomous_report(results)
            
            # Save report in requested format(s)
            output_dir = args.output_dir / "reports"
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # Always save JSON report
            json_report_path = report_generator.save_report(report, 'json')
            print(f"\nðŸŽ‰ AUTONOMOUS FORENSIC ANALYSIS COMPLETED!")
            print(f"ðŸ“Š Questions Answered: {results['questions_answered']}/{results['total_questions']}")
            print(f"ðŸŽ¯ Average Confidence: {results['average_confidence']:.2f}")
            print(f"â±ï¸  Processing Time: {results['processing_time']:.2f}s")
            print(f"ðŸ¤– LLM Provider: {results['llm_provider']}")
            print(f"ðŸ“„ JSON Report: {json_report_path}")
            
            # Also save PDF if requested
            if args.report == 'pdf':
                pdf_report_path = report_generator.save_report(report, 'pdf')
                print(f"ðŸ“„ PDF Report: {pdf_report_path}")
            
            # Print summary of results
            print(f"\nðŸ“‹ ANALYSIS SUMMARY:")
            confidence_analysis = report['confidence_analysis']
            print(f"   High Confidence: {confidence_analysis['high_confidence_answers']} answers ({confidence_analysis['confidence_distribution']['high']})")
            print(f"   Medium Confidence: {confidence_analysis['medium_confidence_answers']} answers ({confidence_analysis['confidence_distribution']['medium']})")
            print(f"   Low Confidence: {confidence_analysis['low_confidence_answers']} answers ({confidence_analysis['confidence_distribution']['low']})")
            
            if report['recommendations']:
                print(f"\nðŸ’¡ RECOMMENDATIONS:")
                for i, rec in enumerate(report['recommendations'], 1):
                    print(f"   {i}. {rec}")
            
            return
        
        # INDIVIDUAL WORKFLOW COMPONENTS
        if args.collect_artifacts:
            if not args.target_drive:
                LOGGER.error("Artifact collection requires --target-drive")
                sys.exit(1)
            target = args.target_drive
                
            workflow = ForensicWorkflowManager(args.case_id, args.output_dir, args.verbose)
            
            # Load and inject custom keywords
            keywords = load_keywords(args)
            if keywords:
                workflow.log_custody_event("KEYWORDS_LOADING", 
                                         f"Loading {len(keywords)} custom keywords for case-insensitive flagging")
                inject_keywords(args.case_id, keywords)
            
            success = workflow.collect_artifacts_kape(target, args.kape_path)
            print(f"Artifact collection {'completed' if success else 'failed'}")
            return
            
        if args.parse_artifacts:
            # Initialize database if it doesn't exist
            if not database_exists(args.case_id):
                LOGGER.info("Database doesn't exist, initializing...")
                initialize_database()
            
            workflow = ForensicWorkflowManager(args.case_id, args.output_dir, args.verbose)
            
            # Load and inject custom keywords
            keywords = load_keywords(args)
            if keywords:
                workflow.log_custody_event("KEYWORDS_LOADING", 
                                         f"Loading {len(keywords)} custom keywords for case-insensitive flagging")
                inject_keywords(args.case_id, keywords)
            
            success = workflow.parse_artifacts_plaso(args.plaso_path, args.fast_mode, args.date_from, args.date_to, args.artifacts_dir, args.enable_winevtx)
            print(f"Artifact parsing {'completed' if success else 'failed'}")
            
            # Continue to analytics and reporting phases after parsing
            if not success:
                LOGGER.error("Parsing failed, cannot proceed to analytics and reporting")
                return

        # Import existing plaso file if requested
        if args.plaso_file:
            # Validate plaso file exists
            if not args.plaso_file.exists():
                LOGGER.error(f"Plaso file not found: {args.plaso_file}")
                return
            
            # Initialize database if it doesn't exist
            if not database_exists(args.case_id):
                LOGGER.info("Database doesn't exist, initializing...")
                initialize_database()

            workflow = ForensicWorkflowManager(args.case_id, args.output_dir, args.verbose)

            # Load and inject custom keywords
            keywords = load_keywords(args)
            if keywords:
                workflow.log_custody_event("KEYWORDS_LOADING",
                                         f"Loading {len(keywords)} custom keywords for case-insensitive flagging")
                inject_keywords(args.case_id, keywords)

            success = workflow.import_plaso_file(args.plaso_file, args.plaso_path)
            print(f"Plaso file import {'completed' if success else 'failed'}")
            
            # Continue to analytics and reporting phases after import
            if not success:
                LOGGER.error("Plaso file import failed, cannot proceed to analytics and reporting")
                return
        
        # Initialize database if requested
        if args.init_db:
            initialize_database()
            return
        
        # Build PSI semantic index if requested
        if args.build_psi:
            print(f"Building PSI semantic index for case {args.case_id}...")
            success = build_psi_from_db(args.case_id)
            if success:
                print("âœ“ PSI semantic index built successfully")
            else:
                print("âœ— Failed to build PSI semantic index")
            return
        
        # Run performance test if requested
        if args.performance_test:
            run_performance_test(args.case_id)
            return

        # Load and inject custom keywords for standalone operations
        keywords = load_keywords(args)
        if keywords:
            inject_keywords(args.case_id, keywords)
        
        # CSV processing removed - using direct artifact â†’ SQLite workflow only
        
        # Search evidence
        if args.search:
            results = search_evidence(args.search, date_from=args.date_from, date_to=args.date_to, days_back=args.days_back)
            time_filter_msg = ""
            if args.days_back:
                time_filter_msg = f" (last {args.days_back} days)"
            elif args.date_from or args.date_to:
                time_filter_msg = f" ({args.date_from or 'start'} to {args.date_to or 'end'})"
            print(f"\nFound {len(results)} results for: {args.search}{time_filter_msg}")
            for result in results[:10]:
                print(f"- {result['artifact']}: {result['summary'][:100]}...")
        
        # Answer forensic question
        if args.question:
            analyzer = ForensicAnalyzer(args.llm_folder)
            answer = analyzer.answer_forensic_question(args.question, args.case_id, args.date_from, args.date_to, args.days_back)
            time_filter_msg = ""
            if args.days_back:
                time_filter_msg = f" (analyzing last {args.days_back} days)"
            elif args.date_from or args.date_to:
                time_filter_msg = f" (analyzing {args.date_from or 'start'} to {args.date_to or 'end'})"
            print(f"\nQuestion: {args.question}{time_filter_msg}")
            print(f"Answer: {answer}")
        
        # Generate report
        if args.report:
            # FORENSIC DATABASE VALIDATION (always required for chain of custody)
            print(f"\nðŸ” Performing forensic database validation...")
            db_path = CONFIG.base_dir / "extracts" / f"{args.case_id}_bhsm.db"
            
            # Initialize workflow for chain of custody logging
            if not 'workflow' in locals():
                workflow = ForensicWorkflowManager(args.case_id, args.output_dir, args.verbose)
            
            # Validate database exists
            if not db_path.exists():
                error_msg = f"Database not found: {db_path}"
                workflow.log_custody_event("DATABASE_VALIDATION_FAILED", error_msg)
                print(f"\nâŒ FORENSIC VALIDATION FAILED: {error_msg}")
                print("ðŸ›‘ WORKFLOW TERMINATED: Cannot generate forensic reports without valid database")
                print("ðŸ“‹ REQUIRED ACTION: Run with --parse-artifacts first to create the database.")
                sys.exit(1)
            
            # Validate database integrity and content
            try:
                conn = sqlite3.connect(db_path)
                
                # Check evidence table exists and has data
                cursor = conn.execute("SELECT COUNT(*) FROM evidence")
                evidence_count = cursor.fetchone()[0]
                
                # Get database file size for validation
                db_size_mb = db_path.stat().st_size / (1024 * 1024)
                
                # Check for basic table structure
                cursor = conn.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                conn.close()
                
                # Validate minimum forensic requirements
                if evidence_count == 0:
                    error_msg = f"Database validation failed: No evidence records found (database size: {db_size_mb:.1f}MB)"
                    workflow.log_custody_event("DATABASE_VALIDATION_FAILED", error_msg)
                    print(f"\nâŒ FORENSIC VALIDATION FAILED: {error_msg}")
                    print("ðŸ›‘ WORKFLOW TERMINATED: Cannot generate forensic reports from empty database")
                    print("ðŸ“‹ REQUIRED ACTION: Run with --parse-artifacts first to populate the database.")
                    sys.exit(1)
                
                # Log successful validation to chain of custody
                validation_msg = f"Database validation successful: {evidence_count} evidence records, {db_size_mb:.1f}MB, tables: {', '.join(tables)}"
                workflow.log_custody_event("DATABASE_VALIDATION_SUCCESS", validation_msg)
                print(f"âœ… FORENSIC VALIDATION PASSED: {evidence_count} evidence records ({db_size_mb:.1f}MB)")
                print(f"ðŸ”’ Database integrity verified - proceeding with report generation")
                
            except Exception as e:
                error_msg = f"Database validation error: {str(e)}"
                workflow.log_custody_event("DATABASE_VALIDATION_ERROR", error_msg)
                print(f"\nâŒ FORENSIC VALIDATION ERROR: {error_msg}")
                print("ðŸ›‘ WORKFLOW TERMINATED: Database integrity cannot be verified")
                print("ðŸ“‹ REQUIRED ACTION: Check database file and re-run parsing if necessary")
                sys.exit(1)
            
            # Proceed with report generation using validated database
            print(f"\nðŸ“Š Generating forensic report from validated database...")
            workflow.log_custody_event("REPORT_GENERATION_START", f"Starting {args.report.upper()} report generation with {evidence_count} evidence records")
            
            generator = ModernReportGenerator(args.case_id, args.llm_folder)
            report = generator.generate_comprehensive_report()
            report_path = generator.save_report(report, args.report)
            
            workflow.log_custody_event("REPORT_GENERATION_COMPLETE", f"Report generated: {report_path}")
            print(f"âœ… Report generated: {report_path}")
        
        # Generate chain of custody documentation
        if args.chain_of_custody:
            # Use existing workflow if available, otherwise create new one
            if not 'workflow' in locals():
                workflow = ForensicWorkflowManager(args.case_id, args.output_dir, args.verbose)
            custody_file = workflow.generate_chain_of_custody_report()
            print(f"\nðŸ“œ Chain of custody generated: {custody_file}")
    
    except Exception as e:
        LOGGER.error(f"Error in main workflow: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()

