#!/usr/bin/env python3
"""
CSV Timeline Import Script for FORAI
Imports plaso CSV timeline files - more reliable than JSON for forensic analysis
"""

import csv
import sqlite3
import sys
import json
from pathlib import Path
from datetime import datetime
import hashlib

# Increase CSV field size limit for forensic data (100MB should handle any forensic field)
csv.field_size_limit(100 * 1024 * 1024)

def create_database_schema(db_path):
    """Create the FORAI database schema"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Create evidence table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS evidence (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            case_id TEXT NOT NULL,
            host TEXT,
            user TEXT,
            timestamp TEXT,
            datetime_utc TEXT,
            source TEXT,
            source_long TEXT,
            source_file TEXT,
            message TEXT,
            summary TEXT,
            data_json TEXT,
            parser TEXT,
            display_name TEXT,
            filename TEXT,
            inode TEXT,
            notes TEXT,
            format TEXT,
            extra TEXT,
            sha256_hash TEXT,
            artifact TEXT,
            flagged INTEGER DEFAULT 0,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    # Create indexes
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_evidence_case_id ON evidence(case_id)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_evidence_timestamp ON evidence(timestamp)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_evidence_source ON evidence(source)')
    cursor.execute('CREATE INDEX IF NOT EXISTS idx_evidence_flagged ON evidence(flagged)')
    
    # Create keywords table
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS keywords (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            case_id TEXT NOT NULL,
            keyword TEXT NOT NULL,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    ''')
    
    conn.commit()
    conn.close()
    print(f"âœ… Database schema created: {db_path}")

def detect_csv_format(csv_path):
    """Detect the CSV format and column headers"""
    print(f"ğŸ” Analyzing CSV format: {csv_path}")
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        # Read first few lines to detect format
        sample_lines = []
        for i in range(5):
            line = f.readline()
            if line:
                sample_lines.append(line.strip())
        
        print("ğŸ“‹ First 5 lines:")
        for i, line in enumerate(sample_lines):
            print(f"  {i+1}: {line[:200]}{'...' if len(line) > 200 else ''}")
    
    # Try to detect delimiter and headers
    with open(csv_path, 'r', encoding='utf-8') as f:
        # Use csv.Sniffer to detect format
        sample = f.read(8192)
        f.seek(0)
        
        sniffer = csv.Sniffer()
        try:
            dialect = sniffer.sniff(sample)
            print(f"ğŸ“Š Detected delimiter: '{dialect.delimiter}'")
            print(f"ğŸ“Š Quote character: '{dialect.quotechar}'")
        except:
            print("ğŸ“Š Using default CSV format (comma-separated)")
            dialect = csv.excel
        
        # Read headers
        reader = csv.reader(f, dialect=dialect)
        headers = next(reader)
        print(f"ğŸ“Š Headers ({len(headers)} columns): {headers}")
        
        return dialect, headers

def import_csv_timeline(csv_path, db_path, case_id):
    """Import CSV timeline into database"""
    print(f"ğŸ“¥ Importing CSV timeline: {csv_path}")
    print(f"ğŸ“Š File size: {csv_path.stat().st_size / 1024 / 1024:.1f} MB")
    
    # Detect CSV format
    dialect, headers = detect_csv_format(csv_path)
    
    # Map common plaso CSV headers to our database fields
    header_map = {}
    for i, header in enumerate(headers):
        header_lower = header.lower()
        if 'date' in header_lower or 'time' in header_lower:
            if 'timestamp' not in header_map:
                header_map['timestamp'] = i
            if 'datetime' not in header_map:
                header_map['datetime'] = i
        elif 'source' in header_lower and 'long' not in header_lower:
            header_map['source'] = i
        elif 'source' in header_lower and 'long' in header_lower:
            header_map['source_long'] = i
        elif 'message' in header_lower or 'description' in header_lower:
            header_map['message'] = i
        elif 'parser' in header_lower:
            header_map['parser'] = i
        elif 'display' in header_lower or 'filename' in header_lower:
            header_map['display_name'] = i
        elif 'file' in header_lower and 'name' in header_lower:
            header_map['filename'] = i
        elif 'inode' in header_lower:
            header_map['inode'] = i
        elif 'format' in header_lower or 'type' in header_lower:
            header_map['format'] = i
    
    print(f"ğŸ“Š Field mapping: {header_map}")
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    imported_count = 0
    error_count = 0
    batch_size = 1000
    batch = []
    
    try:
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f, dialect=dialect)
            
            # Skip header row
            next(reader)
            
            for row_num, row in enumerate(reader, 2):  # Start at 2 since we skipped header
                if row_num % 100000 == 0:
                    print(f"ğŸ“ˆ Processing row {row_num:,}...")
                
                try:
                    # Handle rows with different column counts
                    if len(row) < len(headers):
                        row.extend([''] * (len(headers) - len(row)))
                    
                    # Extract fields using header mapping
                    timestamp = row[header_map.get('timestamp', 0)] if header_map.get('timestamp') is not None else ''
                    datetime_utc = row[header_map.get('datetime', 1)] if header_map.get('datetime') is not None else ''
                    source = row[header_map.get('source', 2)] if header_map.get('source') is not None else ''
                    source_long = row[header_map.get('source_long', 3)] if header_map.get('source_long') is not None else ''
                    message = row[header_map.get('message', 4)] if header_map.get('message') is not None else ''
                    parser = row[header_map.get('parser', 5)] if header_map.get('parser') is not None else ''
                    display_name = row[header_map.get('display_name', 6)] if header_map.get('display_name') is not None else ''
                    filename = row[header_map.get('filename', 7)] if header_map.get('filename') is not None else ''
                    inode = row[header_map.get('inode', 8)] if header_map.get('inode') is not None else ''
                    format_type = row[header_map.get('format', 9)] if header_map.get('format') is not None else ''
                    
                    # Store remaining columns as extra data
                    extra_data = {}
                    for i, value in enumerate(row):
                        if i not in header_map.values() and i < len(headers):
                            extra_data[headers[i]] = value
                    extra = str(extra_data) if extra_data else ''
                    
                    # Create SHA256 hash of the entry
                    entry_str = f"{timestamp}{source}{message}{filename}"
                    sha256_hash = hashlib.sha256(entry_str.encode()).hexdigest()[:16]
                    
                    # Map CSV data to database columns
                    # Use message as summary and create JSON data structure
                    summary = message[:500] if message else ''  # Truncate for summary
                    data_json = json.dumps({
                        'source': source,
                        'source_long': source_long,
                        'message': message,
                        'parser': parser,
                        'display_name': display_name,
                        'filename': filename,
                        'format': format_type,
                        'extra': extra
                    }) if any([source, source_long, message, parser, display_name, filename, format_type, extra]) else ''
                    
                    batch.append((
                        case_id, '', '', timestamp, datetime_utc, source, source_long, filename,
                        message, summary, data_json, parser, display_name, filename, inode,
                        '', format_type, extra, sha256_hash, 'timeline', 0
                    ))
                    
                    if len(batch) >= batch_size:
                        cursor.executemany('''
                            INSERT INTO evidence (
                                case_id, host, user, timestamp, datetime_utc, source, source_long, source_file,
                                message, summary, data_json, parser, display_name, filename, inode,
                                notes, format, extra, sha256_hash, artifact, flagged
                            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                        ''', batch)
                        conn.commit()
                        imported_count += len(batch)
                        batch = []
                        
                        if imported_count % 50000 == 0:
                            print(f"ğŸ’¾ Imported {imported_count:,} records...")
                    
                except Exception as e:
                    error_count += 1
                    if error_count <= 10:  # Show first 10 errors
                        print(f"âš ï¸  Error processing row {row_num}: {e}")
                    elif error_count == 11:
                        print(f"âš ï¸  Suppressing further error messages (total errors: {error_count})")
                    continue
        
        # Insert remaining batch
        if batch:
            cursor.executemany('''
                INSERT INTO evidence (
                    case_id, host, user, timestamp, datetime_utc, source, source_long, source_file,
                    message, summary, data_json, parser, display_name, filename, inode,
                    notes, format, extra, sha256_hash, artifact, flagged
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', batch)
            conn.commit()
            imported_count += len(batch)
    
    finally:
        conn.close()
    
    print(f"âœ… Import completed: {imported_count:,} records imported")
    if error_count > 0:
        print(f"âš ï¸  Total errors encountered: {error_count:,} (forensic integrity maintained)")
    return imported_count

def inject_keywords(db_path, case_id, keywords_file):
    """Inject keywords into database"""
    if not keywords_file.exists():
        print(f"âš ï¸  Keywords file not found: {keywords_file}")
        return
    
    keywords = []
    with open(keywords_file, 'r', encoding='utf-8') as f:
        for line in f:
            keyword = line.strip()
            if keyword and not keyword.startswith('#'):
                keywords.append(keyword.lower())
    
    if not keywords:
        print("âš ï¸  No keywords found")
        return
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # Insert keywords
    for keyword in keywords:
        cursor.execute('INSERT INTO keywords (case_id, keyword) VALUES (?, ?)', (case_id, keyword))
    
    # Flag evidence containing keywords
    flagged_count = 0
    for keyword in keywords:
        cursor.execute('''
            UPDATE evidence 
            SET flagged = 1 
            WHERE case_id = ? AND (
                LOWER(message) LIKE ? OR 
                LOWER(source) LIKE ? OR 
                LOWER(filename) LIKE ?
            )
        ''', (case_id, f'%{keyword}%', f'%{keyword}%', f'%{keyword}%'))
        flagged_count += cursor.rowcount
    
    conn.commit()
    conn.close()
    
    print(f"âœ… Keywords injected: {len(keywords)} keywords, {flagged_count} records flagged")

def main():
    if len(sys.argv) != 4:
        print("Usage: python csv_import.py <csv_file> <case_id> <keywords_file>")
        print("Example: python csv_import.py CASE001_timeline.csv CASE001 keywords.txt")
        sys.exit(1)
    
    csv_file = Path(sys.argv[1])
    case_id = sys.argv[2]
    keywords_file = Path(sys.argv[3])
    
    if not csv_file.exists():
        print(f"âŒ CSV file not found: {csv_file}")
        sys.exit(1)
    
    # Create single database path - FORAI.py uses forai.db
    forai_db = csv_file.parent / "forai.db"
    
    print(f"ğŸš€ Starting CSV import for case {case_id}")
    print(f"ğŸ“ CSV file: {csv_file}")
    print(f"ğŸ—„ï¸  Database: {forai_db}")
    
    # Create single database
    create_database_schema(forai_db)
    
    # Import timeline from CSV
    count = import_csv_timeline(csv_file, forai_db, case_id)
    
    # Inject keywords
    inject_keywords(forai_db, case_id, keywords_file)
    
    print(f"\nğŸ‰ CSV import completed successfully!")
    print(f"ğŸ“Š Total records: {count:,}")
    print(f"ğŸ—„ï¸  Database size: {forai_db.stat().st_size / 1024 / 1024:.1f} MB")
    print(f"\nâœ… Ready for autonomous analysis!")
    print(f"ğŸ”¬ Forensic integrity maintained - all data preserved")

if __name__ == "__main__":
    main()